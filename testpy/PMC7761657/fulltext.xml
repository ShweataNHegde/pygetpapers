<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Entropy (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Entropy (Basel)</journal-id><journal-id journal-id-type="publisher-id">entropy</journal-id><journal-title-group><journal-title>Entropy</journal-title></journal-title-group><issn pub-type="epub">1099-4300</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">7761657</article-id><article-id pub-id-type="pmid">33279911</article-id><article-id pub-id-type="doi">10.3390/e22121365</article-id><article-id pub-id-type="publisher-id">entropy-22-01365</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Semiotic Aggregation in Deep Learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mu&#x0015f;at</surname><given-names>Bogdan</given-names></name><xref ref-type="aff" rid="af1-entropy-22-01365">1</xref><xref ref-type="aff" rid="af2-entropy-22-01365">2</xref><xref rid="c1-entropy-22-01365" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6015-3151</contrib-id><name><surname>Andonie</surname><given-names>R&#x00103;zvan</given-names></name><xref ref-type="aff" rid="af1-entropy-22-01365">1</xref><xref ref-type="aff" rid="af3-entropy-22-01365">3</xref></contrib></contrib-group><aff id="af1-entropy-22-01365"><label>1</label>Department of Electronics and Computers, Transilvania University, 500036 Bra&#x0015f;ov, Romania</aff><aff id="af2-entropy-22-01365"><label>2</label>Xperi Corporation, 3025 Orchard Parkway, San Jose, CA 95134, USA</aff><aff id="af3-entropy-22-01365"><label>3</label>Department of Computer Science, Central Washington University, Ellensburg, WA 98926, USA; <email>razvan.andonie@cwu.edu</email></aff><author-notes><corresp id="c1-entropy-22-01365"><label>*</label>Correspondence: <email>bogdan_musat_adrian@yahoo.com</email></corresp></author-notes><pub-date pub-type="epub"><day>03</day><month>12</month><year>2020</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2020</year></pub-date><volume>22</volume><issue>12</issue><elocation-id>1365</elocation-id><history><date date-type="received"><day>03</day><month>11</month><year>2020</year></date><date date-type="accepted"><day>27</day><month>11</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; 2020 by the authors.</copyright-statement><copyright-year>2020</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Convolutional neural networks utilize a hierarchy of neural network layers. The statistical aspects of information concentration in successive layers can bring an insight into the feature abstraction process. We analyze the saliency maps of these layers from the perspective of semiotics, also known as the study of signs and sign-using behavior. In computational semiotics, this aggregation operation (known as superization) is accompanied by a decrease of spatial entropy: signs are aggregated into supersign. Using spatial entropy, we compute the information content of the saliency maps and study the superization processes which take place between successive layers of the network. In our experiments, we visualize the superization process and show how the obtained knowledge can be used to explain the neural decision model. In addition, we attempt to optimize the architecture of the neural model employing a semiotic greedy technique. To the extent of our knowledge, this is the first application of computational semiotics in the analysis and interpretation of deep neural networks.</p></abstract><kwd-group><kwd>deep learning</kwd><kwd>spatial entropy</kwd><kwd>saliency maps</kwd><kwd>semiotics</kwd><kwd>convolutional neural networks</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-entropy-22-01365"><title>1. Introduction</title><p>Convolutional neural networks (CNNs) were first made popular by Lecun et al. [<xref rid="B1-entropy-22-01365" ref-type="bibr">1</xref>] with their seminal work on handwritten character recognition, where they introduced the currently popular LeNet-5 architecture. At that time, computational power constraints and lack of data prohibited those CNNs from achieving their true potential in terms of computer vision capabilities. Years later, Krizhevsky et al. [<xref rid="B2-entropy-22-01365" ref-type="bibr">2</xref>] marked the start of the current deep learning revolution, when, during the ILSVRC 2012 competition, their CNN, entitled AlexNet, overrun its competitor from the previous year by a margin of almost 10%. Since then, research on novel CNN architectures became very popular producing candidates like VGG [<xref rid="B3-entropy-22-01365" ref-type="bibr">3</xref>], GoogleNet [<xref rid="B4-entropy-22-01365" ref-type="bibr">4</xref>], ResNet [<xref rid="B5-entropy-22-01365" ref-type="bibr">5</xref>], and more recently EfficientNet [<xref rid="B6-entropy-22-01365" ref-type="bibr">6</xref>].</p><p>Despite the ability of generating human-alike predictions, CNNs still lack a major component: interpretability. Neural networks in general are known for their black-box type of behavior, being capable of capturing semantic information using numerical computations and gradient-based learning, but hiding the inner working mechanisms of reasoning. However, reasoning is of crucial importance for areas like medicine, law, and finance, where most decisions need to come along with good explanations for taking one particular action in favor of another. Usually, there is a trade-off between accuracy and interpretability. For instance, extracted IF-THEN rules from a neural network are highly interpretable but less accurate.</p><p>Since the emergence of deep learning, there have been efforts to analyze the interpretability issue and come up with potential solutions that might equip neural networks with a sense of causality [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>,<xref rid="B8-entropy-22-01365" ref-type="bibr">8</xref>,<xref rid="B9-entropy-22-01365" ref-type="bibr">9</xref>,<xref rid="B10-entropy-22-01365" ref-type="bibr">10</xref>,<xref rid="B11-entropy-22-01365" ref-type="bibr">11</xref>,<xref rid="B12-entropy-22-01365" ref-type="bibr">12</xref>,<xref rid="B13-entropy-22-01365" ref-type="bibr">13</xref>]. The high complexity of deep models makes these models hard to interpret. It is not feasible to extract (and interpret) classical IF-THEN rules from a ResNet with over 200 layers.</p><p>We need different interpretation methods for deep models and an idea comes from image processing/understanding. A common technique for understanding the decisions of image classification systems is to find regions of an input image that were particularly influential to the final classification. This technique is known under various names: sensitivity map, saliency map, or pixel attribution map. We will use the term <italic>saliency map</italic>. Saliency maps have long been present and used in image recognition. Essentially, a saliency map is a 2D topological map that indicates visual attention priorities. Applications of saliency maps include image segmentation, object detection, image re-targeting, image/video compression, and advertising design [<xref rid="B13-entropy-22-01365" ref-type="bibr">13</xref>].</p><p>Recently, saliency maps became a popular tool for gaining insight into deep learning. In this case, saliency maps are typically rendered as heatmaps of neural layers, where &#x0201c;hotness&#x0201d; corresponds to regions that have a big impact on the model&#x02019;s final decision. We illustrate with an intuitive gradient-based approach, the Vanilla Gradient algorithm [<xref rid="B8-entropy-22-01365" ref-type="bibr">8</xref>], which proceeds as follows: forward pass with data, backward pass to the input layer to get the gradient, and render the gradient as a normalized heatmap.</p><p>Certainly, saliency maps are not the universal tool for interpreting neural models. They focus on the input and may neglect to explain how the model makes decisions. It is possible that saliency maps are extremely similar for very different output predictions of the neural model. An example was provided by Alvin Wan (<uri xlink:href="https://bair.berkeley.edu/blog/2020/04/23/decisions/#fn:saliency">https://bair.berkeley.edu/blog/2020/04/23/decisions/#fn:saliency</uri>) using the Grad-CAM (Gradient-weighted Class Activation Mapping) saliency map generator [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>]. In addition, some widely deployed saliency methods are incapable of supporting tasks that require explanations that are faithful to the model or the data generating process. Relying only on visual assessment of the saliency maps can be misleading and two tests for assessing the scope and quality of explanation methods were introduced in [<xref rid="B14-entropy-22-01365" ref-type="bibr">14</xref>].</p><p>A good visual interpretation should be class-discriminative (i.e., localize the category in the image) and high-resolution (i.e., capture fine-grained details) [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>]. Guided Grad-CAM [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>] is an example of a visualization which is both high-resolution and class-discriminative: important regions of the image which correspond to any decision of interest are visualized in high-resolution detail even if the image contains evidence for multiple possible concepts.</p><p>In our approach, we focus on the statistical aspects of the information concentration processes which appear in the saliency maps of successive CNN layers. We analyze the saliency maps of these layers from the perspective of semiotics. In computational semiotics, this aggregation operation (known as superization) is accompanied by a decrease of spatial entropy: signs are aggregated into supersigns. A saliency map aggregates information from the previous layer of the network. In computational semiotics, this aggregation operation is known as <italic>superization</italic>, and it can be measured by a decrease of spatial entropy. In this case, signs are synthesized into supersigns.</p><p>Our contribution is an original, and to our knowledge, the first application application of computational semiotics in the analysis and interpretation of deep neural networks. <italic>Semiotics</italic> is known as the study of signs and sign-using behavior. According to [<xref rid="B15-entropy-22-01365" ref-type="bibr">15</xref>], <italic>computational semiotics</italic> is an interdisciplinary field which proposes a new kind of approach to intelligent systems, where an explicit account for the notion of sign is prominent. In our work, the definition of computational semiotics refers to the application of semiotics to artificial intelligence. We put the notion of sign from semiotics into service to give a new interpretation of deep learning, and this is new. We use computational semiotics&#x02019; concepts to explain decision processes in CNN models. We also study the possibility of applying semiotic tools to optimize the architecture of deep learning neural networks. Currently, model architecture optimization is a hot research topic in machine learning.</p><p>The inputs for our model are saliency maps, generated for each CNN layer by Grad-CAM, which currently is a state-of-the-art method. We compute the entropy of the saliency maps, meaning that we quantify the information content of these maps. This allows us to study the superization processes which take place between successive layers of the network. In our experiments, we show how the obtained knowledge can be used to explain the neural decision model. In addition, we attempt to optimize the architecture of the neural model employing a semiotic greedy technique.</p><p>The paper proceeds as follows: <xref ref-type="sec" rid="sec2-entropy-22-01365">Section 2</xref> describes the visualization of CNN networks through saliency maps, with a focus on the Grad-CAM method used in our approach. Image spatial entropy and its connection to saliency maps are presented in <xref ref-type="sec" rid="sec3-entropy-22-01365">Section 3</xref>. <xref ref-type="sec" rid="sec4-entropy-22-01365">Section 4</xref> introduces semiotic aggregation in the context of deep learning. <xref ref-type="sec" rid="sec5-entropy-22-01365">Section 5</xref> concentrates the conceptual core of or contribution&#x02014;the links between semiotic aggregation and CNN saliency maps. The experimental results are described in <xref ref-type="sec" rid="sec6-entropy-22-01365">Section 6</xref>. <xref ref-type="sec" rid="sec7-entropy-22-01365">Section 7</xref> discusses how semiotic aggregation could be used to optimize the architecture of a CNN. <xref ref-type="sec" rid="sec8-entropy-22-01365">Section 8</xref> contains final remarks and open problems.</p></sec><sec id="sec2-entropy-22-01365"><title>2. Saliency Maps in CNNs</title><p>This section describes the most recent techniques used for the visualization of CNN layers. Overviews of saliency models applied to deep learning networks can be found in [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>,<xref rid="B13-entropy-22-01365" ref-type="bibr">13</xref>].</p><p>One of the earliest works belong to Zeiler et al. [<xref rid="B9-entropy-22-01365" ref-type="bibr">9</xref>]. They used a Deconvolutional Network (deconvnet) to visualize the relevant parts from an input image that excite the top nine most activated neurons from a feature map (i.e., the output of a convolutional layer, a maxpool layer, a nonlinear activation function) resulted at a particular layer. A deconvnet represents a map from the hidden neuron activities back to the input pixel space by means of inverting the operations done in a CNN: unpooling, rectifying (using ReLU), and convolutional filtering. Their visualization technique strengthened the intuition that convolutional filters do indeed learn hierarchical features, starting from simple strokes and edges to object parts and in the end composing whole objects, as the depth of the network increases. Springenberg et al. [<xref rid="B10-entropy-22-01365" ref-type="bibr">10</xref>] demonstrated that by slightly changing the way the gradient through the ReLU nonlinearity is computed&#x02014;by discarding negative values (guided backpropagation)&#x02014;they can visualize convolutional filters for a CNN with strided convolution instead of pooling.</p><p>In visual recognition, a saliency map (e.g., <xref ref-type="fig" rid="entropy-22-01365-f001">Figure 1</xref>) can capture the most important or salient features (pixels) of an input image which are responsible for a particular decision. In the case of a CNN classifier, this decision translates into finding the class with the maximum likelihood score. As can be seen in <xref ref-type="fig" rid="entropy-22-01365-f001">Figure 1</xref>, the saliency map can be represented as a heatmap, where the intensity represents the importance of the features.</p><p>The notion of saliency map is not novel and has been used even before the emergence of CNNs [<xref rid="B16-entropy-22-01365" ref-type="bibr">16</xref>]. In the context of CNNs, the work of Simonyan et al. [<xref rid="B8-entropy-22-01365" ref-type="bibr">8</xref>] was among the first ones to explore saliency maps by using as a signal the backpropagated gradients with respect to the input image. Higher magnitudes of the gradient tensor corresponded to higher importance of the respective pixels. In deep CNNs, the class-aware gradient signal is mostly lost while moving backwards to the input of the network. Thus, for a lot of images, the resulted saliency maps in [<xref rid="B16-entropy-22-01365" ref-type="bibr">16</xref>] were noisy and difficult to interpret. The same paper introduced a method for generating class specific images: apply gradient descent on a random input noise image until convergence, in order to maximize the likelihood of a class. The resulting images managed to capture some of the semantics of a real image belonging to that class.</p><p>A related approach is SmoothGrad, proposed by Smilkov et al. [<xref rid="B11-entropy-22-01365" ref-type="bibr">11</xref>], where the gradient corresponding to an input image is computed as the average of the gradients of multiple samples obtained by adding Gaussian noise to the original input image. This has the effect of smoothing the resulted gradient with a Gaussian kernel by means of a stochastic approximation, resulting in a less noisy saliency map.</p><p>Grad-CAM, a recent popular technique for saliency map visualization [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>], uses the gradient information obtained from backpropagating the error signal from the loss function with respect to a specific feature map <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>h</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, at any layer <italic>l</italic> of the network, where <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic>c</italic> represent the width, height, and number of channels of that feature map, respectively. The gradient signal is averaged over the spatial dimensions <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to obtain a <italic>c</italic>-dimensional vector of importance weights <inline-formula><mml:math id="mm4"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The importance weights are used to perform a weighted channel-wise combination with the feature maps <inline-formula><mml:math id="mm5"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and ultimately passed through a ReLU activation function:<disp-formula id="FD1-entropy-22-01365"><label>(1)</label><mml:math id="mm6"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>O</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:munderover><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm7"><mml:mrow><mml:msubsup><mml:mi>O</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> in Formula (<xref ref-type="disp-formula" rid="FD1-entropy-22-01365">1</xref>) is the output resulted by applying the Grad-CAM technique on a particular layer <italic>l</italic>. By normalizing the values between <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using the min-max normalization scheme and then, multiplying by 255, it will result in a map of pixel intensities <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>255</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where 255 denotes maximum importance and 0 denotes no importance.</p><p>The ReLU activation function is applied because only features that have a positive influence on the class of interest usually matter. Negative values are features likely to belong to other categories in the image. In [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>], the authors justify that, without the ReLU function, the saliency maps could sometimes highlight more than just the desired class of interest.</p><p>Grad-CAM can be used to explain activations in any layer of a deep network. In [<xref rid="B7-entropy-22-01365" ref-type="bibr">7</xref>], it was applied only to the final layer, in order to interpret the output layer decisions. In our experiments, we use Grad-CAM to generate the saliency maps of all CNN layers.</p></sec><sec id="sec3-entropy-22-01365"><title>3. Image Spatial Entropy</title><p>Our work analyzes the entropy variations of 2D saliency maps. For this, we need to compute the entropy of 2D structures. This is very different than the approach in [<xref rid="B16-entropy-22-01365" ref-type="bibr">16</xref>], where saliency maps are obtained from local entropy calculation. Rather than generating maps using an entropy measure, we compute the entropy of saliency maps generated by the gradient method in Grad-CAM.</p><p>The most trivial solution is to use the univariate entropy, which assumes all pixels as being independent and does not take into consideration the contextual aspect information.</p><p>A more accurate model is the Spatial Disorder Entropy (SDE) [<xref rid="B17-entropy-22-01365" ref-type="bibr">17</xref>], which considers an entropy measure for each possible spatial distance in an image. Let us define the joint probability of pixels at spatial locations <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to take the value <italic>g</italic>, respectively <inline-formula><mml:math id="mm12"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD2-entropy-22-01365"><label>(2)</label><mml:math id="mm13"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>g</italic> and <inline-formula><mml:math id="mm14"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are pixel intensity values (<inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>255</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). If we assume that <inline-formula><mml:math id="mm16"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is independent of <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (the homogeneity assumption [<xref rid="B17-entropy-22-01365" ref-type="bibr">17</xref>]), we define for each pair <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the entropy
<disp-formula id="FD3-entropy-22-01365"><label>(3)</label><mml:math id="mm19"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>g</mml:mi></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where the summations are over the number of outcome values (256 in our case). A standardized relative measure of bivariate entropy is [<xref rid="B17-entropy-22-01365" ref-type="bibr">17</xref>]:<disp-formula id="FD4-entropy-22-01365"><label>(4)</label><mml:math id="mm20"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The maximum entropy <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to the case of two independent variables. <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the univariate entropy, which assumes all pixels as being independent, and we have <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02265;</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Based on the relative entropy for <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the SDE for an <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> image <inline-formula><mml:math id="mm26"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> was defined in [<xref rid="B17-entropy-22-01365" ref-type="bibr">17</xref>] as:<disp-formula id="FD5-entropy-22-01365"><label>(5)</label><mml:math id="mm27"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>j</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="mm28"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mo>&#x0003e;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is always equal or very close to one. Consequently, <inline-formula><mml:math id="mm30"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is usually very close to one (the max value) for most images, which is not convenient for our purposes. In addition, the complexity of SDE computation is high.</p><p>For these reasons, we decided to use a simplified version&#x02014;the Aura Matrix Entropy (AME, see [<xref rid="B18-entropy-22-01365" ref-type="bibr">18</xref>]), which only considers the second order neighbors from the SDE computation:<disp-formula id="FD6-entropy-22-01365"><label>(6)</label><mml:math id="mm31"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The additional assumption is that the image is isotropic, which causes different orientations of the neighboring pixels to have the same entropy. In other words, the joint pdf of the vertical and horizontal neighboring process is averaged to obtain a global joint pdf of the image. This averaging makes the resulting pdf smoother and more equally distributed throughout the entire sample space. In a comparison study [<xref rid="B19-entropy-22-01365" ref-type="bibr">19</xref>], the AME measure provided the most effective outcome among several other image spatial entropy definitions, even if it overestimates the image information.</p><p>Putting it all together, starting from a map obtained by Formula (<xref ref-type="disp-formula" rid="FD1-entropy-22-01365">1</xref>), we compute the probabilities <inline-formula><mml:math id="mm32"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in Formula (<xref ref-type="disp-formula" rid="FD2-entropy-22-01365">2</xref>), and finally the AME in Formula (<xref ref-type="disp-formula" rid="FD6-entropy-22-01365">6</xref>).</p></sec><sec id="sec4-entropy-22-01365"><title>4. Semiotic Aggregation in Deep Learning</title><p>We aim to introduce in this section the semiotic framework used to analyze visual representations (saliency maps) of multi-layered neural networks. Our main operation is aggregation, applied layer-wise in such networks. The basic computational tool is information theory, but the aggregation operation is applied in a semiotic framework and this makes our contribution interdisciplinary.</p><p>In semiotics (or <italic>semiosis</italic>), a <italic>sign</italic> is anything that communicates a meaning that is not the sign itself, to the interpreter of the sign. This definition is very general. Alternative in-depth definitions can be found in [<xref rid="B20-entropy-22-01365" ref-type="bibr">20</xref>,<xref rid="B21-entropy-22-01365" ref-type="bibr">21</xref>,<xref rid="B22-entropy-22-01365" ref-type="bibr">22</xref>]. We consider the triadic model of semiosis, as stated by Charles Sanders Peirce. Peirce defined semiosis as an irreducible triadic relation between Sign&#x02013;Object&#x02013;Interpretant [<xref rid="B23-entropy-22-01365" ref-type="bibr">23</xref>].</p><p>Charles Morris [<xref rid="B24-entropy-22-01365" ref-type="bibr">24</xref>] defined semiotics as grouped into three branches:<list list-type="bullet"><list-item><p>Syntactics: relations among or between signs in formal structures without regard to meaning.</p></list-item><list-item><p>Semantics: relation between signs and the things to which they refer: their signified denotata, or meaning.</p></list-item><list-item><p>Pragmatics: relations between the sign system and its human (or animal) user.</p></list-item></list></p><p>In a simplistic manner, semiotics already played some role in computer science during the sixties. The distinction of syntactics, semantics, and pragmatics by Charles Morris was at that time imported into programming language theory [<xref rid="B25-entropy-22-01365" ref-type="bibr">25</xref>]. More recent results can be found in [<xref rid="B26-entropy-22-01365" ref-type="bibr">26</xref>].</p><p>Computational semiotics is built upon a mathematical description of concepts from classic semiotics. In [<xref rid="B27-entropy-22-01365" ref-type="bibr">27</xref>], it was stated that semantic networks can implement computational intelligence models: fuzzy systems, neural networks, and evolutionary computation algorithms. Later, some computational model of Peirce&#x02019;s triadic notion of meaning processes were proposed [<xref rid="B15-entropy-22-01365" ref-type="bibr">15</xref>,<xref rid="B28-entropy-22-01365" ref-type="bibr">28</xref>,<xref rid="B29-entropy-22-01365" ref-type="bibr">29</xref>].</p><p>Taking advantage of Peircean semiotics and recent results in cognitive science, Baxter et al. proposed a unified framework for the interpretation of medical image segmentation as a sign exchange in which each sign acts as an interface metaphor [<xref rid="B30-entropy-22-01365" ref-type="bibr">30</xref>]. This framework provides a unified approach to the understanding and development of medical image segmentation interfaces. A complete computational model of Peirce&#x02019;s semiosis is very complex and still not available.</p><p>According to Mihai Nadin, almost all inference engines deployed today in machine learning encode semiotic elements, although, at times, those who designed them are rather driven by semiotic intuition than by semiotic knowledge [<xref rid="B31-entropy-22-01365" ref-type="bibr">31</xref>,<xref rid="B32-entropy-22-01365" ref-type="bibr">32</xref>].</p><p>Recently, there is a huge interest in self-explaining machine learning models. This can be regarded as exposure of the self-interpretation and semiotic awareness mechanism. The concept of sign and semiotics offers a very promising and tempting conceptual basis to machine learning.</p><p>In this work, we focus on computational aspects of semiotics in deep learning. Our semiotic infrastructure is at the intersection of Peirce&#x02019;s theory and information theory, a theory developed by Max Bense [<xref rid="B33-entropy-22-01365" ref-type="bibr">33</xref>] and Helmar Frank [<xref rid="B34-entropy-22-01365" ref-type="bibr">34</xref>].</p><p>The usual signs designate material entities which are unconsciously perceived. These so-called <italic>first level signs</italic> may be agglomerated into signs at the next hierarchical level, called <italic>second level supersigns</italic>. Iterating the process, we obtain more abstract <italic>k-th level supersigns</italic>. The transition from <italic>k</italic>-th level to <inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>-th level supersigns is called <italic>superization</italic>. Frank [<xref rid="B34-entropy-22-01365" ref-type="bibr">34</xref>] identified two types of superization:<list list-type="order"><list-item><p><bold>Type I</bold> &#x0201c;Durch Klassesbildung&#x0201d; (by class formation, in German): building equivalence classes and thus reducing the number of signs. The letters of a text may be considered first level signs. The equivalence class of all types of letter &#x0201c;a&#x0201d; (handwritten, capital, and so on) is a second level supersign.</p></list-item><list-item><p><bold>Type II</bold> &#x0201c;Durch Komplexbildung&#x0201d; (by compound formation, in German): building compound supersigns from simpler component supersigns. Reconsidering the previous example, we may obtain this way words from letters, sentences from words, and more and more complex and abstract syntactic-semantic structures afterward.</p></list-item></list></p><p>Superization is a semiotic aggregation process characterized at each perception level by a specific repertory of supersigns. Hierarchical computer vision data structures (e.g., quadtrees, multi-resolution pyramids) may be considered simplistic superizations [<xref rid="B35-entropy-22-01365" ref-type="bibr">35</xref>,<xref rid="B36-entropy-22-01365" ref-type="bibr">36</xref>]. The basic idea is to treat each component as a pixel at the given hierarchical level. In this case, there is a similarity between hierarchical aggregative representation and superization processes. However, there are also differences: superizations are not simple combinatorial processes, but subtle syntactic-semantic perception frames related to Peirce&#x02019;s triadic model of semiosis.</p><p>A multi-resolution image representation can be characterized at each level by an information measure. The resolution-dependent Shannon entropy can be derived from the probability distribution of grey-level events observed at that level [<xref rid="B37-entropy-22-01365" ref-type="bibr">37</xref>]. Using the newspaper&#x02019;s reading analogy, at the magnified level, where only white and black patches are visible, the entropy <italic>H</italic> will be low. As the picture is brought to normal focusing distance, a great variety of grey levels become apparent, and, consequently, the entropy increases. As the picture is moved further away from the eyes, the entropy decreases. Finally, it may become nearly uniformly grey in appearance, with <inline-formula><mml:math id="mm34"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x02248;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The observation that associates with the peak value of the entropy is one of the most meaningful observations of the picture. However, because of other factors, the maximum entropy is not always associated with the &#x0201c;optimal&#x0201d; resolution [<xref rid="B36-entropy-22-01365" ref-type="bibr">36</xref>].</p><p>From an informational psychology view, the entropy increases until it reaches its peak value. In our opinion [<xref rid="B35-entropy-22-01365" ref-type="bibr">35</xref>,<xref rid="B36-entropy-22-01365" ref-type="bibr">36</xref>], this phase may be associated with the informational adaptation of the perceiver. The subsequent entropy decrease is related to the processing of structural information [<xref rid="B37-entropy-22-01365" ref-type="bibr">37</xref>]. The rate of decrease depends largely upon the amount of structural information in the picture. The entropy falls quickly when little structural information is available, whereas, when major structural information is present, the entropy will remain high over most of its range. The variation of entropy can indicate the type and quantity of structural information in the picture in terms of size and relationships to detailed features. In the current study, we focus only on the entropy decrease phase, since the analyzed CNNs do not adapt to the inputs by changing dynamically the input image resolution.</p><p>The idea of considering the CNN layers as multi-resolution representations of the input images is interesting, but not very new [<xref rid="B38-entropy-22-01365" ref-type="bibr">38</xref>,<xref rid="B39-entropy-22-01365" ref-type="bibr">39</xref>,<xref rid="B40-entropy-22-01365" ref-type="bibr">40</xref>]. For instance, in [<xref rid="B38-entropy-22-01365" ref-type="bibr">38</xref>] a spatial pyramid pooling layer is introduced between convolutional layers and fully connected layers to avoid the need for cropping or warping of the input images. In [<xref rid="B40-entropy-22-01365" ref-type="bibr">40</xref>], the incoming convolution layers at multiple sampling rates are applied to the convolutional layers to capture objects as well as image context at multiple scales.</p><p>In our approach, we consider the multi-resolution image representation example in the context of a semiotic recognition process, where the machine (or the interpretant) attempts to classify an input image. We imagine the recognition process as a feedforward multi-layer neural classifier where each layer performs a superization of the previous layer. We assume that the subjective information (measured by the entropy) is made available to an interpretant (i.e., the computer or the human supervisor) who attempts to classify the input image.</p><p>Let us consider the entropies computed at two successive layers: <inline-formula><mml:math id="mm35"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm36"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The extracted information by the interpretant can be measured by the difference <inline-formula><mml:math id="mm37"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Details can be found in [<xref rid="B41-entropy-22-01365" ref-type="bibr">41</xref>]. We have the following result:</p><statement><label><bold>Theorem</bold>&#x000a0;<bold>1</bold></label><p>(from [<xref rid="B34-entropy-22-01365" ref-type="bibr">34</xref>])<bold>.</bold>
<italic>: Superization tends to concentrate information by decreasing entropy.</italic></p></statement><statement><label><bold>Proof of Theorem 1.</bold>&#x000a0;</label><p>We consider separately the two types of superization. For a set <inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi>Z</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of supersigns with the corresponding probabilities <inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, using a superization of the first type, we may obtain supersigns of the next level <inline-formula><mml:math id="mm41"><mml:mrow><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi>Z</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with the corresponding probabilities <inline-formula><mml:math id="mm42"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. We have the following inequality: <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="0.222222em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.222222em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For two sets of supersigns <italic>X</italic> and <italic>Y</italic>, using the second type of superization, we obtain compound supersigns from the joint set <inline-formula><mml:math id="mm44"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A well-known relation completes the proof: <inline-formula><mml:math id="mm45"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02265;</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.&#x02003;&#x025a1;</p></statement><p>An intuitive application of this theorem is when we consider the neural layers of a CNN. A type I superization appears when we reduce the spatial resolution of a layer <inline-formula><mml:math id="mm46"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> by subsampling layer <italic>k</italic>. This is similar to class formation because we reduce the variation of the input values (i.e., we reduce the number of signs). In CNNs, this is typically performed by a pooling operator. The pooling operator can be considered as a form of nonlinear down-sampling which partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, it computes its mean (average pooling) or max value (max pooling). The formula for max pooling applied to a feature map <italic>F</italic> at layer <italic>k</italic> and locations <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a kernel of 2 &#x000d7; 2 is:<disp-formula id="FD7-entropy-22-01365"><label>(7)</label><mml:math id="mm48"><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A type II superization is produced when applying a convolutional operator to a neural layer <italic>k</italic>. As an effect, layer <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> will focus on more complex objects, composed of objects already detected by layer <italic>k</italic>. The convolutional operator for a feature map <italic>F</italic> at layer <italic>k</italic> and pixel locations <inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with a 3 &#x000d7; 3 kernel <italic>W</italic> has the following formula:<disp-formula id="FD8-entropy-22-01365"><label>(8)</label><mml:math id="mm51"><mml:mrow><mml:mrow><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The output <italic>O</italic> of the convolutional operator is a linear combination of the input features and the learned kernel weights. Thus, a resulting neuron will be able to detect a combination of simpler object forming a more complex one, by a composition of supersigns.</p><p>We observe that the effect of superization is a tendency of entropy decrease at each level. This is different than in the case of multi-resolution image representation. In [<xref rid="B36-entropy-22-01365" ref-type="bibr">36</xref>], we explained this difference by the following thesis: &#x0201c;The first level signs are perceived at a complexity level which corresponds to the &#x0201c;optimal&#x0201d; resolution.&#x0201d; However, this thesis does not apply to a computer recognition model (a classifier), but to human perception.</p><p>In a simplified form, a multi-layered classifier can be interpreted from Morris&#x02019; semiotic theory as a transition: syntactics&#x02013;semantics&#x02013;pragmatics. At the end of a successful recognition process, the entropy of the output layer becomes 0 and no further information needs to be extracted. The last layer (the fully connected layer in a CNN network) is connected to the outer world, the world of objects. This may be considered the pragmatic level in Morris&#x02019; semiotic theory, since it shows the relation between the input signs and the output objects which can be related to decisions and actions.</p></sec><sec id="sec5-entropy-22-01365"><title>5. Signs and Supersigns in CNN Saliency Maps</title><p>Theorem 1 is a simplification of the superization processes taking place in the successive layers of saliency maps. We have both class formation and the compound formation superization, and the computed entropy is spatial. We calculate superizations at the level of saliency maps. In other words, our signs and supersigns refer to values computed in successive saliency maps computed by the Grad-CAM method.</p><p>Our hypothesis is that, at the core of a CNN, both types of superizations exist. For type I superization (by class formation), the pooling operation combines signs (scalar values) by criteria like average value or maximum value, resulting in a single sign, and thus reducing their number and building equivalence classes. Another potential interpretation of the pooling operation is that it builds equivalence classes by grouping spatially neighboring elements. In our experiments (as we will see in <xref ref-type="sec" rid="sec6-entropy-22-01365">Section 6</xref>), this phenomenon could be noticed after each pooling layer, where the magnitude of the spatial entropy of the saliency maps would have a big drop. Visually, the saliency maps start to become more concentrated around connected regions as more complex signs are formed.</p><p>For type II superization (by compound formation), it is known that CNNs compose whole objects starting from simple object parts [<xref rid="B9-entropy-22-01365" ref-type="bibr">9</xref>]. This phenomenon describes exactly the second type of superization, as it builds compound supersigns from simpler component supersigns. They manage to do so by gradually enlarging the receptive field after each convolutional layer is applied. As the receptive field grows, a single neuron inside a hidden layer can cover a much larger region of interest from the input image and thus get activated for more and more complex objects.</p><p>What complicates the interpretation in case of CNN networks is the fact that for some layers both superizations operate simultaneously, and it can be difficult to separate their effects.</p><p>Our hypothesis is that, in order to decrease the spatial entropy noticeably, the first type of superization is more effective, while the second type is more responsible with building supersigns with semantic roles, not affecting spatial entropy that much.</p></sec><sec id="sec6-entropy-22-01365"><title>6. Experiments</title><p>The goal for the next experiments is to explore the variation of the spatial entropy of the saliency maps computed with Grad-CAM on some representative CNN architectures. We expect the entropy to decrease along with depth, and this can be related to type I superization processes.</p><p>We consider three standard network architectures: AlexNet [<xref rid="B2-entropy-22-01365" ref-type="bibr">2</xref>], VGG16 [<xref rid="B3-entropy-22-01365" ref-type="bibr">3</xref>], and ResNet50 [<xref rid="B5-entropy-22-01365" ref-type="bibr">5</xref>]. In addition, we also study the entropy variation on a custom LeNet-5-like network (The original LeNet-5 was introduced in [<xref rid="B1-entropy-22-01365" ref-type="bibr">1</xref>]).</p><p>We use the deep learning programming framework PyTorch [<xref rid="B42-entropy-22-01365" ref-type="bibr">42</xref>] (version 1.4.0) and the public implementation of Grad-CAM (<uri xlink:href="https://github.com/utkuozbulak/pytorch-cnn-visualizations">https://github.com/utkuozbulak/pytorch-cnn-visualizations</uri>), modified to our needs. Except the custom network, all CNNs are used as provided by the PyTorch repository, with their default pretrained weights.</p><p>The experiments are performed in different contexts on the following datasets:<list list-type="order"><list-item><p>A subset of ImageNet [<xref rid="B43-entropy-22-01365" ref-type="bibr">43</xref>] composed of the &#x0201c;beaver&#x0201d; class from the training set, to test the pretrained and randomly initialized use-cases.</p></list-item><list-item><p>CIFAR-10 [<xref rid="B44-entropy-22-01365" ref-type="bibr">44</xref>] to: <italic>(a)</italic> train the custom network without downsampling; and <italic>(b)</italic> test the newly trained network and a randomly initialized one, with the same architecture, using this dataset as a test set.</p></list-item><list-item><p>&#x0201c;kangaroo&#x0201d; class from Caltech101 [<xref rid="B45-entropy-22-01365" ref-type="bibr">45</xref>] to test a network pretrained on ImageNet. The fact that we train and test on different (but somehow similar) datasets can have an impact on the generalization performance of the network and expose possible overfitting on the training data. This is known as <italic>zero-shot</italic> learning, and it can be viewed as an extreme case of domain adaptation.</p></list-item><list-item><p>Caltech101 [<xref rid="B45-entropy-22-01365" ref-type="bibr">45</xref>] to test for the case where the network is pretrained on ImageNet, then trained (fine-tuned) on Caltech101. This is the <italic>transfer learning</italic> approach.</p></list-item></list></p><sec id="sec6dot1-entropy-22-01365"><title>6.1. Experiments on Standard CNN Architectures</title><p>We present the experimental results for each of the considered CNN architectures. In the next tables, we use the following terms: (i) Pretrained&#x02014;publicly available pretrained weights on ImageNet, (ii) Random&#x02014;randomly initialized weights, (iii) Fine-tuning&#x02014;fine-tuned weights starting from the pretrained ones trained on ImageNet, (iv) ImageNet&#x02014;&#x0201c;beaver&#x0201d; class from the ImageNet training set, (v) Caltech101&#x02014;&#x0201c;kangaroo&#x0201d; class from the Caltech101 training set.</p><p>AlexNet [<xref rid="B2-entropy-22-01365" ref-type="bibr">2</xref>] is composed of a sequence of convolutional, max-pooling, and ReLU layers, followed at the end by fully connected layers which linearly project the extracted features from the convolutional backbone to the desired number of output classes. <xref rid="entropy-22-01365-t001" ref-type="table">Table 1</xref> captures the experimental result values for each layer of the network.</p><p>VGG16 [<xref rid="B3-entropy-22-01365" ref-type="bibr">3</xref>] has a relatively simple and compact architecture, consisting of only <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions, max-pooling, and ReLU, followed by multiple fully connected layers. The trick behind the VGG16 architecture is to use two <inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> sequential convolution to replace a bigger <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> one, thus obtaining the same receptive field coverage by using less parameters. The caveat of VGG16 is that most of its parameters reside in the fully connected layers, making the network very parameter and memory inefficient. <xref rid="entropy-22-01365-t002" ref-type="table">Table 2</xref> depicts the entropy values at different levels of the network.</p><p>The novelty of ResNet [<xref rid="B5-entropy-22-01365" ref-type="bibr">5</xref>] stands in the residual connections which alleviate the vanishing gradient problem, an issue that followed deep neural networks since their early days. During backpropagation, gradients would start to gradually decrease in magnitude because of the chain rule applied to very small values, until they become 0, and, consequently, many layers would lack any gradient signal on which basis to update their respective weights. ResNet solves this problem by creating residual branches from an input block to an output block in the form of <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>x</italic> is the block&#x02019;s input and <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a sequence of multiple layers. Instead of learning a function, as in earlier architectures like AlexNet or VGG16, ResNets are trying to learn a residual for the input <italic>x</italic>, hence the name of the architecture. Entropy values for various layers are shown in <xref rid="entropy-22-01365-t003" ref-type="table">Table 3</xref>.</p><p>For all three networks, we observe a tendency of the spatial entropy to decrease, especially after max-pooling layers, which in our hypothesis are layers responsible for type I superization. Type II superization can be noticed by applying multiple consecutive convolutional layers. In this case, the spatial entropy does not necessarily decrease, but the general purpose is to enlarge the receptive field of the network, such that neurons activate for more complex objects while progressing through the layers.</p><p>Considering our above experiments and the well known fact that CNNs compose complex objects starting from simpler ones, this supports our hypothesis that type I superization is more effective for the entropy decrease. We did not notice a systematic entropy decrease for type II superization, and conclude that it is more responsible for building supersigns with semantic roles.</p><p>To prove the benefic effects of transfer learning when fine-tuning, we also train starting from a random initialization. We use the Caltech101 dataset [<xref rid="B45-entropy-22-01365" ref-type="bibr">45</xref>], since it consists of real images like the ones in ImageNet. The results for fine-tuning and training from scratch are available in <xref rid="entropy-22-01365-t004" ref-type="table">Table 4</xref>. For both experiments, we use a learning rate of <inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and train the networks for 100 epochs. The training set consists of the full dataset, apart from five random samples for each class, which are held for testing. The results when training from scratch are clearly worse than when fine-tuning from a strong baseline. Since the new training dataset is very small (&#x02248;9000 samples) compared to the size of ImagetNet (&#x02248;1.3 M samples), the network overfits on the training samples. This explains the weak performance when training from scratch.</p></sec><sec id="sec6dot2-entropy-22-01365"><title>6.2. Experiments on a Custom Network</title><p>Since all standard CNNs use a form of downsampling, either through strided convolutions or pooling, we notice that type I superization is always present. In these standard CNNs, both superization types are simultaneously present. The question is how to isolate the type II superization from the type I superization.</p><p>For this, we create a custom network by removing all spatial subsampling operations (strided convolutions and max-poolings) from original LeNet-5. This way, we remove the type I superization (class formation) and analyze entropy variation with respect to type II superization (compound formation) only.</p><p>We add two more convolutional layers to increase the receptive field of the network such that it can build more complex type II supersigns and simply have more layers to study the spatial entropy. The architectural details are depicted in <xref rid="entropy-22-01365-t005" ref-type="table">Table 5</xref>.</p><p>We train this network on CIFAR-10 for 20 epochs, with the Stochastic Gradient Descent (SGD) optimizer and a learning rate of <inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, until it reaches &#x02248;72% accuracy on the test set, and then use it to generate the saliency maps. The accuracy performance is less relevant, since in this experiment we focus on the variation of the entropy. In <xref rid="entropy-22-01365-t006" ref-type="table">Table 6</xref>, we can observe that the entropy does not vary too much for both the pretrained and random versions, but the random one exhibits much larger values.</p></sec></sec><sec id="sec7-entropy-22-01365"><title>7. CNN Architecture Optimization</title><p>It is known that modern neural network architectures are overparametrized [<xref rid="B46-entropy-22-01365" ref-type="bibr">46</xref>], and so, an important emerging trend in deep learning is the optimization of such deep neural networks to satisfy various hardware constraints. An overview of such optimization techniques can be found in [<xref rid="B47-entropy-22-01365" ref-type="bibr">47</xref>,<xref rid="B48-entropy-22-01365" ref-type="bibr">48</xref>]. Among them, pruning is regarded as a fundamental method which has been studied since the late 1980s [<xref rid="B49-entropy-22-01365" ref-type="bibr">49</xref>], and consists of reducing redundant operations by means of removing unnecessary or weak connections at the level of weights or layers. In the last couple of years, the state-of-the-art pruning methods have advanced considerably and are now capable of reducing the computational overhead of a deep neural network by a few times without incurring any loss in accuracy [<xref rid="B50-entropy-22-01365" ref-type="bibr">50</xref>].</p><p>The experiments described in <xref ref-type="sec" rid="sec6-entropy-22-01365">Section 6</xref> showed that the spatial entropy of the CNN saliency maps generally decreases layer by layer, and we can relate this to semiotic superization. We aim to show how this interpretation could also help to optimize (or simplify) the architecture of the network. We perform an ablation study to see if we can determine redundant layers for pruning based on the spatial entropy information of the saliency maps. It is beyond the scope of this paper to systematically compare our approach with other CNN architecture optimization techniques. We only explore this area as a proof-of-concept, since it is the first time that such a semiotic method is used for neural architecture optimization.</p><p>On the VGG16 network, we iteratively apply the following greedy algorithm: (i) train the network on CIFAR-10 using the SGD optimizer with a learning rate of <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (ii) compute the spatial entropy for each saliency map; (iii) remove a layer for which the entropy does not decrease; and (iv) repeat steps (i)&#x02013;(iii) until the performance does not degrade too much.</p><p>From the results (see <xref ref-type="fig" rid="entropy-22-01365-f002">Figure 2</xref>), we notice that up to eight convolutional layers can be completely removed from the network, and this affects the performance by less than <inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. When removing the 9th layer, the accuracy decreases significantly; therefore, we stop the iterative process at this stage.</p><p>An interesting finding is that the order in which we remove layers matters significantly. If small layers with few parameters from the beginning of the network are removed first, the accuracy goes down by <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> after the 3rd removal. When removing from the big (over-parametrized) layers starting from the mid-end level of the network, the accuracy is maintained. The accuracy degrades especially fast after the 2nd convolutional layer with 64 output channels being removed.</p><p>Our explanation is that the first two convolutional layers are crucial for the downstream performance of the network. This first part of a network, before a subsampling operation is applied, is known in the literature as stem [<xref rid="B51-entropy-22-01365" ref-type="bibr">51</xref>]. Some variants of ResNets implement this stem as three <inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional layers or a big <inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> layer. These early layers are responsible with detecting low level features like edge detectors. Having only a <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional layer, instead of two or three, means that the receptive field before the first max-pooling operation is <inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, which might be too small to properly detect basic strokes and edges.</p><p>The resulted network has the following configuration: 64, 64, M, 128, M, 256, M, 512, M, M, where &#x0201c;M&#x0201d; stands for max-pooling and the integers represent a convolutional layer with the respective number of output channels, followed by a ReLU nonlinearity. The fully connected layers do not change from the original architecture. We compare our resulted network with VGG11, which is the smallest architecture from the VGG family. The results are displayed in <xref rid="entropy-22-01365-t007" ref-type="table">Table 7</xref>. It can be noticed that, even when reducing the network capacity by a factor of approximately <inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:mn>7.5</mml:mn><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the accuracy is still maintained, meaning that the network is too over-parametrized for this task.</p><p>To check that the configuration translates to other tasks as well, we also trained the network on CIFAR-100 and compared it with the full VGG16&#x02019;s performance. For the full VGG16 network, we obtain an accuracy of <inline-formula><mml:math id="mm67"><mml:mrow><mml:mrow><mml:mn>62.61</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, whereas for the optimized VGG architecture we get <inline-formula><mml:math id="mm68"><mml:mrow><mml:mrow><mml:mn>63.78</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As can be seen, the small network even slightly improves the performance of the full network, while being much smaller.</p><p>We can visualize (<xref ref-type="fig" rid="entropy-22-01365-f003">Figure 3</xref>) this iterative removal experiment by plotting the saliency maps for a CIFAR-10 image from the &#x0201c;truck&#x0201d; class at different key layers, where the spatial entropy value saw a large drop from the previous layer, starting from the full VGG16 network and removing one layer at a time. The rows in <xref ref-type="fig" rid="entropy-22-01365-f003">Figure 3</xref> represent layers at a particular depth, while the columns different architecture configurations found by the iterative method described above. In compliance with the theory of semiotic superization, it is visible how supersigns are gradually formed, layer by layer, from simpler supersigns. We observe this phenomenon from the fact that yellow regions (which denote pixel importance) become more structured and connected as we traverse through the layers. If we compare the saliency maps from the first column (corresponding to the full network) to the ones preceding them, we notice that the overall structure is maintained across all architecture configurations. This suggests that semiotic superization takes place inside a deep neural network regardless of the architecture of the network.</p></sec><sec sec-type="conclusions" id="sec8-entropy-22-01365"><title>8. Conclusions</title><p>We introduced a novel computational semiotics interpretation of CNN architectures based on the statistical aspects of the information concentration processes (semiotic superizations) which appear in the saliency maps of successive CNN layers. At the core of a CNN, the two types of superization co-exist. According to our results, the first type of superization is effective at decreasing the spatial entropy. Type II superization is more responsible for building supersigns with semantic roles.</p><p>Beyond the exploratory aspect of our work, our main insights are twofold. On the knowledge extraction side, the obtained interpretation can be used to visualize and explain decision processes within CNN models. On the neural model optimization side, the question is how to use the semiotic information extracted from saliency maps to optimize the architecture of the CNN. We were able to significantly simplify the architecture of a CNN employing a semiotic greedy technique. While this optimization process can be slow, our work tries to use the notion of computational semiotics to prune an existing state-of-the-art network in a top-down approach instead of constructing one using a bottom-up approach like neural architecture search. Thorough analysis has to be done in future work to consider other network architectures and robustness of the method.</p><p>Some computational improvements for calculating the spatial entropy were proposed by Razlighi et al. [<xref rid="B52-entropy-22-01365" ref-type="bibr">52</xref>,<xref rid="B53-entropy-22-01365" ref-type="bibr">53</xref>]. The computational overhead can be significantly reduced if we accept a reduction of the approximation accuracy. We plan to use this trick in the future.</p><p>In this work, we considered only one type of neural network topology: CNNs. Since CNNs are mostly suited for images, those became the subject of our study. In the future, we intend to study the connection with other fields (audio, text) and architecture types (recurrent neural networks). The semiotic approach can be extended to other deep learning models, since semiotic superization appears to be present in many architectures. The computational semiotics approach is very promising especially for the explanation and optimization of deep networks, where multiple levels of superization are implied.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes><title>Author Contributions</title><p>Investigation, B.M. and R.A.; Project administration, B.M. and R.A.; Supervision, R.A.; Visualization, B.M.; Writing&#x02014;review &#x00026; editing, B.M. and R.A. The authors have equally contributed to the published work. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Funding</title><p>The costs to publish in open access were covered by Xperi Corporation.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-entropy-22-01365"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Haffner</surname><given-names>P.</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc. IEEE</source><year>1998</year><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="B2-entropy-22-01365"><label>2.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group><article-title>ImageNet Classification with Deep Convolutional Neural NetworksImagenet Classification with Deep Convolutional Neural Networks</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B3-entropy-22-01365"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">abs/1409.1556</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B4-entropy-22-01365"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>Y.</given-names></name><name><surname>Sermanet</surname><given-names>P.</given-names></name><name><surname>Reed</surname><given-names>S.E.</given-names></name><name><surname>Anguelov</surname><given-names>D.</given-names></name><name><surname>Erhan</surname><given-names>D.</given-names></name><name><surname>Vanhoucke</surname><given-names>V.</given-names></name><name><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group><article-title>Going deeper with convolutions</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">abs/1409.4842</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B5-entropy-22-01365"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">abs/1512.03385</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B6-entropy-22-01365"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>M.</given-names></name><name><surname>Le</surname><given-names>Q.V.</given-names></name></person-group><article-title>EfficientNet: Rethinking model scaling for convolutional neural networks</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">abs/1905.11946</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.11946">https://arxiv.org/abs/1905.11946</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B7-entropy-22-01365"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selvaraju</surname><given-names>R.R.</given-names></name><name><surname>Das</surname><given-names>A.</given-names></name><name><surname>Vedantam</surname><given-names>R.</given-names></name><name><surname>Cogswell</surname><given-names>M.</given-names></name><name><surname>Parikh</surname><given-names>D.</given-names></name><name><surname>Batra</surname><given-names>D.</given-names></name></person-group><article-title>Grad-CAM: Why did you say that? Visual explanations from deep networks via gradient-based localization</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">abs/1610.02391</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1610.02391">https://arxiv.org/abs/1610.02391</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B8-entropy-22-01365"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Vedaldi</surname><given-names>A.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Deep inside convolutional networks: Visualising image classification models and saliency maps</article-title><source>arXiv</source><year>2013</year><pub-id pub-id-type="arxiv">abs/1312.6034</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6034f">https://arxiv.org/abs/1312.6034f</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B9-entropy-22-01365"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>M.D.</given-names></name><name><surname>Fergus</surname><given-names>R.</given-names></name></person-group><article-title>Visualizing and understanding convolutional networks</article-title><source>arXiv</source><year>2013</year><pub-id pub-id-type="arxiv">abs/1311.2901</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B10-entropy-22-01365"><label>10.</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Springenberg</surname><given-names>J.</given-names></name><name><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name><name><surname>Riedmiller</surname><given-names>M.</given-names></name></person-group><article-title>Striving for Simplicity: The All Convolutional Net</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://lmb.informatik.uni-freiburg.de/Publications/2015/DB15a">http://lmb.informatik.uni-freiburg.de/Publications/2015/DB15a</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B11-entropy-22-01365"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smilkov</surname><given-names>D.</given-names></name><name><surname>Thorat</surname><given-names>N.</given-names></name><name><surname>Kim</surname><given-names>B.</given-names></name><name><surname>Vi&#x000e9;gas</surname><given-names>F.B.</given-names></name><name><surname>Wattenberg</surname><given-names>M.</given-names></name></person-group><article-title>SmoothGrad: Removing noise by adding noise</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">abs/1706.03825</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.03825">https://arxiv.org/abs/1706.03825</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B12-entropy-22-01365"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B.</given-names></name><name><surname>Khosla</surname><given-names>A.</given-names></name><name><surname>Lapedriza</surname><given-names>&#x000c0;.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group><article-title>Learning deep features for discriminative localization</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">abs/1512.04150</pub-id><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1512.04150">https://arxiv.org/abs/1512.04150</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-11-30">(accessed on 30 November 2020)</date-in-citation></element-citation></ref><ref id="B13-entropy-22-01365"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahdi</surname><given-names>A.</given-names></name><name><surname>Qin</surname><given-names>J.</given-names></name><name><surname>Crosby</surname><given-names>G.</given-names></name></person-group><article-title>DeepFeat: A bottom-up and top-down saliency model based on deep features of convolutional neural networks</article-title><source>IEEE Trans. Cogn. Dev. Syst.</source><year>2020</year><volume>12</volume><fpage>54</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1109/TCDS.2019.2894561</pub-id></element-citation></ref><ref id="B14-entropy-22-01365"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Adebayo</surname><given-names>J.</given-names></name><name><surname>Gilmer</surname><given-names>J.</given-names></name><name><surname>Muelly</surname><given-names>M.</given-names></name><name><surname>Goodfellow</surname><given-names>I.</given-names></name><name><surname>Hardt</surname><given-names>M.</given-names></name><name><surname>Kim</surname><given-names>B.</given-names></name></person-group><article-title>Sanity checks for saliency maps</article-title><source>Proceedings of the Advances in Neural Information Processing Systems</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>3&#x02013;8 December 2018</conf-date><fpage>9525</fpage><lpage>9536</lpage></element-citation></ref><ref id="B15-entropy-22-01365"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gudwin</surname><given-names>R.</given-names></name><name><surname>Queiroz</surname><given-names>J.</given-names></name></person-group><article-title>Towards an introduction to computational semiotics</article-title><source>Proceedings of the International Conference on Integration of Knowledge Intensive Multi-Agent Systems</source><conf-loc>Waltham, MA, USA</conf-loc><conf-date>18&#x02013;21 April 2005</conf-date><fpage>393</fpage><lpage>398</lpage></element-citation></ref><ref id="B16-entropy-22-01365"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y.</given-names></name><name><surname>Fang</surname><given-names>B.</given-names></name><name><surname>Tang</surname><given-names>Y.</given-names></name></person-group><article-title>A computational model for saliency maps by using local entropy</article-title><source>Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence</source><conf-loc>Atlanta, GA, USA</conf-loc><conf-date>11&#x02013;15 July 2010</conf-date><fpage>967</fpage><lpage>973</lpage></element-citation></ref><ref id="B17-entropy-22-01365"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Journel</surname><given-names>A.G.</given-names></name><name><surname>Deutsch</surname><given-names>C.V.</given-names></name></person-group><article-title>Entropy and spatial disorder</article-title><source>Math. Geol.</source><year>1993</year><volume>25</volume><fpage>329</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1007/BF00901422</pub-id></element-citation></ref><ref id="B18-entropy-22-01365"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Volden</surname><given-names>E.</given-names></name><name><surname>Giraudon</surname><given-names>G.</given-names></name><name><surname>Berthod</surname><given-names>M.</given-names></name></person-group><article-title>Modelling image redundancy</article-title><source>Proceedings of the 1995 International Geoscience and Remote Sensing Symposium</source><conf-loc>Firenze, Italy</conf-loc><conf-date>10&#x02013;14 July 1995</conf-date><fpage>2148</fpage><lpage>2150</lpage></element-citation></ref><ref id="B19-entropy-22-01365"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razlighi</surname><given-names>Q.R.</given-names></name><name><surname>Kehtarnavaz</surname><given-names>N.</given-names></name></person-group><article-title>A comparison study of image spatial entropy</article-title><source>Visual Commun. Image Process.</source><year>2009</year><volume>7257</volume><fpage>1</fpage></element-citation></ref><ref id="B20-entropy-22-01365"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eco</surname><given-names>U.</given-names></name></person-group><source>A Theory of Semiotics</source><publisher-name>Indiana University Press</publisher-name><publisher-loc>Indiana, IN, USA</publisher-loc><year>1976</year></element-citation></ref><ref id="B21-entropy-22-01365"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sebeok</surname><given-names>T.</given-names></name></person-group><source>Signs: An Introduction to Semiotics</source><publisher-name>University of Toronto Press</publisher-name><publisher-loc>Toronto, ON, Canada</publisher-loc><year>1994</year></element-citation></ref><ref id="B22-entropy-22-01365"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chandler</surname><given-names>D.</given-names></name></person-group><source>Semiotics: The Basics</source><publisher-name>Taylor &#x00026; Francis Group</publisher-name><publisher-loc>Abingdon-on-Thames, UK</publisher-loc><year>2017</year></element-citation></ref><ref id="B23-entropy-22-01365"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>C.S.</given-names></name></person-group><source>Collected Papers of Charles Sanders Peirce</source><publisher-name>Harvard University Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>1960</year></element-citation></ref><ref id="B24-entropy-22-01365"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>C.</given-names></name><name><surname>Charles</surname><given-names>M.</given-names></name></person-group><source>Writings on the General Theory of Signs</source><publisher-name>Walter de Gruyter</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><publisher-loc>Boston, MA, USA</publisher-loc><year>1972</year></element-citation></ref><ref id="B25-entropy-22-01365"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zemanek</surname><given-names>H.</given-names></name></person-group><article-title>Semiotics and programming languages</article-title><source>Commun. ACM</source><year>1966</year><volume>9</volume><fpage>139</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1145/365230.365249</pub-id></element-citation></ref><ref id="B26-entropy-22-01365"><label>26.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tanaka-Ishii</surname><given-names>K.</given-names></name></person-group><source>Semiotics of Programming</source><publisher-name>Cambridge University Press</publisher-name><publisher-loc>Cambridge, UK</publisher-loc><year>2010</year></element-citation></ref><ref id="B27-entropy-22-01365"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gudwin</surname><given-names>R.</given-names></name><name><surname>Gomide</surname><given-names>F.</given-names></name></person-group><article-title>A computational semiotics approach for soft computing</article-title><source>Proceedings of the 1997 IEEE International Conference on Systems, Man, and Cybernetics, Computational Cybernetics and Simulation</source><conf-loc>Orlando, FL, USA</conf-loc><conf-date>12&#x02013;15 October 1997</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>1997</year><fpage>3981</fpage><lpage>3986</lpage></element-citation></ref><ref id="B28-entropy-22-01365"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomes</surname><given-names>A.</given-names></name><name><surname>Gudwin</surname><given-names>R.</given-names></name><name><surname>Queiroz</surname><given-names>J.</given-names></name></person-group><article-title>Towards meaning processes in computers from peircean semiotics</article-title><source>SEED J. Semiot. Evol. Energy Dev.</source><year>2003</year><volume>3</volume><fpage>69</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1007/s11299-007-0031-9</pub-id></element-citation></ref><ref id="B29-entropy-22-01365"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gudwin</surname><given-names>R.R.</given-names></name></person-group><article-title>Semiotic synthesis and semionic networks</article-title><source>SEED J. Semiot. Evol. Energy Dev.</source><year>2002</year><volume>2</volume><fpage>55</fpage><lpage>83</lpage></element-citation></ref><ref id="B30-entropy-22-01365"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baxter</surname><given-names>J.S.</given-names></name><name><surname>Gibson</surname><given-names>E.</given-names></name><name><surname>Eagleson</surname><given-names>R.</given-names></name><name><surname>Peters</surname><given-names>T.M.</given-names></name></person-group><article-title>The semiotics of medical image segmentation</article-title><source>Med. Image Anal.</source><year>2018</year><volume>44</volume><fpage>54</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1016/j.media.2017.11.007</pub-id><pub-id pub-id-type="pmid">29190576</pub-id></element-citation></ref><ref id="B31-entropy-22-01365"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadin</surname><given-names>M.</given-names></name></person-group><article-title>Information and semiotic processes: The semiotics of computation</article-title><source>Cybern. Hum. Knowing</source><year>2011</year><volume>18</volume><fpage>153</fpage><lpage>175</lpage></element-citation></ref><ref id="B32-entropy-22-01365"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadin</surname><given-names>M.</given-names></name></person-group><article-title>Semiotic machine</article-title><source>Public J. Semiot.</source><year>2007</year><volume>1</volume><fpage>57</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.37693/pjos.2007.1.8815</pub-id></element-citation></ref><ref id="B33-entropy-22-01365"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bense</surname><given-names>M.</given-names></name></person-group><source>Semiotische Prozesse und Systeme in Wissenschaftstheorie und Design, &#x000c4;sthetik und Mathematik</source><publisher-name>Agis-Verlag</publisher-name><publisher-loc>Baden-Baden, Germany</publisher-loc><year>1975</year></element-citation></ref><ref id="B34-entropy-22-01365"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>H.</given-names></name></person-group><source>Kybernetische Grundlagen der P&#x000e4;dagogik: Eine Einf&#x000fc;hrung in die Informationspsychologie und ihre Philosophischen, Mathematischen und Physiologischen Grundlagen</source><publisher-name>Agis-Verlag</publisher-name><publisher-loc>Baden-Baden, Germany</publisher-loc><year>1969</year></element-citation></ref><ref id="B35-entropy-22-01365"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Andonie</surname><given-names>R.</given-names></name></person-group><article-title>A semiotic approach to hierarchical computer vision</article-title><source>Cybernetics and Systems (Proceedings of the Seventh International Congress of Cybernetics and Systems, London, UK, 7&#x02013;11 September 1987)</source><publisher-name>Thales Publication</publisher-name><publisher-loc>Paris, France</publisher-loc><year>1987</year><fpage>930</fpage><lpage>933</lpage></element-citation></ref><ref id="B36-entropy-22-01365"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andonie</surname><given-names>R.</given-names></name></person-group><article-title>Semiotic aggregation in computer vision</article-title><source>Revue roumaine de linguistique, Cahiers de linguistique th&#x000e9;orique et appliqu&#x000e9;e</source><year>1987</year><volume>24</volume><fpage>103</fpage><lpage>107</lpage></element-citation></ref><ref id="B37-entropy-22-01365"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>A.K.</given-names></name><name><surname>Vogel</surname><given-names>M.A.</given-names></name></person-group><article-title>Resolution-dependent information measures for image analysis</article-title><source>IEEE Trans. Syst. Man Cybern.</source><year>1977</year><volume>7</volume><fpage>49</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1109/TSMC.1977.4309589</pub-id></element-citation></ref><ref id="B38-entropy-22-01365"><label>38.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</article-title><source>Computer Vision&#x02014;ECCV 2014</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberger, Germany</publisher-loc><year>2014</year><fpage>346</fpage><lpage>361</lpage></element-citation></ref><ref id="B39-entropy-22-01365"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kokkinos</surname><given-names>I.</given-names></name></person-group><article-title>Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</article-title><source>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2017</year><fpage>5454</fpage><lpage>5463</lpage></element-citation></ref><ref id="B40-entropy-22-01365"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L.C.</given-names></name><name><surname>Papandreou</surname><given-names>G.</given-names></name><name><surname>Kokkinos</surname><given-names>I.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>Yuille</surname><given-names>A.L.</given-names></name></person-group><article-title>DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id></element-citation></ref><ref id="B41-entropy-22-01365"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stan</surname><given-names>I.</given-names></name><name><surname>Andonie</surname><given-names>R.</given-names></name></person-group><article-title>Cybernetical model of the artist-consumer relationship (in Romanian</article-title><source>Studia Universitatis Babes-Bolyai</source><year>1977</year><volume>2</volume><fpage>9</fpage><lpage>15</lpage></element-citation></ref><ref id="B42-entropy-22-01365"><label>42.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><name><surname>Killeen</surname><given-names>T.</given-names></name><name><surname>Lin</surname><given-names>Z.</given-names></name><name><surname>Gimelshein</surname><given-names>N.</given-names></name><name><surname>Antiga</surname><given-names>L.</given-names></name><etal/></person-group><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><source>Advances in Neural Information Processing Systems 32</source><publisher-name>Curran Associates, Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2019</year><fpage>8026</fpage><lpage>8037</lpage></element-citation></ref><ref id="B43-entropy-22-01365"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Dong</surname><given-names>W.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>L.J.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group><article-title>ImageNet: A Large-Scale Hierarchical Image Database</article-title><source>Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Miami, FL, USA</conf-loc><conf-date>20&#x02013;25 June 2009</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2009</year></element-citation></ref><ref id="B44-entropy-22-01365"><label>44.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><source>Learning Multiple Layers of Features from Tiny Images</source><publisher-name>Citeseer</publisher-name><publisher-loc>Princeton, NJ, USA</publisher-loc><year>2012</year></element-citation></ref><ref id="B45-entropy-22-01365"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fei-Fei</surname><given-names>L.</given-names></name><name><surname>Fergus</surname><given-names>R.</given-names></name><name><surname>Perona</surname><given-names>P.</given-names></name></person-group><article-title>Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</article-title><source>Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop</source><conf-loc>Washington, DC, USA</conf-loc><conf-date>27 June&#x02013;2 July 2004</conf-date><publisher-name>IEEE Computer Society</publisher-name><publisher-loc>Washington, DC, USA</publisher-loc><year>2004</year><fpage>178</fpage></element-citation></ref><ref id="B46-entropy-22-01365"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oymak</surname><given-names>S.</given-names></name><name><surname>Soltanolkotabi</surname><given-names>M.</given-names></name></person-group><article-title>Towards moderate overparameterization: Global convergence guarantees for training shallow neural networks</article-title><source>IEEE J. Sel. Areas Inf. Theory</source><year>2020</year><volume>1</volume><fpage>19844181</fpage><pub-id pub-id-type="doi">10.1109/JSAIT.2020.2991332</pub-id></element-citation></ref><ref id="B47-entropy-22-01365"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Zhou</surname><given-names>P.</given-names></name><name><surname>Zhang</surname><given-names>T.</given-names></name></person-group><article-title>A survey of model compression and acceleration for deep neural networks</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1710.09282</pub-id></element-citation></ref><ref id="B48-entropy-22-01365"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sze</surname><given-names>V.</given-names></name><name><surname>Chen</surname><given-names>Y.H.</given-names></name><name><surname>Yang</surname><given-names>T.J.</given-names></name><name><surname>Emer</surname><given-names>J.S.</given-names></name></person-group><article-title>Efficient processing of deep neural networks: A tutorial and survey</article-title><source>Proc. IEEE</source><year>2017</year><volume>105</volume><fpage>2295</fpage><lpage>2329</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2017.2761740</pub-id></element-citation></ref><ref id="B49-entropy-22-01365"><label>49.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Michael</surname><given-names>C.M.</given-names></name><name><surname>Paul</surname><given-names>S.</given-names></name></person-group><article-title>Skeletonization: A technique for trimming the fat from a network via relevance assessment</article-title><source>Advances in Neural Information Processing Systems 1</source><publisher-name>Morgan-Kaufmann</publisher-name><publisher-loc>Burlington, MA, USA</publisher-loc><year>1989</year><fpage>107</fpage><lpage>115</lpage></element-citation></ref><ref id="B50-entropy-22-01365"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blalock</surname><given-names>D.</given-names></name><name><surname>Ortiz</surname><given-names>J.J.G.</given-names></name><name><surname>Frankle</surname><given-names>J.</given-names></name><name><surname>Guttag</surname><given-names>J.</given-names></name></person-group><article-title>What is the state of neural network pruning?</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.03033</pub-id></element-citation></ref><ref id="B51-entropy-22-01365"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Ioffe</surname><given-names>S.</given-names></name><name><surname>Vanhoucke</surname><given-names>V.</given-names></name><name><surname>Alemi</surname><given-names>A.</given-names></name></person-group><article-title>Inception-v4, inception-resnet and the impact of residual connections on learning</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1602.07261</pub-id></element-citation></ref><ref id="B52-entropy-22-01365"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razlighi</surname><given-names>Q.R.</given-names></name><name><surname>Kehtarnavaz</surname><given-names>N.</given-names></name><name><surname>Nosratinia</surname><given-names>A.</given-names></name></person-group><article-title>Computation of image spatial entropy using Quadrilateral Markov Random Field</article-title><source>IEEE Trans. Image Process.</source><year>2009</year><volume>18</volume><fpage>2629</fpage><lpage>2639</lpage><pub-id pub-id-type="doi">10.1109/TIP.2009.2029988</pub-id><pub-id pub-id-type="pmid">19674952</pub-id></element-citation></ref><ref id="B53-entropy-22-01365"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razlighi</surname><given-names>Q.R.</given-names></name><name><surname>Rahman</surname><given-names>M.T.</given-names></name><name><surname>Kehtarnavaz</surname><given-names>N.</given-names></name></person-group><article-title>Fast computation methods for estimation of image spatial entropy</article-title><source>J. Real-Time Image Process.</source><year>2011</year><volume>6</volume><fpage>137</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1007/s11554-009-0144-y</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="entropy-22-01365-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>A saliency map (generated using the Grad-CAM method) which highlights the most important pixels that contribute to the prediction of the class &#x0201c;boxer&#x0201d; (dog). Red denotes important regions.</p></caption><graphic xlink:href="entropy-22-01365-g001"/></fig><fig id="entropy-22-01365-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Accuracy measurements of VGG16 on CIFAR-10 as more layers are removed.</p></caption><graphic xlink:href="entropy-22-01365-g002"/></fig><fig id="entropy-22-01365-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Saliency maps of a truck image from the CIFAR-10 dataset. Each row represents a layer within the CNN. Each column represents a new network configuration from which we removed one layer from the previous structure, as detailed above. We used the &#x0201c;plasma&#x0201d; effect to indicate the hotness of the saliency map, where the transition from purple to yellow denotes more important regions.</p></caption><graphic xlink:href="entropy-22-01365-g003"/></fig><table-wrap id="entropy-22-01365-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">entropy-22-01365-t001_Table 1</object-id><label>Table 1</label><caption><p>Entropy values for saliency maps for AlexNet at different levels in the network.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">AlexNet</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Layer</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Pretrained</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Random</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Pretrained</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Fine-tuning</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ImageNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ImageNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Caltech101</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Caltech101</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6830</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6816</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6786</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6829</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6806</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6802</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6746</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6795</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">maxpool1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5252</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5113</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5264</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5356</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5311</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5100</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5395</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5352</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5231</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5096</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5297</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5191</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">maxpool2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4147</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3952</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4241</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4116</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4423</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3861</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4508</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4474</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4326</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3864</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4437</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4454</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4272</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3867</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4375</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4292</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4214</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3934</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4222</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4304</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4056</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3934</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4019</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3925</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3928</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3949</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3878</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3784</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">maxpool3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3114</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3038</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3077</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3071</td></tr></tbody></table></table-wrap><table-wrap id="entropy-22-01365-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">entropy-22-01365-t002_Table 2</object-id><label>Table 2</label><caption><p>Entropy values for saliency maps for VGG16 at different levels in the network.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">VGG16</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Layer</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Pretrained</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Random</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Pretrained</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Fine-tuning</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ImageNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ImageNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Caltech101</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Caltech101</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8516</td><td align="center" valign="middle" rowspan="1" colspan="1">0.785</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8418</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8369</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8017</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7322</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7883</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7731</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6742</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6308</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6681</td><td align="center" valign="middle" rowspan="1" colspan="1">0.648</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv10</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5491</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5155</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5556</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5429</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv12</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5112</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5155</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5127</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4901</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv13</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4213</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4035</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4281</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4135</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3868</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4288</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3994</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3599</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">maxpool5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3131</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3443</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3238</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3086</td></tr></tbody></table></table-wrap><table-wrap id="entropy-22-01365-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">entropy-22-01365-t003_Table 3</object-id><label>Table 3</label><caption><p>Entropy values for saliency maps for ResNet50 at different levels in the network.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="5" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">ResNet50</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Layer</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Pretrained</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Random</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Pretrained</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>Fine-Tuning</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ImageNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>ImageNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Caltech101</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Caltech101</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7854</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6574</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7705</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7633</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">block1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6849</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5108</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6807</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6794</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">block2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5912</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4193</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5901</td><td align="center" valign="middle" rowspan="1" colspan="1">0.582</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">block3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4574</td><td align="center" valign="middle" rowspan="1" colspan="1">0.3398</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4588</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4607</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">block4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2847</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2754</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2862</td></tr></tbody></table></table-wrap><table-wrap id="entropy-22-01365-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">entropy-22-01365-t004_Table 4</object-id><label>Table 4</label><caption><p>Experimental results for accuracy on the Caltech101 dataset when fine-tuning from the available ImageNet pretrained weights versus starting from scratch.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Network</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fine-Tuning from ImageNet</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Training from Scratch</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">AlexNet</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm69"><mml:mrow><mml:mrow><mml:mn>83.168</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mn>42.376</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mn>87.327</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:mn>61.584</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:mn>92.673</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm74"><mml:mrow><mml:mrow><mml:mn>43.168</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></table></table-wrap><table-wrap id="entropy-22-01365-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">entropy-22-01365-t005_Table 5</object-id><label>Table 5</label><caption><p>Custom network architecture for the CIFAR-10 use-case.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Custom Network</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Layer</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Kernel Size</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Input Channels</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Output Channels</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Conv1 + ReLU</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm75"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Conv2 + ReLU</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm76"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Conv3 + ReLU</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm77"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td><td align="center" valign="middle" rowspan="1" colspan="1">24</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Conv4 + ReLU</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm78"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">24</td><td align="center" valign="middle" rowspan="1" colspan="1">32</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fc1 + ReLU</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mn>32</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>32</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Fc2 + ReLU</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">120</td><td align="center" valign="middle" rowspan="1" colspan="1">84</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fc3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td></tr></tbody></table></table-wrap><table-wrap id="entropy-22-01365-t006" orientation="portrait" position="float"><object-id pub-id-type="pii">entropy-22-01365-t006_Table 6</object-id><label>Table 6</label><caption><p>Entropy values for saliency maps for the custom network at different levels in the network.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Custom Network with No Spatial Downsampling</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Layer</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Pretrained</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Random</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4273</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5948</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4454</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6062</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4505</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5802</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5025</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6189</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4354</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6101</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">relu3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4661</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6123</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">conv4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.4187</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5674</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">relu4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4415</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5798</td></tr></tbody></table></table-wrap><table-wrap id="entropy-22-01365-t007" orientation="portrait" position="float"><object-id pub-id-type="pii">entropy-22-01365-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparisons on CIFAR-10&#x02014;top 1 accuracy between VGG16, VGG11 (the smallest configuration from the VGG family), VGG16 after four layers removed (which has roughly the same number of parameters as VGG11) and VGG16 after eight layers removed (which is the smallest configuration which maintains the accuracy within a <inline-formula><mml:math id="mm80"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> difference).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Network</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of Parameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15,245,130</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm81"><mml:mrow><mml:mrow><mml:mn>89.55</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG11</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9,750,922</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm82"><mml:mrow><mml:mrow><mml:mn>87.83</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">after 4 layers</td><td align="center" valign="middle" rowspan="1" colspan="1">9,345,354</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm83"><mml:mrow><mml:mrow><mml:mn>89.57</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">removed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">after 8 layers</td><td align="center" valign="middle" rowspan="1" colspan="1">2,118,346</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm84"><mml:mrow><mml:mrow><mml:mn>89.49</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">removed</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table></table-wrap></floats-group></article>