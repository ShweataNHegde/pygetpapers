<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="review-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Mol Cell Biol</journal-id><journal-id journal-id-type="iso-abbrev">J Mol Cell Biol</journal-id><journal-id journal-id-type="publisher-id">jmcb</journal-id><journal-title-group><journal-title>Journal of Molecular Cell Biology</journal-title></journal-title-group><issn pub-type="ppub">1674-2788</issn><issn pub-type="epub">1759-4685</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">7883817</article-id><article-id pub-id-type="pmid">32573721</article-id><article-id pub-id-type="doi">10.1093/jmcb/mjaa030</article-id><article-id pub-id-type="publisher-id">mjaa030</article-id><article-categories><subj-group subj-group-type="heading"><subject>Perspective</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01180</subject></subj-group></article-categories><title-group><article-title>Modern deep learning in bioinformatics</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Haoyang</given-names></name><xref ref-type="aff" rid="mjaa030-aff1">1</xref><xref ref-type="aff" rid="mjaa030-aff2">2</xref><xref ref-type="author-notes" rid="mjaa030-FM1"/></contrib><contrib contrib-type="author"><name><surname>Tian</surname><given-names>Shuye</given-names></name><xref ref-type="aff" rid="mjaa030-aff3">3</xref><xref ref-type="author-notes" rid="mjaa030-FM1"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yu</given-names></name><xref ref-type="aff" rid="mjaa030-aff4">4</xref><xref ref-type="author-notes" rid="mjaa030-FM1"/></contrib><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Qiming</given-names></name><xref ref-type="aff" rid="mjaa030-aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Tan</surname><given-names>Renbo</given-names></name><xref ref-type="aff" rid="mjaa030-aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Pan</surname><given-names>Yijie</given-names></name><xref ref-type="aff" rid="mjaa030-aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Chao</given-names></name><xref ref-type="aff" rid="mjaa030-aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Ying</given-names></name><xref ref-type="corresp" rid="mjaa030-cor1"/><xref ref-type="aff" rid="mjaa030-aff1">1</xref><xref ref-type="aff" rid="mjaa030-aff2">2</xref><xref ref-type="aff" rid="mjaa030-aff7">7</xref><!--<email>xin.gao@kaust.edu.sa</email>--></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-7108-3574</contrib-id><name><surname>Gao</surname><given-names>Xin</given-names></name><xref ref-type="corresp" rid="mjaa030-cor1"/><xref ref-type="aff" rid="mjaa030-aff4">4</xref><!--<email>xin.gao@kaust.edu.sa</email>--></contrib></contrib-group><aff id="mjaa030-aff1"><label>1</label>
<institution>Cancer Systems Biology Center, The China-Japan Union Hospital, Jilin University</institution>, Changchun 130033, China</aff><aff id="mjaa030-aff2"><label>2</label>
<institution>MOE Key Laboratory of Symbolic Computation and Knowledge Engineering, College of Computer Science and Technology, Jilin University</institution>, Changchun 130012, China</aff><aff id="mjaa030-aff3"><label>3</label>
<institution>Department of Biology, Southern University of Science and Technology</institution>, Shenzhen 518055, China</aff><aff id="mjaa030-aff4"><label>4</label>
<institution>Computational Bioscience Research Center (CBRC), Computer Electrical and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST)</institution>, Thuwal 23955, Saudi Arabia</aff><aff id="mjaa030-aff5"><label>5</label>
<institution>School of Computer Science and Technology, Hangzhou Dianzi University</institution>, Hangzhou 310018, China</aff><aff id="mjaa030-aff6"><label>6</label>
<institution>Ningbo Institute of Information Technology Application, Chinese Academy of Sciences</institution>, Ningbo 315040, China</aff><aff id="mjaa030-aff7"><label>7</label>
<institution>Computational Systems Biology Lab, Department of Biochemistry and Molecular Biology and Institute of Bioinformatics, University of Georgia</institution>, Athens, GA 30602, USA</aff><author-notes><fn id="mjaa030-FM1"><p>
<sup>&#x02020;</sup>Haoyang Li, Shuye Tian3 and Yu Li contributed equally to this work.</p></fn><corresp id="mjaa030-cor1">Correspondence to: Xin Gao, E-mail: <email>xin.gao@kaust.edu.sa</email>; Ying Xu, E-mail: <email>xyn@uga.edu</email></corresp></author-notes><pub-date pub-type="collection"><month>11</month><year>2020</year></pub-date><pub-date pub-type="epub" iso-8601-date="2020-06-23"><day>23</day><month>6</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>6</month><year>2020</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>12</volume><issue>11</issue><fpage>823</fpage><lpage>827</lpage><history><date date-type="received"><day>14</day><month>10</month><year>2019</year></date><date date-type="rev-recd"><day>01</day><month>4</month><year>2020</year></date><date date-type="accepted"><day>23</day><month>4</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) (2020). Published by Oxford University Press on behalf of <italic>Journal of Molecular Cell Biology</italic>, IBCB, SIBS, CAS.</copyright-statement><copyright-year>2020</copyright-year><license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p></license></permissions><self-uri xlink:href="mjaa030.pdf"/><counts><page-count count="5"/></counts></article-meta></front><body><p>Deep learning (DL) has shown explosive growth in its application to bioinformatics and has demonstrated thrillingly promising power to mine the complex relationship hidden in large-scale biological and biomedical data. A number of comprehensive reviews have been published on such applications, ranging from high-level reviews with future perspectives to those mainly serving as tutorials. These reviews have provided an excellent introduction to and guideline for applications of DL in bioinformatics, covering multiple types of machine learning (ML) problems, different DL architectures, and ranges of biological/biomedical problems. However, most of these reviews have focused on previous research, whereas current trends in the principled DL field and perspectives on their future developments and potential new applications to biology and biomedicine are still scarce. We will focus on modern DL, the ongoing trends and future directions of the principled DL field, and postulate new and major applications in bioinformatics.</p><sec sec-type="intro"><title>Introduction</title><p>ML has been the main contributor to the recent resurgence of artificial intelligence. The most essential piece in modern ML technology is DL. DL is founded on artificial neural networks (ANNs), which have been theoretically proven to be capable of approximating any nonlinear function within any specified accuracy (<xref rid="mjaa030-B10" ref-type="bibr">Hornik, 1991</xref>) and have been widely used to solve various computational tasks (<xref rid="mjaa030-B18" ref-type="bibr">Li et&#x000a0;al., 2019</xref>). However, they have been criticized for being black boxes. This lack of interpretability has limited their applications, particularly when their performance did not stand out among other more interpretable ML methods, such as linear regression, logistic regression, support vector machines, and decision trees.</p><p>During the past decade, three important advances in science and technology have led to the rejuvenation of ANNs, particularly via DL. First, unprecedented quantities of data have been generated in modern life, mostly imaging and natural language data. The complex nature of information derivation from such data has posed great challenges to other ML methods but has been handled well by ANNs. Similarly, high-throughput biological data such as next-generation sequencing, metabolomic data, proteome data, and electron microscopic structural data, has raised equally challenging computational problems. Second, computational power has been increasing rapidly with affordable costs, including the development of new computing devices, such as graphics processing units and field programmable gate arrays. Such devices provide ideal hardware platforms for highly parallel models. Third, a range of proposed optimization algorithms have made deep ANNs stand out as an ideal technique for large and complex data analyses and information discovery compared to competing techniques in the big data era. Here are also some problems in the bioinformatics field as follows, which need to be tackled. First, the interpretability of model is essential to biologists to understand how model helps solve the biological problem, e.g. predicting DNA&#x02013;protein binding (<xref rid="mjaa030-B23" ref-type="bibr">Luo et&#x000a0;al., 2020</xref>). Second, the clinical expect accuracy of computational model related to the healthcare or disease diagnosis is &#x0223c;98%&#x02012;99% and it is tough to reach that high accuracy. Moreover, two fundamental breakthroughs have tremendously increased the applicability of ANN techniques: convolutional neural networks (CNNs) for imaging data and recurrent neural networks (RNNs) for natural language data, which will be introduced in the <xref ref-type="supplementary-material" rid="sup1">Supplementary material</xref> with other well-known architectures. We surveyed the literature and tabulated the number of publications in log-scale for 14 commonly studied biological topics appearing together with &#x02018;RNN&#x02019;, &#x02018;CNN&#x02019;, or &#x02018;deep learning&#x02019; according to PubMed, which are detailed in <xref ref-type="fig" rid="mjaa030-F1">Figure&#x000a0;1</xref>. As expected, &#x02018;image&#x02019; is the most commonly approached topic by DL, and &#x02018;disease&#x02019; and &#x02018;imaging&#x02019; follow closely. CNNs are much more frequently used in bioinformatics than RNNs because CNNs can easily capture local features, solving fundamental issues, such as identifying and applying conserved sequence motifs.
</p><fig id="mjaa030-F1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Number of publications (log-scale) for 14 biological topics. For each topic, the three bars show the number of publications mentioning the terms &#x02018;RNN&#x02019;, &#x02018;CNN&#x02019;, and &#x02018;deep learning&#x02019;, respectively.</p></caption><graphic xlink:href="mjaa030f1"/></fig><p>Here, we focus on the ongoing trends and future directions of modern DL, perspective on future developments and potential new applications to biology and biomedicine.</p></sec><sec><title>Current trend in principled DL</title><sec><title>Attention mechanism</title><p>Attention mechanisms, which were first proposed to conduct machine-based translation tasks (<xref rid="mjaa030-B27" ref-type="bibr">Vaswani et&#x000a0;al., 2017</xref>), can alleviate the problems faced by RNNs when applied to bioinformatics problems, thus expanding their domain of applications in bioinformatics. The self-attention layer can translate the original representation of an input sequence (e.g. one-hot encoding for RNA, DNA, or protein sequences) into another representation of the sequence. For each position in the sequence, the other positions in the input sequence try to better characterize that position for capturing the semantic meaning of the sequence and interactions between different sequential positions.</p><p>Attention mechanisms can potentially be used in a wide range of biosequence analysis problems, such as RNA sequence analysis and prediction (<xref rid="mjaa030-B25" ref-type="bibr">Park et&#x000a0;al., 2017</xref>), protein structure and function prediction from amino acid sequences (<xref rid="mjaa030-B30" ref-type="bibr">Zou et&#x000a0;al., 2018</xref>), and identification of enhancer&#x02013;promoter interactions (EPIs) (<xref rid="mjaa030-B9" ref-type="bibr">Hong et&#x000a0;al., 2020</xref>). For example, EPIs show great significance to human development because they are critical to the regulation of gene expression and are closely related to the occurrence of human diseases. However, experimental methods to identify EPIs require too much time, manpower, and money. EPIVAN (<xref rid="mjaa030-B9" ref-type="bibr">Hong et&#x000a0;al., 2020</xref>) was designed to predict long-range EPIs using only genomic sequences via DL methods and attention mechanisms. This method has been tested on six cell lines, and the area under the receiver operating characteristic (AUROC) and area under the precision-recall curve (AUPR) values of EPIVAN are higher than those without the attention mechanism, which indicates that the attention mechanism is more concerned with cell line-specific features and can better capture the hidden information from the perspective of sequences.</p></sec><sec><title>Reinforcement learning</title><p>Reinforcement learning (<xref rid="mjaa030-B24" ref-type="bibr">Mnih et&#x000a0;al., 2015</xref>) considers what actions to take, given the current state of the partial solution to maximize the cumulative reward. After each action, the state can change. Observations about the set of change-of-state become guiding information for future actions. This type of reinforcement learning has recently been incorporated into the DL paradigm, referred to as deep reinforcement learning. Note that a key distinguishing feature is that users do not have to predefine all the states, and a model can be trained in an end-to-end manner, which has become an increasingly active research field with numerous algorithms being developed.</p><p>Reinforcement learning can be applied in collective cell migration (<xref rid="mjaa030-B11" ref-type="bibr">Hou et&#x000a0;al., 2019</xref>), DNA fragment assembly (<xref rid="mjaa030-B4" ref-type="bibr">Bocicor et&#x000a0;al., 2012</xref>), and characterizing cell movement (<xref rid="mjaa030-B29" ref-type="bibr">Wang et&#x000a0;al., 2018</xref>). DNA fragment assembly is a technique that aims to reconstruct the original DNA sequence from a large number of fragments by determining the order in which the fragments have to be assembled back into the original DNA molecule, and it is also an NP-hard optimization problem. <xref rid="mjaa030-B4" ref-type="bibr">Bocicor et&#x000a0;al. (2012)</xref> proposed a new reinforcement learning-based model for solving this problem. Reinforcement learning in this problem was formulated as training the agent to find a path during assembling fragments from the initial to a final alignment state, maximizing the performance measure, one of the fitness functions, which sums the overlap scores over all adjacent fragments. This reinforcement learning model shows less computational complexity and unnecessary external supervision in the learning process compared with the genetic algorithm and supervised approach, respectively.</p></sec><sec><title>Few-shot learning</title><p>Although there is a large amount of data in the bioinformatics field (<xref rid="mjaa030-B18" ref-type="bibr">Li et&#x000a0;al., 2019</xref>), data scarcity still occurs in biology and biomedicine. For example, under the enzyme commission (EC) classification (<xref rid="mjaa030-B20" ref-type="bibr">Li et&#x000a0;al., 2017a</xref>), only one enzyme belongs to the class of phosphonate dehydrogenase (EC 1.20.1.1). In this case, standard DL algorithms cannot work because one needs numerous data for each class to train a generalizable DL model (<xref rid="mjaa030-B19" ref-type="bibr">Li et&#x000a0;al., 2018</xref>). Few-shot learning, as its name indicates, is designed to handle these cases. In principle, few-shot learning trains an ML model with a very small quantity of data. In extreme cases, there is only one training sample for one class, referred to as one-shot learning (<xref rid="mjaa030-B6" ref-type="bibr">Fei-Fei et&#x000a0;al., 2006</xref>). Similarly defined is zero-shot learning (<xref rid="mjaa030-B26" ref-type="bibr">Socher et&#x000a0;al., 2013</xref>) when a class has no training sample. Using few-shot learning algorithms, a model can be trained with reasonable performance on some difficult problems by utilizing only the existing limited data.</p><p>Few-shot learning is suitable for many problems in bioinformatics that have limited data, such as protein function prediction (<xref rid="mjaa030-B20" ref-type="bibr">Li et&#x000a0;al., 2017a</xref>) and drug discovery (<xref rid="mjaa030-B16" ref-type="bibr">Joslin et&#x000a0;al., 2018</xref>). For instance, the drug discovery problem is to optimize the candidate molecule that can modulate essential pathways to achieve therapeutic activity by finding analogue molecules with increased pharmaceutical activity. Due to the limitation of small biological data, it is challenging to form accurate predictions for novel compounds. As we searched, one-shot learning has been used to significantly lower the quantity of data required and achieves precise predictions in drug discovery (<xref rid="mjaa030-B2" ref-type="bibr">Altae-Tran et&#x000a0;al., 2017</xref>). The method proposed in this work combines iterative refinement long short-term memory (LSTM) and graph CNNs and can improve the learning of meaningful distance metrics over small molecules. Iterative refinement LSTMs can generalize to new experimental assays related but not identical to assays in the training collection, and graph convolutional networks are useful for transforming small molecules into continuous vectorial representations. The results of applying one-shot models to a number of assay collections show strong performance compared to other methods, such as random forest and graph CNNs. Consequently, this one-shot method is capable of transferring information between related but distinct learning tasks.</p></sec><sec><title>Deep generative models</title><p>In biology, high-throughput omic data tend to have high dimensionality and be intrinsically noisy, such as single-cell transcriptomic data (<xref rid="mjaa030-B22" ref-type="bibr">Lopez et&#x000a0;al., 2018</xref>). The widely used dimensionality reduction methods, such as principal component analysis, may not work well with such data because of those properties. Deep generative models, such as variational autoencoders (VAEs) (<xref rid="mjaa030-B5" ref-type="bibr">Doersch, 2016</xref>), are powerful networks for information derivation using unsupervised learning, which has achieved remarkable success in recent years. Generally, it is almost impossible to model the exact distributions of any property of such datasets; those methods are designed to model an approximate distribution that is as similar to the true distribution as possible, implicitly or explicitly. When training a VAE, a low-dimensional latent representation of the raw data with latent variables can be learned, which were assumed to generate the real data. Those generated samples, which do not exist in the real world, can be useful for various biological data modelling problems, such as drug design and protein design.</p><p>Deep generative models can be applied to problems related to protein structure design (<xref rid="mjaa030-B3" ref-type="bibr">Anand and Huang, 2018</xref>; <xref rid="mjaa030-B15" ref-type="bibr">Ingraham et&#x000a0;al., 2019</xref>), 3D compound design (<xref rid="mjaa030-B14" ref-type="bibr">Imrie et&#x000a0;al., 2020</xref>), protein loop modelling (<xref rid="mjaa030-B21" ref-type="bibr">Li et&#x000a0;al., 2017b</xref>), and DNA design (<xref rid="mjaa030-B17" ref-type="bibr">Killoran et&#x000a0;al., 2017</xref>). The structure and function of proteins is a key feature of understanding biology at the molecular and cellular levels. However, there might be missing regions that need to be reconstructed, and the prediction of those missing regions is also called the loop modelling problem. A generative adversarial network (GAN) is applied for this problem, which can capture the context of the loop region and predict the missing area (<xref rid="mjaa030-B21" ref-type="bibr">Li et&#x000a0;al., 2017b</xref>). The 3D protein structure is represented by the 2D distance map in which each value is a real Euclidean distance of C&#x003b1; atoms of two amino acids. The root-mean-square deviation score of their GAN method has 44% improvement compared to other tools, and their GAN method obtains the smallest standard deviation compared to other tools, which show the stability of their prediction.</p></sec><sec><title>Meta learning</title><p>Meta learning (<xref rid="mjaa030-B7" ref-type="bibr">Finn et&#x000a0;al., 2017</xref>), also known as &#x02018;learn-to-learn&#x02019;, attempts to produce such models, which can quickly learn a new task with a few training samples based on models trained for related tasks. A good meta learning model should generalize to a new task even if the task has never been encountered during the training time. The key idea is that when training a model is finished, the model needs to be exposed to a new task during the testing phase, several steps of fine-tuning are performed, and then the model&#x02019;s performance on the new task is checked. In brief, meta learning outputs an ML model that can learn quickly.</p><p>For instance, the ability of an antibody to respond to an antigen depends on the antibody&#x02019;s specific recognition of an epitope (<xref rid="mjaa030-B12" ref-type="bibr">Hu et&#x000a0;al., 2014</xref>). Thus, meta learning can be used in B-cell conformational epitope prediction in continuously evolving viruses, which is useful for vaccine design. The proposed meta learning approach is based on stacked and cascade generalizations. In the hierarchical architecture, the meta learner of each level will input the meta features outputted from a low level and output the meta features to successive levels until the top level which will output the final classification result. Low correlation among these meta learners indicates that these learners truly have complementary predictive capabilities, and the ablation analysis indicates that these learners differentially interacted and contributed to the final meta model. Consequently, the meta learner can analyse the complementary predictive strengths in different prediction tools and integrate these tools to outperform the single best-performing model through meta learning.</p></sec><sec><title>Symbolic reasoning empowered DL</title><p>It is noteworthy that until recently, DL has yet to include symbolic reasoning or logic as part of its toolkit, hence having omitted the essential information provided by logic reason and the associated explainability (<xref rid="mjaa030-B13" ref-type="bibr">Hu et&#x000a0;al., 2016</xref>). In recent years, ML researchers have developed a number of methods to incorporate symbolic reasoning with DL. For example, SATNet (<xref rid="mjaa030-B28" ref-type="bibr">Wang et&#x000a0;al., 2019</xref>) uses a differentiable satisfiability solver to bridge DL and logic reasoning; NLM (<xref rid="mjaa030-B8" ref-type="bibr">Hamilton et&#x000a0;al., 2018</xref>) exploits the power of both DL and logic programming, utilizing it to perform inductive learning and logic reasoning efficiently.</p><p>In the bioinformatics field, symbolic reasoning is applied and evaluated on structured biological knowledge, which can be used for data integration, retrieval, and federated queries in the knowledge graph (<xref rid="mjaa030-B1" ref-type="bibr">Alshahrani et&#x000a0;al., 2017</xref>). This method combines symbolic methods, in particular, knowledge representation using symbolic logic and automated reasoning, with neural networks that encode for related information within knowledge graphs, and these embeddings can be applied to predict the edges in the knowledge graph, such as drug&#x02212;target relations. The performance combining symbolic methods outperforms traditional approaches.</p></sec></sec><sec sec-type="conclusion"><title>Conclusion</title><p>DL is a relatively new field compared to traditional ML, and the application of DL in bioinformatics is an even newer field. However, the last decade has witnessed the rapid development of DL with thrillingly promising power to mine complex relationships hidden in large-scale biological and biomedical data. In this article, we reviewed some selected modern and principled DL methodologies, some of which have recently been applied to bioinformatics, while others have not yet been applied. This perspective may shed new light on the foreseeable future applications of modern DL methods in bioinformatics.</p><p>
<italic>[<xref ref-type="supplementary-material" rid="sup1">Supplementary material</xref> is available at Journal of Molecular Cell Biology online. The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST), under award numbers FCC/1/1976-18-01, FCC/1/1976-23-01, FCC/1/1976-25-01, FCC/1/1976-26-01, URF/1/3450-01-01, URF/1/3454-01-01, URF/1/4098-01-01, URF/1/4077-01-01, and REI/1/0018-01-01. Y.X. and X.G. conceived the study; H.L., S.T., and Y.L. wrote the paper together; Q.F., R.T., Y.P., and C.H. contributed materials and critical revisions to the paper.]</italic>
</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="sup1"><label>mjaa030_Supplementary_Data</label><media xlink:href="mjaa030_supplementary_data.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ref-list id="ref1"><title>References</title><ref id="mjaa030-B1"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Alshahrani</surname>
<given-names>M.</given-names>
</string-name>, <string-name><surname>Khan</surname><given-names>M.A.</given-names></string-name>, <string-name><surname>Maddouri</surname><given-names>O.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2017</year>). 
<article-title>Neuro-symbolic representation learning on biological knowledge graphs</article-title>. <source>Bioinformatics</source> &#x000a0;<volume>33</volume>, <fpage>2723</fpage>&#x02013;<lpage>2730</lpage>.<pub-id pub-id-type="pmid">28449114</pub-id></mixed-citation></ref><ref id="mjaa030-B2"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Altae-Tran</surname>
<given-names>H.</given-names>
</string-name>, <string-name><surname>Ramsundar</surname><given-names>B.</given-names></string-name>, <string-name><surname>Pappu</surname><given-names>A.S.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2017</year>). 
<article-title>Low data drug discovery with one-shot learning</article-title>. <source>ACS Cent. Sci.</source> &#x000a0;<volume>3</volume>, <fpage>283</fpage>&#x02013;<lpage>293</lpage>.<pub-id pub-id-type="pmid">28470045</pub-id></mixed-citation></ref><ref id="mjaa030-B3"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Anand</surname>
<given-names>N.</given-names>
</string-name>, <string-name><surname>Huang</surname><given-names>P.</given-names></string-name></person-group> (<year>2018</year>). &#x02018;Generative modeling for protein structures&#x02019;. In: Advances in Neural Information Processing Systems 31: 32nd Conference (NeurIPS 2018), Montreal, Canada. 7494&#x02013;7505. La Jolla, USA: Neural Information Processing Systems (NIPS).</mixed-citation></ref><ref id="mjaa030-B4"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Bocicor</surname>
<given-names>M.-I.</given-names>
</string-name>, <string-name><surname>Czibula</surname><given-names>G.</given-names></string-name>, <string-name><surname>Czibula</surname><given-names>I.</given-names></string-name></person-group> (<year>2012</year>). &#x02018;A reinforcement learning approach for solving the fragment assembly problem&#x02019;. In: 13th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, Timisoara, Romania, 2011. 191&#x02013;198. Los Alamitos, USA: IEEE Computer Society.</mixed-citation></ref><ref id="mjaa030-B5"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Doersch</surname>
<given-names>C.</given-names>
</string-name></person-group> (<year>2016</year>). Tutorial on variational autoencoders. arXiv, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.05908">https://arxiv.org/abs/1606.05908</ext-link>.</mixed-citation></ref><ref id="mjaa030-B6"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Fei-Fei</surname>
<given-names>L.</given-names>
</string-name>, <string-name><surname>Fergus</surname><given-names>R.</given-names></string-name>, <string-name><surname>Perona</surname><given-names>P.</given-names></string-name></person-group> (<year>2006</year>). 
<article-title>One-shot learning of object categories</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source> &#x000a0;<volume>28</volume>, <fpage>594</fpage>&#x02013;<lpage>611</lpage>.<pub-id pub-id-type="pmid">16566508</pub-id></mixed-citation></ref><ref id="mjaa030-B7"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Finn</surname>
<given-names>C.</given-names>
</string-name>, <string-name><surname>Abbeel</surname><given-names>P.</given-names></string-name>, <string-name><surname>Levine</surname><given-names>S.</given-names></string-name></person-group> (<year>2017</year>). &#x02018;Model-agnostic meta-learning for fast adaptation of deep networks&#x02019;. In: Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, 2017. PMLR 70, 1126&#x02013;1135. <ext-link ext-link-type="uri" xlink:href="http://www.jmlr.org/">http://www.jmlr.org/</ext-link>.</mixed-citation></ref><ref id="mjaa030-B8"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Hamilton</surname>
<given-names>W.</given-names>
</string-name>, <string-name><surname>Bajaj</surname><given-names>P.</given-names></string-name>, <string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2018</year>). &#x02018;Embedding logical queries on knowledge graphs&#x02019;. In: Advances in Neural Information Processing Systems 31: 32nd Conference (NeurIPS 2018), Montreal, Canada. 2026&#x02013;2037. La Jolla, USA: Neural Information Processing Systems (NIPS).</mixed-citation></ref><ref id="mjaa030-B9"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Hong</surname>
<given-names>Z.</given-names>
</string-name>, <string-name><surname>Zeng</surname><given-names>X.</given-names></string-name>, <string-name><surname>Wei</surname><given-names>L.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2020</year>). 
<article-title>Identifying enhancer&#x02013;promoter interactions with neural network based on pre-trained DNA vectors and attention mechanism</article-title>. <source>Bioinformatics</source> &#x000a0;<volume>36</volume>, <fpage>1037</fpage>&#x02013;<lpage>1043</lpage>.<pub-id pub-id-type="pmid">31588505</pub-id></mixed-citation></ref><ref id="mjaa030-B10"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Hornik</surname>
<given-names>K.</given-names>
</string-name></person-group> (<year>1991</year>). 
<article-title>Approximation capabilities of multilayer feedforward networks</article-title>. <source>Neural Netw.</source> &#x000a0;<volume>4</volume>, <fpage>251</fpage>&#x02013;<lpage>257</lpage>.</mixed-citation></ref><ref id="mjaa030-B11"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Hou</surname>
<given-names>H.</given-names>
</string-name>, <string-name><surname>Gan</surname><given-names>T.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2019</year>) 
<article-title>Using deep reinforcement learning to speed up collective cell migration</article-title>. <source>BMC Bioinformatics</source> &#x000a0;<volume>20</volume>, <fpage>571</fpage>.<pub-id pub-id-type="pmid">31760946</pub-id></mixed-citation></ref><ref id="mjaa030-B12"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Hu</surname>
<given-names>Y.-J.</given-names>
</string-name>, <string-name><surname>Lin</surname><given-names>S.-C.</given-names></string-name>, <string-name><surname>Lin</surname><given-names>Y.-L.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2014</year>). 
<article-title>A meta-learning approach for B-cell conformational epitope prediction</article-title>. <source>BMC Bioinformatics</source> &#x000a0;<volume>15</volume>, <fpage>378</fpage>.<pub-id pub-id-type="pmid">25403375</pub-id></mixed-citation></ref><ref id="mjaa030-B13"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Hu</surname>
<given-names>Z.</given-names>
</string-name>, <string-name><surname>Ma</surname><given-names>X.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Z.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2016</year>). Harnessing deep neural networks with logic rules. arXiv, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.06318">https://arxiv.org/abs/1603.06318</ext-link>.</mixed-citation></ref><ref id="mjaa030-B14"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Imrie</surname>
<given-names>F.</given-names>
</string-name>, <string-name><surname>Bradley</surname><given-names>A.R.</given-names></string-name>, <string-name><surname>van der Schaar</surname><given-names>M.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2020</year>). Deep generative models for 3D compound design. J. Chem. Inf. Model. <volume>60</volume>, 1983&#x02013;1995.</mixed-citation></ref><ref id="mjaa030-B15"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Ingraham</surname>
<given-names>J.</given-names>
</string-name>, <string-name><surname>Garg</surname><given-names>V.K.</given-names></string-name>, <string-name><surname>Barzilay</surname><given-names>R.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2019</year>). &#x02018;Generative models for graph-based protein design&#x02019;. In: Advances in Neural Information Processing Systems 32: 33rd Conference (NeurIPS 2019), Vancouver, Canada. 15741&#x02013;15752. La Jolla, USA: Neural Information Processing Systems (NIPS).</mixed-citation></ref><ref id="mjaa030-B16"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Joslin</surname>
<given-names>J.</given-names>
</string-name>, <string-name><surname>Gilligan</surname><given-names>J.</given-names></string-name>, <string-name><surname>Anderson</surname><given-names>P.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2018</year>). 
<article-title>A fully automated high-throughput flow cytometry screening system enabling phenotypic drug discovery</article-title>. <source>SLAS Discov.</source> &#x000a0;<volume>23</volume>, <fpage>697</fpage>&#x02013;<lpage>707</lpage>.<pub-id pub-id-type="pmid">29843542</pub-id></mixed-citation></ref><ref id="mjaa030-B17"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Killoran</surname>
<given-names>N.</given-names>
</string-name>, <string-name><surname>Lee</surname><given-names>L.J.</given-names></string-name>, <string-name><surname>Delong</surname><given-names>A.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2017</year>). Generating and designing DNA with deep generative models. arXiv, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1712.06148">https://arxiv.org/abs/1712.06148</ext-link>.</mixed-citation></ref><ref id="mjaa030-B18"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Li</surname>
<given-names>Y.</given-names>
</string-name>, <string-name><surname>Huang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2019</year>). 
<article-title>Deep learning in bioinformatics: Introduction, application, and perspective in the big data era</article-title>. <source>Methods</source> &#x000a0;<volume>166</volume>, <fpage>4</fpage>&#x02013;<lpage>21</lpage>.<pub-id pub-id-type="pmid">31022451</pub-id></mixed-citation></ref><ref id="mjaa030-B19"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Li</surname>
<given-names>Y.</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2018</year>). Supportnet: solving catastrophic forgetting in class incremental learning with support data. arXiv, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.02942">https://arxiv.org/abs/1806.02942</ext-link>.</mixed-citation></ref><ref id="mjaa030-B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Li</surname>
<given-names>Y.</given-names>
</string-name>, <string-name><surname>Wang</surname><given-names>S.</given-names></string-name>, <string-name><surname>Umarov</surname><given-names>R.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2017</year>a). 
<article-title>DEEPre: sequence-based enzyme EC number prediction by deep learning</article-title>. <source>Bioinformatics</source> &#x000a0;<volume>34</volume>, <fpage>760</fpage>&#x02013;<lpage>769</lpage>.</mixed-citation></ref><ref id="mjaa030-B21"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Li</surname>
<given-names>Z.</given-names>
</string-name>, <string-name><surname>Nguyen</surname><given-names>S.P.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>D.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2017</year>b). &#x02018;Protein loop modeling using deep generative adversarial network&#x02019;. In: 2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI 2017), Boston, USA. 1085&#x02013;1091. New York, USA: IEEE.</mixed-citation></ref><ref id="mjaa030-B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Lopez</surname>
<given-names>R.</given-names>
</string-name>, <string-name><surname>Regier</surname><given-names>J.</given-names></string-name>, <string-name><surname>Cole</surname><given-names>M.B.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2018</year>). 
<article-title>Deep generative modeling for single-cell transcriptomics</article-title>. <source>Nat. Methods</source> &#x000a0;<volume>15</volume>, <fpage>1053</fpage>.<pub-id pub-id-type="pmid">30504886</pub-id></mixed-citation></ref><ref id="mjaa030-B23"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Luo</surname>
<given-names>X.</given-names>
</string-name>, <string-name><surname>Tu</surname><given-names>X.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>Y.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2020</year>). Expectation pooling: an effective and interpretable pooling method for predicting DNA&#x02013;protein binding. Bioinformatics <volume>36</volume>, 1405&#x02013;1412.</mixed-citation></ref><ref id="mjaa030-B24"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Mnih</surname>
<given-names>V.</given-names>
</string-name>, <string-name><surname>Kavukcuoglu</surname><given-names>K.</given-names></string-name>, <string-name><surname>Silver</surname><given-names>D.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2015</year>). 
<article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source> &#x000a0;<volume>518</volume>, <fpage>529</fpage>.<pub-id pub-id-type="pmid">25719670</pub-id></mixed-citation></ref><ref id="mjaa030-B25"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Park</surname>
<given-names>S.</given-names>
</string-name>, <string-name><surname>Min</surname><given-names>S.</given-names></string-name>, <string-name><surname>Choi</surname><given-names>H.-S.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2017</year>). &#x02018;Deep recurrent neural network-based identification of precursor micrornas&#x02019;. In: Advances in Neural Information Processing Systems 30: 31st Annual Conference (NIPS 2017), Long Beach, USA. 2891&#x02013;2900. La Jolla, USA: Neural Information Processing Systems (NIPS).</mixed-citation></ref><ref id="mjaa030-B26"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Socher</surname>
<given-names>R.</given-names>
</string-name>, <string-name><surname>Ganjoo</surname><given-names>M.</given-names></string-name>, <string-name><surname>Manning</surname><given-names>C.D.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2013</year>). &#x02018;Zero-shot learning through cross-modal transfer&#x02019;. In: Advances in Neural Information Processing Systems 26: 27th Annual Conference 2013, Lake Tahoe, USA. 935&#x02013;943. La Jolla, USA: Neural Information Processing Systems (NIPS).</mixed-citation></ref><ref id="mjaa030-B27"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Vaswani</surname>
<given-names>A.</given-names>
</string-name>, <string-name><surname>Shazeer</surname><given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2017</year>). &#x02018;Attention is all you need&#x02019;. In: Advances in Neural Information Processing Systems 30: 31st Annual Conference (NIPS 2017), Long Beach, USA. 5998&#x02013;6008. La Jolla, USA: Neural Information Processing Systems (NIPS).</mixed-citation></ref><ref id="mjaa030-B28"><mixed-citation publication-type="other">
<person-group person-group-type="author"><string-name>
<surname>Wang</surname>
<given-names>P.-W.</given-names>
</string-name>, <string-name><surname>Donti</surname><given-names>P.L.</given-names></string-name>, <string-name><surname>Wilder</surname><given-names>B.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2019</year>). SATNet: bridging deep learning and logical reasoning using a differentiable satisfiability solver. arXiv, <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.12149">https://arxiv.org/abs/1905.12149</ext-link>.</mixed-citation></ref><ref id="mjaa030-B29"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Wang</surname>
<given-names>Z.</given-names>
</string-name>, <string-name><surname>Wang</surname><given-names>D.</given-names></string-name>, <string-name><surname>Li</surname><given-names>C.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2018</year>). 
<article-title>Deep reinforcement learning of cell movement in the early stage of C. elegans embryogenesis</article-title>. <source>Bioinformatics</source> &#x000a0;<volume>34</volume>, <fpage>3169</fpage>&#x02013;<lpage>3177</lpage>.<pub-id pub-id-type="pmid">29701853</pub-id></mixed-citation></ref><ref id="mjaa030-B30"><mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name>
<surname>Zou</surname>
<given-names>Z.</given-names>
</string-name>, <string-name><surname>Tian</surname><given-names>S.</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X.</given-names></string-name></person-group>, <etal>et al</etal> (<year>2018</year>). 
<article-title>mldeepre: multi-functional enzyme function prediction with hierarchical multi-label deep learning</article-title>. <source>Front. Genet.</source> &#x000a0;<volume>9</volume>, <fpage>714</fpage>.<pub-id pub-id-type="pmid">30723495</pub-id></mixed-citation></ref></ref-list></back></article>