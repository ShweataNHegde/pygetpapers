<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Int J Environ Res Public Health</journal-id><journal-id journal-id-type="iso-abbrev">Int J Environ Res Public Health</journal-id><journal-id journal-id-type="publisher-id">ijerph</journal-id><journal-title-group><journal-title>International Journal of Environmental Research and Public Health</journal-title></journal-title-group><issn pub-type="ppub">1661-7827</issn><issn pub-type="epub">1660-4601</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">7967559</article-id><article-id pub-id-type="doi">10.3390/ijerph18052428</article-id><article-id pub-id-type="publisher-id">ijerph-18-02428</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Deep Learning Model for Classification of Endoscopic Gastroesophageal Reflux Disease</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8222-0503</contrib-id><name><surname>Wang</surname><given-names>Chi-Chih</given-names></name><xref ref-type="aff" rid="af1-ijerph-18-02428">1</xref><xref ref-type="aff" rid="af2-ijerph-18-02428">2</xref><xref ref-type="aff" rid="af3-ijerph-18-02428">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2449-9769</contrib-id><name><surname>Chiu</surname><given-names>Yu-Ching</given-names></name><xref ref-type="aff" rid="af4-ijerph-18-02428">4</xref></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Wei-Liang</given-names></name><xref ref-type="aff" rid="af3-ijerph-18-02428">3</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Tzu-Wei</given-names></name><xref ref-type="aff" rid="af1-ijerph-18-02428">1</xref><xref ref-type="aff" rid="af2-ijerph-18-02428">2</xref><xref ref-type="aff" rid="af3-ijerph-18-02428">3</xref></contrib><contrib contrib-type="author"><name><surname>Tsai</surname><given-names>Ming-Chang</given-names></name><xref ref-type="aff" rid="af1-ijerph-18-02428">1</xref><xref ref-type="aff" rid="af2-ijerph-18-02428">2</xref><xref ref-type="aff" rid="af3-ijerph-18-02428">3</xref><xref rid="c1-ijerph-18-02428" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8868-1610</contrib-id><name><surname>Tseng</surname><given-names>Ming-Hseng</given-names></name><xref ref-type="aff" rid="af5-ijerph-18-02428">5</xref><xref ref-type="aff" rid="af6-ijerph-18-02428">6</xref><xref rid="c1-ijerph-18-02428" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Ryu</surname><given-names>Keun Ho</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-ijerph-18-02428"><label>1</label>Institute of Medicine, Chung Shan Medical University, Taichung 402, Taiwan; <email>bananaudwang@gmail.com</email> (C.-C.W.); <email>joviyoung@gmail.com</email> (T.-W.Y.)</aff><aff id="af2-ijerph-18-02428"><label>2</label>School of Medicine, Chung Shan Medical University, Taichung 402, Taiwan</aff><aff id="af3-ijerph-18-02428"><label>3</label>Division of Gastroenterology and Hepatology, Department of Internal Medicine, Chung Shan Medical University Hospital, Taichung 402, Taiwan; <email>grincia@yahoo.com.tw</email></aff><aff id="af4-ijerph-18-02428"><label>4</label>Master Program in Medical Informatics, Chung Shan Medical University, Taichung 402, Taiwan; <email>cru912@gmail.com</email></aff><aff id="af5-ijerph-18-02428"><label>5</label>Department of Medical Informatics, Chung Shan Medical University, Taichung 402, Taiwan</aff><aff id="af6-ijerph-18-02428"><label>6</label>Information Technology Office, Chung Shan Medical University Hospital, Taichung 402, Taiwan</aff><author-notes><corresp id="c1-ijerph-18-02428"><label>*</label>Correspondence: <email>tsaimc1110@gmail.com</email> (M.-C.T.); <email>mht@csmu.edu.tw</email> (M.-H.T.); Tel.: +886-4-24739595 (ext. 38315) (M.-C.T.); +886-4-24730022 (ext. 12214) (M.-H.T.)</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>3</month><year>2021</year></pub-date><pub-date pub-type="collection"><month>3</month><year>2021</year></pub-date><volume>18</volume><issue>5</issue><elocation-id>2428</elocation-id><history><date date-type="received"><day>30</day><month>12</month><year>2020</year></date><date date-type="accepted"><day>25</day><month>2</month><year>2021</year></date></history><permissions><copyright-statement>&#x000a9; 2021 by the authors.</copyright-statement><copyright-year>2021</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Gastroesophageal reflux disease (GERD) is a common disease with high prevalence, and its endoscopic severity can be evaluated using the Los Angeles classification (LA grade). This paper proposes a deep learning model (i.e., GERD-VGGNet) that employs convolutional neural networks for automatic classification and interpretation of routine GERD LA grade. The proposed model employs a data augmentation technique, a two-stage no-freezing fine-tuning policy, and an early stopping criterion. As a result, the proposed model exhibits high generalizability. A dataset of images from 464 patients was used for model training and validation. An additional 32 patients served as a test set to evaluate the accuracy of both the model and our trainees. Experimental results demonstrate that the best model for the development set exhibited an overall accuracy of 99.2% (grade A&#x02013;B), 100% (grade C&#x02013;D), and 100% (normal group) using narrow-band image (NBI) endoscopy. On the test set, the proposed model resulted in an accuracy of 87.9%, which was significantly higher than the results of the trainees (75.0% and 65.6%). The proposed GERD-VGGNet model can assist automatic classification of GERD in conventional and NBI environments and thereby increase the accuracy of interpretation of the results by inexperienced endoscopists.</p></abstract><kwd-group><kwd>gastroesophageal reflux disease classification</kwd><kwd>artificial intelligence</kwd><kwd>deep learning</kwd><kwd>conventional endoscopy</kwd><kwd>narrow-band image</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-ijerph-18-02428"><title>1. Introduction</title><p>Gastroesophageal reflux disease (GERD), which is a condition that develops when the reflux of stomach contents causes symptoms of discomfort and/or associated complications [<xref rid="B1-ijerph-18-02428" ref-type="bibr">1</xref>], is among the diseases with the highest prevalence over the past two decades [<xref rid="B2-ijerph-18-02428" ref-type="bibr">2</xref>,<xref rid="B3-ijerph-18-02428" ref-type="bibr">3</xref>]. GERD can be classified as either erosive or non-erosive esophagitis and is characterized by endoscopically visible breaks in the distal esophageal mucosa in the former category and a lack of such breaks in the latter [<xref rid="B4-ijerph-18-02428" ref-type="bibr">4</xref>,<xref rid="B5-ijerph-18-02428" ref-type="bibr">5</xref>]. Although double-contrast barium swallow examination has been previously used to diagnose GERD [<xref rid="B6-ijerph-18-02428" ref-type="bibr">6</xref>], esophagogastroduodenoscopy (EGD) is now the gold standard test for suspected GERD to evaluate the alarming features and/or the possibility of Barrett&#x02019;s esophagus in high-risk patients [<xref rid="B7-ijerph-18-02428" ref-type="bibr">7</xref>,<xref rid="B8-ijerph-18-02428" ref-type="bibr">8</xref>]. In addition, the popular and powerful Los Angeles classification (LA grade) system, which was established more than 20 years ago, is used in endoscopy examinations to classify GERD [<xref rid="B9-ijerph-18-02428" ref-type="bibr">9</xref>].</p><p>Previous studies have focused on inter-observer [<xref rid="B10-ijerph-18-02428" ref-type="bibr">10</xref>] and intra-observer [<xref rid="B11-ijerph-18-02428" ref-type="bibr">11</xref>] variations of the LA grade, and those results found the agreement on LA grade between experienced endoscopists to be better than that of less experienced endoscopists [<xref rid="B11-ijerph-18-02428" ref-type="bibr">11</xref>,<xref rid="B12-ijerph-18-02428" ref-type="bibr">12</xref>]. Compared to conventional endoscopy, which is referred to as white light endoscopy, narrow-band image (NBI), which filters out the red spectrum of light, improves the consistency of esophagitis grading in both inter-observer and intra-observer settings in the previous study [<xref rid="B13-ijerph-18-02428" ref-type="bibr">13</xref>].</p><p>Due to technological improvements in artificial intelligence (AI), basic screening surveillance methods, e.g., chest X-rays, are likely to be replaced by AI in the near future. Deep learning comprises a series of computational methods that allows an algorithm to program itself by learning from a large number of examples that demonstrate the desired behavior without the need to regulate the rules. Compared to traditional machine learning algorithms that must extract image features based on manual experience, deep learning is a hierarchical feature learning architecture that can automatically capture features from image data for disease prediction. There are successful examples of AI applications in the medical field, e.g., radiology image interpretation [<xref rid="B14-ijerph-18-02428" ref-type="bibr">14</xref>,<xref rid="B15-ijerph-18-02428" ref-type="bibr">15</xref>], obstructive pulmonary disease recognition in computed tomography [<xref rid="B16-ijerph-18-02428" ref-type="bibr">16</xref>], diabetic retinopathy screening [<xref rid="B17-ijerph-18-02428" ref-type="bibr">17</xref>,<xref rid="B18-ijerph-18-02428" ref-type="bibr">18</xref>,<xref rid="B19-ijerph-18-02428" ref-type="bibr">19</xref>], esophageal cancer endoscopic diagnosis [<xref rid="B20-ijerph-18-02428" ref-type="bibr">20</xref>], dysplasia in Barrett&#x02019;s esophagus, and detection of early gastric cancers [<xref rid="B21-ijerph-18-02428" ref-type="bibr">21</xref>]. Therefore, AI can assist or even replace basic interpretation techniques in the medical field.</p><p>To the best of our knowledge, only two studies have investigated GERD prediction using AI technologies. One study suggested that the combination between the QUestionario Italiano Diagnostico (QUID) questionnaire and an artificial neural network (ANN)-assisted algorithm is useful to differentiate GERD patients from healthy individuals but fails to further discriminate erosive from non-erosive patients [<xref rid="B22-ijerph-18-02428" ref-type="bibr">22</xref>]. The other study proposed a hierarchical heterogeneous descriptor fusion support vector machine (HHDF-SVM) method for GERD diagnosis from conventional endoscopic images [<xref rid="B23-ijerph-18-02428" ref-type="bibr">23</xref>]. However, the AI systems are only applicable to GERD prediction in binary classification. Therefore, to the best of our knowledge, our study is the first to develop a deep learning model for computer-aided diagnosis that focuses on automatic grading of GERD according to LA grades.</p><p>We attempted to train AI to identify GERD endoscopic features using the LA classification by developing a deep learning model from endoscopic images of the esophago-cardiac junction (EC-J). We then compared the performance of the endoscopic images under conventional and NBI endoscopy by the proposed AI model. We further evaluated the accuracy of the AI predictions and the results from trainees in an endoscopy society.</p><p>The remainder of this paper is organized as follows: the materials and methods are introduced in the <xref ref-type="sec" rid="sec2-ijerph-18-02428">Section 2</xref>. In the <xref ref-type="sec" rid="sec3-ijerph-18-02428">Section 3</xref>, the results and analysis of why improved results were obtained using the proposed method are explained in detail. The data results and results from other studies are discussed in the <xref ref-type="sec" rid="sec4-ijerph-18-02428">Section 4</xref>. The <xref ref-type="sec" rid="sec5-ijerph-18-02428">Section 5</xref> is the conclusion.</p></sec><sec id="sec2-ijerph-18-02428"><title>2. Materials and Methods</title><p>For AI training development, endoscopic pictures of the EG-J were obtained retrospectively from the endoscopic system at Chung Shan Medical University Hospital. The quality of the endoscopic images and GERD classification were confirmed by two instructors at the Digestive Endoscopy Society of Taiwan. The images were taken from the records of 496 people who had received an EGD exam for either symptomatic diseases or as a health examination between December 2019 and March 2020. All images were deidentified prior to transfer to the study&#x02019;s investigators, and all methods were performed according to relevant local regulations under the surveillance of the Institutional Review Board of Chung Shan Medical University Hospital.</p><sec id="sec2dot1-ijerph-18-02428"><title>2.1. Grading</title><p>We found that all endoscopic images were adequate for reviewing the entire structure of the EC-J. Notably, the brightness and contrast of the images were not artificially altered. The LA grading system was employed as the evaluation scale. An LA grade of A is described as mucosal breaks no longer than 5 mm that do not extend between the tops of two mucosal folds, a grade B includes mucosal breaks of more than 5 mm in length that do not extend between the tops of two mucosal folds; and a C&#x02013;D grade includes one (or more) mucosal break that are continuous between the tops of two or more mucosal folds but involve equal to or less than 75% of the circumference. The image samples are listed in <xref ref-type="fig" rid="ijerph-18-02428-f001">Figure 1</xref>. We divided all images into three groups, i.e., LA grade A&#x02013;B, LA grade C&#x02013;D, and normal EC-J.</p></sec><sec id="sec2dot2-ijerph-18-02428"><title>2.2. Study Design</title><p>In this study, 2000 adult cases were collected retrospectively from the endoscopic system at Chung Shan Medical University Hospital from December 2019 to March 2020. Here, an image quality evaluation was performed to confirm the intactness of the EC-J image, identify the image resolution, and identify any foreign body interference. The endoscopic images of the EC-J from the 496 patients that passed the quality evaluation were then divided into development and test sets. Images of 464 cases were selected as the development set; however, not all cases had both conventional and NBI pictures. Eventually, we obtained 247 GERD A&#x02013;B images, 155 GERD C&#x02013;D images, and 62 normal EC-J images from the conventional images. Initially, we obtained 244 GERD A&#x02013;B images, 157 GERD C&#x02013;D, images, and 48 normal EC-J images from the NBI images. Note that the original image set was a clinical dataset; thus, data imbalance was evident. In addition, the GERD images had rotation invariance; therefore, this study first employed a static data augmentation approach to overcome image skewness in some categories. Specifically, 222 and 233 images were augmented in the NBI and conventional modes by rotating the axis of the original GERD C&#x02013;D and normal EC-J images, respectively. Finally, we constructed a balanced development set for AI model training and internal validation. For external validation, we reserved 32 images to test the recognition of the trained AI system and inexperienced trainees. The tests of the young trainees, who were blinded to this study, were performed using an email system. A detailed flowchart of the study design is shown in <xref ref-type="fig" rid="ijerph-18-02428-f002">Figure 2</xref>.</p></sec><sec id="sec2dot3-ijerph-18-02428"><title>2.3. Model Development</title><p>In this study, the visual geometry group (VGG) neural network model pretrained by ImageNet [<xref rid="B24-ijerph-18-02428" ref-type="bibr">24</xref>] was employed as the base model for image feature extraction. This technique adjusts the structure of certain pretrained neural network models using transfer learning [<xref rid="B25-ijerph-18-02428" ref-type="bibr">25</xref>] to perform other different image classification tasks.</p><p>In consideration of the balance between network capacity and validation accuracy and by discussing the influence of different regularization and optimization strategies [<xref rid="B26-ijerph-18-02428" ref-type="bibr">26</xref>], we designed a deep convolutional neural network (CNN) architecture with high generalizability. The proposed CNN architecture is called GERD-VGGNet (<xref ref-type="fig" rid="ijerph-18-02428-f003">Figure 3</xref>).</p><p>The proposed GERD-VGGNet architecture includes 13 convolutional layers, five max pooling layers, one global average pooling layer, four dense layers, four batch normalization layers [<xref rid="B27-ijerph-18-02428" ref-type="bibr">27</xref>], four activation layers using the rectified linear unit (ReLU) function [<xref rid="B26-ijerph-18-02428" ref-type="bibr">26</xref>,<xref rid="B28-ijerph-18-02428" ref-type="bibr">28</xref>], and the last dense layer with softmax classification.</p><p>This model employs the Adam optimizer with a batch size of 64 examples for two-stage optimization training of the entire network architecture using a non-freezing transfer learning method [<xref rid="B26-ijerph-18-02428" ref-type="bibr">26</xref>]. Here, in the first training stage, the learning rate is set to <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for network training up to 600 epochs. The second fine-tuning stage uses 400 epochs for network tuning with a smaller learning rate of <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To enhance the generalizability of the model, dynamic data augmentation is considered an effective method to train a generally applicable model using a limited amount of training data [<xref rid="B26-ijerph-18-02428" ref-type="bibr">26</xref>,<xref rid="B29-ijerph-18-02428" ref-type="bibr">29</xref>]. To make up for the lack of data, a dynamic data augmentation technique is included in the training. After applying image translation and flipping processing, the original images in the training subset of the development set were altered to create more images to allow the model to continue learning. Specifically, for image translation, we used random shifts in a maximum range of 20% of the total width or height of the image. For image flipping, we randomly applied horizontal and vertical flips. Taking NBI endoscopy as an example, we ended up with a total of ((244 + 229 + 198) &#x000d7; 0.9) &#x000d7; 1000 = 603 &#x000d7; 1000 = 603,000 training images after 1000 learning epochs. It is worth noting that data augmentation should be not performed on the validation set and the test set.</p><p>The entire training process applied a callback mechanism to store a copy of the network model parameters each time the accuracy of the validation set was improved. After the training algorithm was terminated, the best network model parameters were selected using the early stopping criterion [<xref rid="B30-ijerph-18-02428" ref-type="bibr">30</xref>] to obtain the model with the lowest validation set error. Here, we expected that the verification task would support improved generalizability.</p></sec><sec id="sec2dot4-ijerph-18-02428"><title>2.4. Model Evaluation</title><p>To evaluate the advantages and disadvantages of the proposed model, we applied 10-fold cross-validation for verification, where the development set was randomly divided into 10 subsets by selecting one subset as the validation set and considering the remaining nine subsets as the training set. This experiment was repeated 10 times until each subset was used as a validation set. Finally, the average and standard deviation of the classification results of all 10 experiments were calculated as indicators of the model&#x02019;s quality.</p><p>The GERD images were recombined into three categories of classification problems, i.e., LA grade A&#x02013;B, LA grade B&#x02013;C, and normal. In addition, a confusion matrix was used as a model performance evaluation tool; the overall rate of accuracy and rate of accuracy for each category were calculated.</p></sec><sec id="sec2dot5-ijerph-18-02428"><title>2.5. Classifier Performance Comparison</title><p>The statistic <italic>Ps</italic> [<xref rid="B31-ijerph-18-02428" ref-type="bibr">31</xref>], which uses the classical hypothesis testing paradigm to compare the performance of classifier models M1 and M2, is expressed follows:<disp-formula id="FD1-ijerph-18-02428"><label>(1)</label><mml:math id="mm3"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where, <italic>E</italic><sub>1</sub> and <italic>E</italic><sub>2</sub> are the error rates for models M1 and M2, respectively, <italic>q</italic> is (<italic>E</italic><sub>1</sub> + <italic>E</italic><sub>2</sub>)/2, and <italic>n</italic> is the number of examples in the test set. If the value of <italic>Ps</italic> &#x02265; 2, one can be 95% confident that the difference in the test set performance between models M1 and M2 is significant.</p><p>The proposed GERD-VGGNet model was developed using the Python programming language (version 3.8), the TensorFlow 2.3 framework, the Pandas library (version 1.2), and the NumPy library (version 1.19). The complete training and testing processes were performed on an Nvidia GTX 1080 Ti (11 GB RAM) with CUDA version 10.2 and cuDNN version 7.</p></sec></sec><sec sec-type="results" id="sec3-ijerph-18-02428"><title>3. Results and Analysis</title><p>The development and test sets were collected to evaluate the model&#x02019;s training and performance. These sets were mutually independent, and there was no identical image record data. <xref rid="ijerph-18-02428-t001" ref-type="table">Table 1</xref> lists the patient numbers and image numbers in the development and test sets.</p><sec id="sec3dot1-ijerph-18-02428"><title>3.1. Analysis of Experimental Results</title><p>Different pretrained models have different degrees of image feature extraction ability. In this study, the development set was employed to compare four common pretrained models, i.e., VGG16 [<xref rid="B24-ijerph-18-02428" ref-type="bibr">24</xref>], ResNet50 [<xref rid="B32-ijerph-18-02428" ref-type="bibr">32</xref>], ResNet101 [<xref rid="B32-ijerph-18-02428" ref-type="bibr">32</xref>], and InceptionV3 [<xref rid="B33-ijerph-18-02428" ref-type="bibr">33</xref>], into the proposed classification model. Classification accuracy was evaluated to determine which model was the most appropriate for use as the pretrained model. The results of the training using the four models are compared in <xref ref-type="fig" rid="ijerph-18-02428-f004">Figure 4</xref>. In addition, the time costs of model training were evaluated. Here Resnet 101 required the most time, which consumed more CPU time than inceptionv3, res-net50, and VGG16 (in order of decreasing time cost). The results confirm that the pretrained VGG16 model demonstrated the best training accuracy, validation accuracy, and lowest time costs. Thus, in subsequent testing, the pretrained VGG16 model was used as the image feature extraction model.</p><p><xref ref-type="fig" rid="ijerph-18-02428-f005">Figure 5</xref>a shows the model training history in the NBI mode without dynamic data augmentation. As can be seen, the validation accuracy trained by the model shows worse performance; however, with dynamic data augmentation, as shown in <xref ref-type="fig" rid="ijerph-18-02428-f005">Figure 5</xref>b, the validation accuracy is better performance, which demonstrates showing that the dynamic data augmentation technique improved classification performance (<xref ref-type="fig" rid="ijerph-18-02428-f006">Figure 6</xref>).</p><p>In fact, the number of original training images was relatively small, i.e., only 603 samples were used in the no data augmentation case. After 1000 learning epochs, the number of augmented training images was increased to 603,000 samples using dynamic data augmentation. As shown in <xref ref-type="fig" rid="ijerph-18-02428-f006">Figure 6</xref>, the experimental results clearly indicate that implementing data augmentation in the training process realized better accuracy for both the training and validation sets than training without data augmentation. The results shown in <xref ref-type="fig" rid="ijerph-18-02428-f006">Figure 6</xref> demonstrate that accuracy was improved from 98.5% to 100% in the training set and 59.5% to 89.3% in the validation set using data augmentation. When data augmentation was used for model training, the trained model reduced the overfitting phenomenon effectively. In addition, data augmentation ensured good model generalizability.</p><p><xref ref-type="fig" rid="ijerph-18-02428-f007">Figure 7</xref> compares the classification performance of the proposed GERD-VGGNet model and four machine learning models, i.e., the RBF-SVM, Decision Tree, Random Forest, and Adaboost classifiers. As can be seen, the training and validation accuracies of the proposed GERD-VGGNet are better than that of the compared classifiers.</p></sec><sec id="sec3dot2-ijerph-18-02428"><title>3.2. Model Training and Validation Performance Evaluation</title><p>After conducting 10-fold cross-validation, the training, validation, and overall (mean &#x000b1; standard deviation) accuracy rates were 1.000 &#x000b1; 0.001, 0.893 &#x000b1; 0.050, and 0.989 &#x000b1; 0.005, respectively, in NBI mode. For the conventional mode, the training, validation, and overall accuracy rates were 1.000 &#x000b1; 0.001, 0.865 &#x000b1; 0.042, and 0.986 &#x000b1; 0.004, respectively. These results demonstrate that the model training and validation performance of NBI mode was slightly better than that of the conventional mode. The confusion matrix of the proposed GERD-VGGNet model for the NBI development set is shown in <xref rid="ijerph-18-02428-t002" ref-type="table">Table 2</xref>. Here, the results demonstrate that the overall accuracy rate was up to 0.997, and two images were misclassified in each of the A&#x02013;B and C&#x02013;D categories. In contrast, with the conventional mode, the overall accuracy rate was 0.994 (<xref rid="ijerph-18-02428-t002" ref-type="table">Table 2</xref>), and two images were misclassified in the A&#x02013;B category, three images were misclassified in the C&#x02013;D category, and one image was misclassified in the normal category.</p></sec><sec id="sec3dot3-ijerph-18-02428"><title>3.3. Model Testing Performance Evaluation</title><p><xref rid="ijerph-18-02428-t003" ref-type="table">Table 3</xref> shows the confusion matrix of the proposed model and the results of the trainees. In the NBI case, the results demonstrate that the proposed GERD-VGGNet misclassified two images of GERD A&#x02013;B grade, one image of GERD C&#x02013;D grade, and one image of normal. In addition, the overall accuracy rates were 0.875 for GERD-VGGNet, 0.750 for trainee 1, and 0.656 for trainee 2. The accuracy rates for the A&#x02013;B category were 0.833 for GERD-VGGNet, 0.750 for trainee 1, and 0.417 for trainee 2. The accuracy rates for the C&#x02013;D category were 1.0 for GERD-VGGNet, 0.7 for trainee 1, and 0.7 for trainee 2. The correct rates for the normal category were 0.8 for GERD-VGGNet, 0.8 for trainee 1, and 0.9 for trainee 2. The overall accuracy under NBI endoscopy appeared to be better in the AI model than it was for the trainees.</p><p>For the conventional mode, the results demonstrate that seven imagers were misclassified by the proposed GERD-VGGNet), 10 images were misclassified by trainee 1, and seven images were misclassified by trainee 2. This means that the overall accuracy was 0.781 for GERD-VGGNet, 0.688 for trainee 1, and 0.781 for trainee 2. The accuracy rates of the A&#x02013;B category were 0.917 for the proposed GERD-VGGNet, 0.667 for trainee 1, and 0.667 for trainee 2. For the C&#x02013;D category, the accuracy rates were 0.6 for the proposed GERD-VGGNet, 0.7 for trainee 1, and 0.8 for trainee 2. The correct rates for the normal category were 0.8 for the proposed GERD-VGGNet, 0.7 for trainee 1, and 0.9 for trainee 2. The overall accuracy under conventional endoscopy was similar between the AI model and the trainees (<xref rid="ijerph-18-02428-t003" ref-type="table">Table 3</xref>).</p><p><xref rid="ijerph-18-02428-t004" ref-type="table">Table 4</xref> compares the Ps values of the four classifier models. Here, we conclude that, by using the NBI mode, the proposed GERD-VGGNet outperformed trainee 2 at 95% confidence level, as marked * in <xref rid="ijerph-18-02428-t004" ref-type="table">Table 4</xref>. In addition, the proposed GERD-VGGNet outperformed trainee 1 under NBI endoscopy; however, statistical significance was not observed in this case.</p><p><xref rid="ijerph-18-02428-t005" ref-type="table">Table 5</xref> compares the performance of the proposed GERD-VGGNet model and the existing methods [<xref rid="B22-ijerph-18-02428" ref-type="bibr">22</xref>,<xref rid="B23-ijerph-18-02428" ref-type="bibr">23</xref>]. As shown in <xref rid="ijerph-18-02428-t005" ref-type="table">Table 5</xref>, the proposed model outperformed the method proposed by Huang et al. [<xref rid="B23-ijerph-18-02428" ref-type="bibr">23</xref>]. In addition, the proposed model demonstrated similar performance as the method proposed by Pace et al. [<xref rid="B22-ijerph-18-02428" ref-type="bibr">22</xref>]. Note that the existing methods [<xref rid="B22-ijerph-18-02428" ref-type="bibr">22</xref>,<xref rid="B23-ijerph-18-02428" ref-type="bibr">23</xref>] were only applicable to binary classification of GERD, and one method [<xref rid="B22-ijerph-18-02428" ref-type="bibr">22</xref>] used questionnaire data rather than image data. We found that direct use of image data to predict the GERD grade is more beneficial to clinical diagnosis than collecting questionnaire data.</p></sec></sec><sec sec-type="discussion" id="sec4-ijerph-18-02428"><title>4. Discussion</title><sec id="sec4dot1-ijerph-18-02428"><title>4.1. Model Training and Validation Performance</title><p>The original image set was a clinical dataset; thus, data imbalance was evident in the dataset. Therefore, we employed data augmentation to overcome the image skewness problem in some categories. We then applied a dynamic data augmentation approach for AI model training. The experimental results clearly demonstrate that data augmentation is key to training a deep learning neural network. The results shown in <xref ref-type="fig" rid="ijerph-18-02428-f005">Figure 5</xref> and <xref ref-type="fig" rid="ijerph-18-02428-f006">Figure 6</xref> clearly demonstrate that the trained model exhibited serious overfitting problems when data augmentation technology was not applied. In contrast, with data augmentation, the established model effectively reduced the overfitting phenomenon and demonstrated good generalizability.</p><p>In our analysis, the AI interpretation of GERD endoscopic classifications improved after deep learning, and the validation quality was good, especially after at least 600 learning epochs (<xref ref-type="fig" rid="ijerph-18-02428-f004">Figure 4</xref>). The external validation demonstrated that the test accuracy of the proposed GERD-VGGNet model was 91.7% for the conventional A&#x02013;B grade, 60% for the conventional C&#x02013;D grade, and 80% for the normal group). In addition, the external validation showed that the test accuracy of the proposed GERD-VGGNet model under NBI endoscopy was 83.3% for the A&#x02013;B grade, 100% for the C&#x02013;D grade, and 80% for the normal group. The overall prediction accuracy for normal cases increased under NBI endoscopy compared to conventional endoscopy, and this phenomenon is consistent with previous studies that investigated manual interpretation [<xref rid="B13-ijerph-18-02428" ref-type="bibr">13</xref>].</p><p>Overall, the experimental results indicate that the proposed method can automatically diagnose and grade GERD without manual selection of a region of interest, with automatic feature extraction from image data, and achieve better accuracy compared to state-of-the-art AI systems for endoscopic GERD classification. To the best of our knowledge, this study is the first to develop a deep learning model for computer-aided diagnosis and automatic GERD grading according to LA grades.</p></sec><sec id="sec4dot2-ijerph-18-02428"><title>4.2. Performance of NBI in AI Prediction</title><p>The prediction accuracy under NBI was significantly better with the proposed GERD-VGGNet model compared to trainee 2. The proposed GERD-VGGNet model under NBI endoscopy obtained higher accuracy than conventional endoscopy; however, the difference did not demonstrate statistical significance. These results suggest that interpretation of the AI model can be influenced by image contrast, which implies that NBI images can be interpreted with better accuracy. NBI endoscopy enhances the contrast of the mucosal surface and helps diagnose and grade GERD in manual interpretations [<xref rid="B34-ijerph-18-02428" ref-type="bibr">34</xref>]. This effect is similar to previous NBI applications in GERD [<xref rid="B34-ijerph-18-02428" ref-type="bibr">34</xref>,<xref rid="B35-ijerph-18-02428" ref-type="bibr">35</xref>] and in the NBI-guided diagnosis of Barrett&#x02019;s esophagus in England [<xref rid="B36-ijerph-18-02428" ref-type="bibr">36</xref>], but it was first confirmed in the proposed AI prediction model.</p></sec><sec id="sec4dot3-ijerph-18-02428"><title>4.3. Limitations</title><p>This was a pioneer study in endoscopic GERD LA classification comparisons with trainees; thus, we acknowledge that the number of examined cases was small. In addition, the comparison of conventional endoscopy and NBI endoscopy further limited our case numbers because NBI observation is not performed routinely in our daily practice, particularly in case of normal or grade A GERD under initial conventional endoscopy. Although our test set was small, statistical significance was observed between the proposed GERD-VGGNet model and trainee 2 under the NBI endoscopy. Thus, due to the limited amount of data, future large-scale studies are required to further confirm the results presented in this paper.</p></sec></sec><sec sec-type="conclusions" id="sec5-ijerph-18-02428"><title>5. Conclusions</title><p>In this paper, we have proposed the GERD-VGGNet model. The experimental results have confirmed that the proposed model and training strategies can automatically diagnose and grade GERD without manual selection of a region of interest and achieves higher accuracy than state-of-the-art AI systems. Given the outcomes of the interpretation of LA GERD classification in the AI model, we believe that the proposed GERD-VGGNet model can assist endoscopic findings for trainees and that NBI endoscopy increases the accuracy of the interpretation results in AI systems, which has been previously demonstrated in manual interpretations.</p><p>In future research, we will try to integrate different XAI (explainable artificial intelligence) analysis technologies and attention models to improve interpretation capabilities of the proposed AI model.</p></sec></body><back><ack><title>Acknowledgments</title><p>Thanks for the contribution of high quality endoscopic image to all the coworkers in Endoscopy Room of Chung Shan Medical University Hospital.</p></ack><fn-group><fn><p><bold>Publisher&#x02019;s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conception and design: C.-C.W., M.-H.T., and M.-C.T. Acquisition of data: C.-C.W. and W.-L.C. Analysis and interpretation of data: Y.-C.C. and M.-H.T. Drafting of the manuscript: C.-C.W., T.-W.Y., and M.-H.T. Supervision: M.-H.T. and M.-C.T. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Funding</title><p>This research was partially funded by the Ministry of Science and Technology, Taiwan, grant number MOST 109-2121-M-040-001.</p></notes><notes><title>Institutional Review Board Statement</title><p>All images were de-identified prior to their transfer to the study&#x02019;s investigators, and all methods were performed in accordance with the relevant local regulations and under the surveillance of the Institutional Review Board (IRB) of Chung Shan Medical University Hospital.</p></notes><notes><title>Informed Consent Statement</title><p>Patient consent was waived due to only endoscopic images were obtained for this retrospective study without any clinical intervention to patients.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>All the data of images and analysis process were kept at the lab of M.-H.T.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>All authors declare no any potential financial and non-financial conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-ijerph-18-02428"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vakil</surname><given-names>N.</given-names></name><name><surname>van Zanten</surname><given-names>S.V.</given-names></name><name><surname>Kahrilas</surname><given-names>P.</given-names></name><name><surname>Dent</surname><given-names>J.</given-names></name><name><surname>Jones</surname><given-names>R.</given-names></name><name><surname>Global Consensus</surname><given-names>G.</given-names></name></person-group><article-title>The Montreal definition and classification of gastroesophageal reflux disease: A global evidence-based consensus</article-title><source>Am. J. Gastroenterol.</source><year>2006</year><volume>101</volume><fpage>1900</fpage><lpage>1920</lpage><pub-id pub-id-type="doi">10.1111/j.1572-0241.2006.00630.x</pub-id><pub-id pub-id-type="pmid">16928254</pub-id></element-citation></ref><ref id="B2-ijerph-18-02428"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dent</surname><given-names>J.</given-names></name><name><surname>El-Serag</surname><given-names>H.B.</given-names></name><name><surname>Wallander</surname><given-names>M.A.</given-names></name><name><surname>Johansson</surname><given-names>S.</given-names></name></person-group><article-title>Epidemiology of gastro-oesophageal reflux disease: A systematic review</article-title><source>Gut</source><year>2005</year><volume>54</volume><fpage>710</fpage><lpage>717</lpage><pub-id pub-id-type="doi">10.1136/gut.2004.051821</pub-id><pub-id pub-id-type="pmid">15831922</pub-id></element-citation></ref><ref id="B3-ijerph-18-02428"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname><given-names>J.E.</given-names></name><name><surname>Rubenstein</surname><given-names>J.H.</given-names></name></person-group><article-title>Presentation and Epidemiology of Gastroesophageal Reflux Disease</article-title><source>Gastroenterology</source><year>2018</year><volume>154</volume><fpage>267</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1053/j.gastro.2017.07.045</pub-id><?supplied-pmid 28780072?><pub-id pub-id-type="pmid">28780072</pub-id></element-citation></ref><ref id="B4-ijerph-18-02428"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iwakiri</surname><given-names>K.</given-names></name><name><surname>Kinoshita</surname><given-names>Y.</given-names></name><name><surname>Habu</surname><given-names>Y.</given-names></name><name><surname>Oshima</surname><given-names>T.</given-names></name><name><surname>Manabe</surname><given-names>N.</given-names></name><name><surname>Fujiwara</surname><given-names>Y.</given-names></name><name><surname>Nagahara</surname><given-names>A.</given-names></name><name><surname>Kawamura</surname><given-names>O.</given-names></name><name><surname>Iwakiri</surname><given-names>R.</given-names></name><name><surname>Ozawa</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Evidence-based clinical practice guidelines for gastroesophageal reflux disease 2015</article-title><source>J. Gastroenterol.</source><year>2016</year><volume>51</volume><fpage>751</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1007/s00535-016-1227-8</pub-id><pub-id pub-id-type="pmid">27325300</pub-id></element-citation></ref><ref id="B5-ijerph-18-02428"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mc</surname><given-names>H.G.</given-names></name><name><surname>Mc</surname><given-names>H.R.</given-names></name><name><surname>Craighead</surname><given-names>C.C.</given-names></name></person-group><article-title>Erosive esophagitis: Recurrent, reflux, peptic esophagitis; 80 adult cases</article-title><source>GP</source><year>1957</year><volume>16</volume><fpage>75</fpage><lpage>83</lpage><pub-id pub-id-type="pmid">13474125</pub-id></element-citation></ref><ref id="B6-ijerph-18-02428"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sellar</surname><given-names>R.J.</given-names></name><name><surname>De Caestecker</surname><given-names>J.S.</given-names></name><name><surname>Heading</surname><given-names>R.C.</given-names></name></person-group><article-title>Barium radiology: A sensitive test for gastro-oesophageal reflux</article-title><source>Clin. Radiol.</source><year>1987</year><volume>38</volume><fpage>303</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1016/S0009-9260(87)80077-6</pub-id><pub-id pub-id-type="pmid">3581674</pub-id></element-citation></ref><ref id="B7-ijerph-18-02428"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krugmann</surname><given-names>J.</given-names></name><name><surname>Neumann</surname><given-names>H.</given-names></name><name><surname>Vieth</surname><given-names>M.</given-names></name><name><surname>Armstrong</surname><given-names>D.</given-names></name></person-group><article-title>What is the role of endoscopy and oesophageal biopsies in the management of GERD?</article-title><source>Best Pract. Res. Clin. Gastroenterol.</source><year>2013</year><volume>27</volume><fpage>373</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1016/j.bpg.2013.06.010</pub-id><pub-id pub-id-type="pmid">23998976</pub-id></element-citation></ref><ref id="B8-ijerph-18-02428"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>D.</given-names></name></person-group><article-title>Endoscopic evaluation of gastro-esophageal reflux disease</article-title><source>Yale J. Biol. Med.</source><year>1999</year><volume>72</volume><fpage>93</fpage><lpage>100</lpage><pub-id pub-id-type="pmid">10780570</pub-id></element-citation></ref><ref id="B9-ijerph-18-02428"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tefera</surname><given-names>L.</given-names></name><name><surname>Fein</surname><given-names>M.</given-names></name><name><surname>Ritter</surname><given-names>M.P.</given-names></name><name><surname>Bremner</surname><given-names>C.G.</given-names></name><name><surname>Crookes</surname><given-names>P.F.</given-names></name><name><surname>Peters</surname><given-names>J.H.</given-names></name><name><surname>Hagen</surname><given-names>J.A.</given-names></name><name><surname>DeMeester</surname><given-names>T.R.</given-names></name></person-group><article-title>Can the combination of symptoms and endoscopy confirm the presence of gastroesophageal reflux disease?</article-title><source>Am. Surg.</source><year>1997</year><volume>63</volume><fpage>933</fpage><lpage>936</lpage><pub-id pub-id-type="pmid">9322676</pub-id></element-citation></ref><ref id="B10-ijerph-18-02428"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>S.H.</given-names></name><name><surname>Jang</surname><given-names>B.I.</given-names></name><name><surname>Kim</surname><given-names>K.O.</given-names></name><name><surname>Jeon</surname><given-names>S.W.</given-names></name><name><surname>Kwon</surname><given-names>J.G.</given-names></name><name><surname>Kim</surname><given-names>E.Y.</given-names></name><name><surname>Jung</surname><given-names>J.T.</given-names></name><name><surname>Park</surname><given-names>K.S.</given-names></name><name><surname>Cho</surname><given-names>K.B.</given-names></name><name><surname>Kim</surname><given-names>E.S.</given-names></name><etal/></person-group><article-title>Endoscopic experience improves interobserver agreement in the grading of esophagitis by Los Angeles classification: Conventional endoscopy and optimal band image system</article-title><source>Gut Liver</source><year>2014</year><volume>8</volume><fpage>154</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.5009/gnl.2014.8.2.154</pub-id><pub-id pub-id-type="pmid">24672656</pub-id></element-citation></ref><ref id="B11-ijerph-18-02428"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusano</surname><given-names>M.</given-names></name><name><surname>Ino</surname><given-names>K.</given-names></name><name><surname>Yamada</surname><given-names>T.</given-names></name><name><surname>Kawamura</surname><given-names>O.</given-names></name><name><surname>Toki</surname><given-names>M.</given-names></name><name><surname>Ohwada</surname><given-names>T.</given-names></name><name><surname>Kikuchi</surname><given-names>K.</given-names></name><name><surname>Shirota</surname><given-names>T.</given-names></name><name><surname>Kimura</surname><given-names>M.</given-names></name><name><surname>Miyazaki</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Interobserver and intraobserver variation in endoscopic assessment of GERD using the &#x0201c;Los Angeles&#x0201d; classification</article-title><source>Gastrointest. Endosc.</source><year>1999</year><volume>49</volume><fpage>700</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1016/S0016-5107(99)70285-3</pub-id><pub-id pub-id-type="pmid">10343212</pub-id></element-citation></ref><ref id="B12-ijerph-18-02428"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandolfino</surname><given-names>J.E.</given-names></name><name><surname>Vakil</surname><given-names>N.B.</given-names></name><name><surname>Kahrilas</surname><given-names>P.J.</given-names></name></person-group><article-title>Comparison of inter- and intraobserver consistency for grading of esophagitis by expert and trainee endoscopists</article-title><source>Gastrointest. Endosc.</source><year>2002</year><volume>56</volume><fpage>639</fpage><lpage>643</lpage><pub-id pub-id-type="doi">10.1016/S0016-5107(02)70110-7</pub-id><pub-id pub-id-type="pmid">12397269</pub-id></element-citation></ref><ref id="B13-ijerph-18-02428"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y.C.</given-names></name><name><surname>Lin</surname><given-names>J.T.</given-names></name><name><surname>Chiu</surname><given-names>H.M.</given-names></name><name><surname>Liao</surname><given-names>W.C.</given-names></name><name><surname>Chen</surname><given-names>C.C.</given-names></name><name><surname>Tu</surname><given-names>C.H.</given-names></name><name><surname>Tai</surname><given-names>C.M.</given-names></name><name><surname>Chiang</surname><given-names>T.H.</given-names></name><name><surname>Chiu</surname><given-names>Y.H.</given-names></name><name><surname>Wu</surname><given-names>M.S.</given-names></name><etal/></person-group><article-title>Intraobserver and interobserver consistency for grading esophagitis with narrow-band imaging</article-title><source>Gastrointest. Endosc.</source><year>2007</year><volume>66</volume><fpage>230</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/j.gie.2006.10.056</pub-id><?supplied-pmid 17643694?><pub-id pub-id-type="pmid">17643694</pub-id></element-citation></ref><ref id="B14-ijerph-18-02428"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosny</surname><given-names>A.</given-names></name><name><surname>Parmar</surname><given-names>C.</given-names></name><name><surname>Quackenbush</surname><given-names>J.</given-names></name><name><surname>Schwartz</surname><given-names>L.H.</given-names></name><name><surname>Aerts</surname><given-names>H.</given-names></name></person-group><article-title>Artificial intelligence in radiology</article-title><source>Nat. Rev. Cancer</source><year>2018</year><volume>18</volume><fpage>500</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1038/s41568-018-0016-5</pub-id><?supplied-pmid 29777175?><pub-id pub-id-type="pmid">29777175</pub-id></element-citation></ref><ref id="B15-ijerph-18-02428"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>L.Q.</given-names></name><name><surname>Wang</surname><given-names>J.Y.</given-names></name><name><surname>Yu</surname><given-names>S.Y.</given-names></name><name><surname>Wu</surname><given-names>G.G.</given-names></name><name><surname>Wei</surname><given-names>Q.</given-names></name><name><surname>Deng</surname><given-names>Y.B.</given-names></name><name><surname>Wu</surname><given-names>X.L.</given-names></name><name><surname>Cui</surname><given-names>X.W.</given-names></name><name><surname>Dietrich</surname><given-names>C.F.</given-names></name></person-group><article-title>Artificial intelligence in medical imaging of the liver</article-title><source>World J. Gastroenterol.</source><year>2019</year><volume>25</volume><fpage>672</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.3748/wjg.v25.i6.672</pub-id><?supplied-pmid 30783371?><pub-id pub-id-type="pmid">30783371</pub-id></element-citation></ref><ref id="B16-ijerph-18-02428"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>N.</given-names></name><name><surname>Topalovic</surname><given-names>M.</given-names></name><name><surname>Janssens</surname><given-names>W.</given-names></name></person-group><article-title>Artificial intelligence in diagnosis of obstructive lung disease: Current status and future potential</article-title><source>Curr. Opin. Pulm. Med.</source><year>2018</year><volume>24</volume><fpage>117</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1097/MCP.0000000000000459</pub-id><pub-id pub-id-type="pmid">29251699</pub-id></element-citation></ref><ref id="B17-ijerph-18-02428"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>T.Y.</given-names></name><name><surname>Bressler</surname><given-names>N.M.</given-names></name></person-group><article-title>Artificial Intelligence With Deep Learning Technology Looks Into Diabetic Retinopathy Screening</article-title><source>JAMA</source><year>2016</year><volume>316</volume><fpage>2366</fpage><lpage>2367</lpage><pub-id pub-id-type="doi">10.1001/jama.2016.17563</pub-id><pub-id pub-id-type="pmid">27898977</pub-id></element-citation></ref><ref id="B18-ijerph-18-02428"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanagasingam</surname><given-names>Y.</given-names></name><name><surname>Xiao</surname><given-names>D.</given-names></name><name><surname>Vignarajan</surname><given-names>J.</given-names></name><name><surname>Preetham</surname><given-names>A.</given-names></name><name><surname>Tay-Kearney</surname><given-names>M.L.</given-names></name><name><surname>Mehrotra</surname><given-names>A.</given-names></name></person-group><article-title>Evaluation of Artificial Intelligence-Based Grading of Diabetic Retinopathy in Primary Care</article-title><source>JAMA Netw. Open.</source><year>2018</year><volume>1</volume><fpage>e182665</fpage><pub-id pub-id-type="doi">10.1001/jamanetworkopen.2018.2665</pub-id><pub-id pub-id-type="pmid">30646178</pub-id></element-citation></ref><ref id="B19-ijerph-18-02428"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keel</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>P.Y.</given-names></name><name><surname>Scheetz</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Kotowicz</surname><given-names>M.A.</given-names></name><name><surname>MacIsaac</surname><given-names>R.J.</given-names></name><name><surname>He</surname><given-names>M.</given-names></name></person-group><article-title>Feasibility and patient acceptability of a novel artificial intelligence-based screening model for diabetic retinopathy at endocrinology outpatient services: A pilot study</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><fpage>4330</fpage><pub-id pub-id-type="doi">10.1038/s41598-018-22612-2</pub-id><pub-id pub-id-type="pmid">29531299</pub-id></element-citation></ref><ref id="B20-ijerph-18-02428"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumagai</surname><given-names>Y.</given-names></name><name><surname>Takubo</surname><given-names>K.</given-names></name><name><surname>Kawada</surname><given-names>K.</given-names></name><name><surname>Aoyama</surname><given-names>K.</given-names></name><name><surname>Endo</surname><given-names>Y.</given-names></name><name><surname>Ozawa</surname><given-names>T.</given-names></name><name><surname>Hirasawa</surname><given-names>T.</given-names></name><name><surname>Yoshio</surname><given-names>T.</given-names></name><name><surname>Ishihara</surname><given-names>S.</given-names></name><name><surname>Fujishiro</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Diagnosis using deep-learning artificial intelligence based on the endocytoscopic observation of the esophagus</article-title><source>Esophagus</source><year>2019</year><volume>16</volume><fpage>180</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1007/s10388-018-0651-7</pub-id><pub-id pub-id-type="pmid">30547352</pub-id></element-citation></ref><ref id="B21-ijerph-18-02428"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mori</surname><given-names>Y.</given-names></name><name><surname>Kudo</surname><given-names>S.E.</given-names></name><name><surname>Mohmed</surname><given-names>H.E.N.</given-names></name><name><surname>Misawa</surname><given-names>M.</given-names></name><name><surname>Ogata</surname><given-names>N.</given-names></name><name><surname>Itoh</surname><given-names>H.</given-names></name><name><surname>Oda</surname><given-names>M.</given-names></name><name><surname>Mori</surname><given-names>K.</given-names></name></person-group><article-title>Artificial intelligence and upper gastrointestinal endoscopy: Current status and future perspective</article-title><source>Dig. Endosc.</source><year>2019</year><volume>31</volume><fpage>378</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1111/den.13317</pub-id><?supplied-pmid 30549317?><pub-id pub-id-type="pmid">30549317</pub-id></element-citation></ref><ref id="B22-ijerph-18-02428"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pace</surname><given-names>F.</given-names></name><name><surname>Riegler</surname><given-names>G.</given-names></name><name><surname>de Leone</surname><given-names>A.</given-names></name><name><surname>Pace</surname><given-names>M.</given-names></name><name><surname>Cestari</surname><given-names>R.</given-names></name><name><surname>Dominici</surname><given-names>P.</given-names></name><name><surname>Grossi</surname><given-names>E.</given-names></name><name><surname>Group</surname><given-names>E.S.</given-names></name></person-group><article-title>Is it possible to clinically differentiate erosive from nonerosive reflux disease patients? A study using an artificial neural networks-assisted algorithm</article-title><source>Eur. J. Gastroenterol. Hepatol.</source><year>2010</year><volume>22</volume><fpage>1163</fpage><lpage>1168</lpage><pub-id pub-id-type="doi">10.1097/MEG.0b013e32833a88b8</pub-id><pub-id pub-id-type="pmid">20526203</pub-id></element-citation></ref><ref id="B23-ijerph-18-02428"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>C.R.</given-names></name><name><surname>Chen</surname><given-names>Y.T.</given-names></name><name><surname>Chen</surname><given-names>W.Y.</given-names></name><name><surname>Cheng</surname><given-names>H.C.</given-names></name><name><surname>Sheu</surname><given-names>B.S.</given-names></name></person-group><article-title>Gastroesophageal Reflux Disease Diagnosis Using Hierarchical Heterogeneous Descriptor Fusion Support Vector Machine</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2016</year><volume>63</volume><fpage>588</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1109/TBME.2015.2466460</pub-id><pub-id pub-id-type="pmid">26276981</pub-id></element-citation></ref><ref id="B24-ijerph-18-02428"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B25-ijerph-18-02428"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>S.J.</given-names></name><name><surname>Yang</surname><given-names>Q.</given-names></name></person-group><article-title>A survey on transfer learning</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2010</year><volume>22</volume><fpage>1345</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id></element-citation></ref><ref id="B26-ijerph-18-02428"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>S.-C.</given-names></name><name><surname>Wu</surname><given-names>H.-C.</given-names></name><name><surname>Tseng</surname><given-names>M.-H.</given-names></name></person-group><article-title>Remote Sensing Scene Classification and Explanation Using RSSCNet and LIME</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>6151</elocation-id><pub-id pub-id-type="doi">10.3390/app10186151</pub-id></element-citation></ref><ref id="B27-ijerph-18-02428"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1502.03167</pub-id></element-citation></ref><ref id="B28-ijerph-18-02428"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clevert</surname><given-names>D.-A.</given-names></name><name><surname>Unterthiner</surname><given-names>T.</given-names></name><name><surname>Hochreiter</surname><given-names>S.</given-names></name></person-group><article-title>Fast and accurate deep network learning by exponential linear units (elus)</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1511.07289</pub-id></element-citation></ref><ref id="B29-ijerph-18-02428"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shorten</surname><given-names>C.</given-names></name><name><surname>Khoshgoftaar</surname><given-names>T.M.</given-names></name></person-group><article-title>A survey on image data augmentation for deep learning</article-title><source>J. Big Data</source><year>2019</year><volume>6</volume><fpage>60</fpage><pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id></element-citation></ref><ref id="B30-ijerph-18-02428"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Caruana</surname><given-names>R.</given-names></name><name><surname>Lawrence</surname><given-names>S.</given-names></name><name><surname>Giles</surname><given-names>C.L.</given-names></name></person-group><article-title>Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping</article-title><source>Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference</source><publisher-name>The MIT Press</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><publisher-loc>Denver, CO, USA</publisher-loc><month>1</month><year>2000</year><fpage>381</fpage><lpage>387</lpage></element-citation></ref><ref id="B31-ijerph-18-02428"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roiger</surname><given-names>R.</given-names></name><name><surname>Geatz</surname><given-names>M.</given-names></name></person-group><source>Data Mining: A Tutorial-Based Primer</source><publisher-name>Addison Wesley</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2003</year></element-citation></ref><ref id="B32-ijerph-18-02428"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#x02013;1 July 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B33-ijerph-18-02428"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Vanhoucke</surname><given-names>V.</given-names></name><name><surname>Ioffe</surname><given-names>S.</given-names></name><name><surname>Shlens</surname><given-names>J.</given-names></name><name><surname>Wojna</surname><given-names>Z.</given-names></name></person-group><article-title>Rethinking the inception architecture for computer vision</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>26 June&#x02013;1 July 2016</conf-date><fpage>2818</fpage><lpage>2826</lpage></element-citation></ref><ref id="B34-ijerph-18-02428"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tseng</surname><given-names>P.H.</given-names></name><name><surname>Chen</surname><given-names>C.C.</given-names></name><name><surname>Chiu</surname><given-names>H.M.</given-names></name><name><surname>Liao</surname><given-names>W.C.</given-names></name><name><surname>Wu</surname><given-names>M.S.</given-names></name><name><surname>Lin</surname><given-names>J.T.</given-names></name><name><surname>Lee</surname><given-names>Y.C.</given-names></name><name><surname>Wang</surname><given-names>H.P.</given-names></name></person-group><article-title>Performance of narrow band imaging and magnification endoscopy in the prediction of therapeutic response in patients with gastroesophageal reflux disease</article-title><source>J. Clin. Gastroenterol.</source><year>2011</year><volume>45</volume><fpage>501</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1097/MCG.0b013e3181eeb115</pub-id><pub-id pub-id-type="pmid">20733514</pub-id></element-citation></ref><ref id="B35-ijerph-18-02428"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parikh</surname><given-names>N.D.</given-names></name><name><surname>Viana</surname><given-names>A.V.</given-names></name><name><surname>Shah</surname><given-names>S.</given-names></name><name><surname>Laine</surname><given-names>L.</given-names></name></person-group><article-title>Image-enhanced endoscopy is specific for the diagnosis of non-erosive gastroesophageal reflux disease</article-title><source>Scand. J. Gastroenterol.</source><year>2018</year><volume>53</volume><fpage>260</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1080/00365521.2018.1430847</pub-id><?supplied-pmid 29368532?><pub-id pub-id-type="pmid">29368532</pub-id></element-citation></ref><ref id="B36-ijerph-18-02428"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furneri</surname><given-names>G.</given-names></name><name><surname>Klausnitzer</surname><given-names>R.</given-names></name><name><surname>Haycock</surname><given-names>L.</given-names></name><name><surname>Ihara</surname><given-names>Z.</given-names></name></person-group><article-title>Economic value of narrow-band imaging versus white light endoscopy for the diagnosis and surveillance of Barrett&#x02019;s esophagus: Cost-consequence model</article-title><source>PLoS ONE</source><year>2019</year><volume>14</volume><elocation-id>e0212916</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0212916</pub-id><?supplied-pmid 30865673?><pub-id pub-id-type="pmid">30865673</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="ijerph-18-02428-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Sample images to illustrate Los Angeles classification of gastroesophageal reflux disease.</p></caption><graphic xlink:href="ijerph-18-02428-g001"/></fig><fig id="ijerph-18-02428-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Flowchart of the study design.</p></caption><graphic xlink:href="ijerph-18-02428-g002"/></fig><fig id="ijerph-18-02428-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Convolutional neural network classifier architecture of the proposed GERD-VGGNet model.</p></caption><graphic xlink:href="ijerph-18-02428-g003"/></fig><fig id="ijerph-18-02428-f004" orientation="portrait" position="float"><label>Figure 4</label><caption><p>Comparison of accuracy using different pretrained models in the narrow-band image (NBI) mode.</p></caption><graphic xlink:href="ijerph-18-02428-g004"/></fig><fig id="ijerph-18-02428-f005" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Model training history in the NBI mode: (<bold>a</bold>) without data augmentation; and (<bold>b</bold>) with data augmentation.</p></caption><graphic xlink:href="ijerph-18-02428-g005"/></fig><fig id="ijerph-18-02428-f006" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Comparison of accuracy using data augmentation in the NBI mode.</p></caption><graphic xlink:href="ijerph-18-02428-g006"/></fig><fig id="ijerph-18-02428-f007" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Comparison of accuracy using different classifiers in the NBI mode.</p></caption><graphic xlink:href="ijerph-18-02428-g007"/></fig><table-wrap id="ijerph-18-02428-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">ijerph-18-02428-t001_Table 1</object-id><label>Table 1</label><caption><p>Baseline characteristics of development and test sets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Characteristics Patient Number</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Development Set N = 464</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Test Set N = 32</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">%</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">%</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Conventional images</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade <sup>1</sup> A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">247</td><td align="center" valign="middle" rowspan="1" colspan="1">35.4</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">225</td><td align="center" valign="middle" rowspan="1" colspan="1">32.3</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">31.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">225</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">NBI <sup>2</sup> images</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">244</td><td align="center" valign="middle" rowspan="1" colspan="1">36.3</td><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">37.5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">229</td><td align="center" valign="middle" rowspan="1" colspan="1">34.2</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">31.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">198</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Augmentation (conventional)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">70</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">163</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Augmentation (NBI)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LA grade C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">72</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">NA</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">150</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Los Angeles classification = LA grade. <sup>2</sup> Narrow-band image = NBI.</p></fn></table-wrap-foot></table-wrap><table-wrap id="ijerph-18-02428-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">ijerph-18-02428-t002_Table 2</object-id><label>Table 2</label><caption><p>Confusion matrix of proposed GERD-VGGNet on the development set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Image Type</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Conventional</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">NBI <sup>2</sup></th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Real LA <sup>1</sup> Classification</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A&#x02013;B</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C&#x02013;D</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A&#x02013;B</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C&#x02013;D</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>GERD-VGGNet</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">247</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">242</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">221</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">229</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">225</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">198</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.2%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.2%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Los Angeles classification = LA grade. <sup>2</sup> Narrow-band image = NBI.</p></fn></table-wrap-foot></table-wrap><table-wrap id="ijerph-18-02428-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">ijerph-18-02428-t003_Table 3</object-id><label>Table 3</label><caption><p>Confusion matrices of GERD-VGGNet and trainees on the test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Image Type</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Conventional</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">NBI <sup>2</sup></th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Real LA <sup>1</sup> Classification</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A&#x02013;B</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C&#x02013;D</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</th><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">A&#x02013;B</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C&#x02013;D</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>GERD-VGGNet</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">11</td><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td colspan="2" align="center" valign="middle" rowspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td colspan="2" align="center" valign="middle" rowspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80%</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">83.3%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80%</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>Trainee 1</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td colspan="2" align="center" valign="middle" rowspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td colspan="2" align="center" valign="middle" rowspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">75%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80%</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
<bold>Trainee 2</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">A&#x02013;B</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td colspan="2" align="center" valign="middle" rowspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">C&#x02013;D</td><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td colspan="2" align="center" valign="middle" rowspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90%</td><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">41.7%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90%</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Los Angeles classification = LA grade. <sup>2</sup> Narrow-band image = NBI.</p></fn></table-wrap-foot></table-wrap><table-wrap id="ijerph-18-02428-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">ijerph-18-02428-t004_Table 4</object-id><label>Table 4</label><caption><p>Model comparisons between the proposed AI model and trainees with NBI and conventional endoscopy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model 1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model 2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ps</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">GERD-VGGNet-NBI <sup>1</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">Trainee1-NBI</td><td align="center" valign="middle" rowspan="1" colspan="1">1.281</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GERD-VGGNet-NBI</td><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 2-NBI</td><td align="center" valign="middle" rowspan="1" colspan="1">2.068 *</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 1-NBI</td><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 2-NBI</td><td align="center" valign="middle" rowspan="1" colspan="1">0.823</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GERD-VGGNet-conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 1-conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">0.552</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GERD-VGGNet-conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 2-conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">0.293</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 1-conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 2-conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">0.842</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GERD-VGGNet-NBI</td><td align="center" valign="middle" rowspan="1" colspan="1">GERD-VGGNet -conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">1.281</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 1-NBI</td><td align="center" valign="middle" rowspan="1" colspan="1">Trainee 1-conventional</td><td align="center" valign="middle" rowspan="1" colspan="1">0.552</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trainee 2-NBI</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trainee 2-conventional</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.112</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Narrow-band image = NBI.</p></fn></table-wrap-foot></table-wrap><table-wrap id="ijerph-18-02428-t005" orientation="portrait" position="float"><object-id pub-id-type="pii">ijerph-18-02428-t005_Table 5</object-id><label>Table 5</label><caption><p>Performance comparison of different AI systems for prediction of gastroesophageal reflux disease.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Task</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Used</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Evaluation Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Overall Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Specificity</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Binary classification</td><td align="center" valign="middle" rowspan="1" colspan="1">Machine learning (ANN) [<xref rid="B22-ijerph-18-02428" ref-type="bibr">22</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">QUID <sup>1</sup> questionnaire <break/>(577 GERD <sup>2</sup> patients, 94 normal cases)</td><td align="center" valign="middle" rowspan="1" colspan="1">hold-out</td><td align="center" valign="middle" rowspan="1" colspan="1">99.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.1%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.8%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Binary classification</td><td align="center" valign="middle" rowspan="1" colspan="1">Machine learning (HHDF-SVM) [<xref rid="B23-ijerph-18-02428" ref-type="bibr">23</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">147 RGB images <break/>(39 GERD patients, 108 normal cases)</td><td align="center" valign="middle" rowspan="1" colspan="1">10-fold cross-validation</td><td align="center" valign="middle" rowspan="1" colspan="1">93.2%</td><td align="center" valign="middle" rowspan="1" colspan="1">94.9%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.6%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Three-class classification</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deep learning + data augmentation (proposed GERD-VGGNet)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">603,068 NBI <sup>3</sup> images <break/>(GERD A&#x02013;B: GERD C&#x02013;D: normal EC-J = 244:229:198)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10-fold cross validation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.9% &#x000b1; 1%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.8% &#x000b1; 0.2%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.7%&#x000b1; 0.2%</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> QUestionario Italiano Diagnostico = QUID. <sup>2</sup> Gastroesophageal reflux disease = GERD. <sup>3</sup> Narrow-band image = NBI.</p></fn></table-wrap-foot></table-wrap></floats-group></article>