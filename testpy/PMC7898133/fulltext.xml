<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="review-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Philos Trans A Math Phys Eng Sci</journal-id><journal-id journal-id-type="iso-abbrev">Philos Trans A Math Phys Eng Sci</journal-id><journal-id journal-id-type="publisher-id">RSTA</journal-id><journal-id journal-id-type="hwp">roypta</journal-id><journal-title-group><journal-title>Philosophical transactions. Series A, Mathematical, physical, and engineering sciences</journal-title></journal-title-group><issn pub-type="ppub">1364-503X</issn><issn pub-type="epub">1471-2962</issn><publisher><publisher-name>The Royal Society Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">7898133</article-id><article-id pub-id-type="pmid">33583266</article-id><article-id pub-id-type="doi">10.1098/rsta.2020.0097</article-id><article-id pub-id-type="publisher-id">rsta20200097</article-id><article-categories><subj-group subj-group-type="hwp-journal-coll"><subject>1003</subject><subject>7</subject><subject>50</subject><subject>1005</subject><subject>12</subject><subject>127</subject></subj-group><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group><subj-group subj-group-type="leader"><subject>Opinion Piece</subject></subj-group></article-categories><title-group><article-title>Can deep learning beat numerical weather prediction?</article-title><alt-title alt-title-type="short">Deep learning for weather prediction</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3455-774X</contrib-id><name><surname>Schultz</surname><given-names>M. G.</given-names></name><xref ref-type="corresp" rid="cor1"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-1347-5297</contrib-id><name><surname>Betancourt</surname><given-names>C.</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7770-2738</contrib-id><name><surname>Gong</surname><given-names>B.</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0604-3058</contrib-id><name><surname>Kleinert</surname><given-names>F.</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3354-5333</contrib-id><name><surname>Langguth</surname><given-names>M.</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-4154-3397</contrib-id><name><surname>Leufen</surname><given-names>L. H.</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6719-0425</contrib-id><name><surname>Mozaffari</surname><given-names>A.</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6613-9852</contrib-id><name><surname>Stadtler</surname><given-names>S.</given-names></name></contrib></contrib-group><aff><institution>J&#x000fc;lich Supercomputing Centre</institution>, <addr-line>Forschungszentrum J&#x000fc;lich</addr-line>, <country>Germany</country></aff><author-notes><fn fn-type="other"><p>One contribution of 13 to a theme issue &#x02018;<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsta/379/2194">Machine learning for weather and climate modelling</ext-link>&#x02019;.</p></fn><corresp id="cor1">e-mail: <email>m.schultz@fz-juelich.de</email></corresp></author-notes><pub-date pub-type="ppub"><day>5</day><month>4</month><year>2021</year></pub-date><pub-date pub-type="epub"><day>15</day><month>2</month><year>2021</year></pub-date><pub-date pub-type="pmc-release"><day>15</day><month>2</month><year>2021</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>379</volume><issue>2194</issue><issue-title>Theme issue &#x02018;Machine learning for weather and climate modelling&#x02019; compiled and edited by Matthew Chantry, Hannah Christensen, Peter Dueben and Tim Palmer</issue-title><elocation-id>20200097</elocation-id><history><date date-type="accepted"><day>3</day><month>9</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; 2021 The Authors.</copyright-statement><copyright-year>2021</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="version-of-record">http://creativecommons.org/licenses/by/4.0/</ali:license_ref><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="text-data-mining">http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Published by the Royal Society under the terms of the Creative Commons Attribution License <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>, which permits unrestricted use, provided the original author and source are credited.</license-p></license><?release-delay 0|0?></permissions><self-uri content-type="pdf" xlink:href="rsta20200097.pdf"/><abstract><p>The recent hype about artificial intelligence has sparked renewed interest in applying the successful deep learning (DL) methods for image recognition, speech recognition, robotics, strategic games and other application areas to the field of meteorology. There is some evidence that better weather forecasts can be produced by introducing big data mining and neural networks into the weather prediction workflow. Here, we discuss the question of whether it is possible to completely replace the current numerical weather models and data assimilation systems with DL approaches. This discussion entails a review of state-of-the-art machine learning concepts and their applicability to weather data with its pertinent statistical properties. We think that it is not inconceivable that numerical weather models may one day become obsolete, but a number of fundamental breakthroughs are needed before this goal comes into reach.</p><p>This article is part of the theme issue &#x02018;Machine learning for weather and climate modelling&#x02019;.</p></abstract><kwd-group><kwd>numerical weather prediction</kwd><kwd>machine learning</kwd><kwd>deep learning</kwd><kwd>weather AI</kwd><kwd>spatiotemporal pattern recognition</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>H2020 European Research Council</institution><institution-id>http://dx.doi.org/10.13039/100010663</institution-id></institution-wrap></funding-source><award-id>787576</award-id></award-group><award-group><funding-source><institution-wrap><institution>Bundesministerium f&#x000fc;r Bildung und Forschung</institution><institution-id>http://dx.doi.org/10.13039/501100002347</institution-id></institution-wrap></funding-source><award-id>IS18047A</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>cover-date</meta-name><meta-value>April 5, 2021</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><label>1.</label><title>Introduction</title><p>The history of numerical weather prediction (NWP) and that of machine learning (ML) or artificial intelligence (for the purposes of this paper, the two terms can be used interchangeably) differ substantially. First, manual NWP was attempted by Lewis Fry Richardson in Britain in 1922, and early computer-aided weather forecasts were produced in 1950 [<xref rid="RSTA20200097C1" ref-type="bibr">1</xref>]. These were soon followed by operational weather forecasts in Sweden, the USA and Japan. While there has been steady progress in the development of NWP (cf. [<xref rid="RSTA20200097C2" ref-type="bibr">2</xref>]), the history of ML (cf. [<xref rid="RSTA20200097C3" ref-type="bibr">3</xref>]) has been more disruptive: the first neural network (NN) was proposed in 1943 by McCulloch &#x00026; Pitts [<xref rid="RSTA20200097C4" ref-type="bibr">4</xref>]. The field expanded until the early 1960s, when the existing algorithms proved inefficient and unstable. The invention of backpropagation in 1970 [<xref rid="RSTA20200097C5" ref-type="bibr">5</xref>,<xref rid="RSTA20200097C6" ref-type="bibr">6</xref>] led to a second wave of ML applications as it became possible to build more extensive NNs and train them to recognize nonlinear relationships in data (e.g. [<xref rid="RSTA20200097C7" ref-type="bibr">7</xref>]). Even though the development of ML algorithms continued, the enthusiasm about them soon dwindled again, because they rarely showed significant performance gain, and computing resources were not sufficient to solve larger problems. Furthermore, big amounts of labelled data which are mandatory for most data-driven ML approaches were hard to come by (note that this was before the advent of the world wide web). Three significant developments around 2010 started the third wave of artificial intelligence, which continues to the present: computing capabilities were vastly expanded due to massive parallel processing in graphical processing units (GPUs), convolutional neural networks (CNN) allowed much more efficient analysis of massive (image) datasets, and large benchmark datasets were made available on the internet. Highly complex NNs with greater than 10<sup>6</sup> parameters enabled a breakthrough in image recognition [<xref rid="RSTA20200097C8" ref-type="bibr">8</xref>], soon followed by remarkable success stories in speech recognition [<xref rid="RSTA20200097C9" ref-type="bibr">9</xref>], gaming [<xref rid="RSTA20200097C10" ref-type="bibr">10</xref>,<xref rid="RSTA20200097C11" ref-type="bibr">11</xref>], and video analysis and prediction [<xref rid="RSTA20200097C12" ref-type="bibr">12</xref>,<xref rid="RSTA20200097C13" ref-type="bibr">13</xref>]. Today&#x02019;s NNs are often deep networks with greater than 10 layers, and the research field which develops such NNs and the associated methods for training and validation is called deep learning (DL).</p><p>The weather and climate research community is increasingly aware of modern DL technologies and tries to adopt them to solve specific data analysis, numerical modelling and post-processing problems in the context of NWP. Nevertheless, as the workshop on &#x02018;Machine learning in weather and climate&#x02019; (Oxford, September 2019) has also shown, there are still reservations about DL in this community. Two core arguments in this regard are the lack of explainability of deep NNs and the lack of physical constraints. Furthermore, some scepticism prevails due to the fact that researchers have experimented with rather simple NNs which were clearly unsuited to capture the complexity of meteorological data and feedback processes, but then extrapolate these results to discredit any NN application including the much more powerful DL systems. In their review of &#x02018;Deep learning and process understanding for data-driven Earth system science&#x02019;, Reichstein <italic>et al.</italic> [<xref rid="RSTA20200097C14" ref-type="bibr">14</xref>] argue that traditional ML approaches might not be optimally suited to address the specific data challenges posed by Earth system data. They suggest that &#x02018;deep learning methods are needed to cope with complex statistics, multiple outputs, different noise sources and high-dimensional spaces [of Earth system data]. New network topologies that not only exploit local neighbourhood (even at different scales) but also long-range relationships (for example, for teleconnections) are urgently needed, but the exact cause-and-effect relations between variables are not clear in advance and need to be discovered.&#x02019; As modern DL methods begin to deliver such concepts, we take this opportunity to expand on the analysis of [<xref rid="RSTA20200097C14" ref-type="bibr">14</xref>] and explore the applicability of such methods to the NWP workflow (<xref ref-type="fig" rid="RSTA20200097F1">figure 1</xref>, left column).
<fig id="RSTA20200097F1" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>Idealized workflows of current numerical weather prediction (left), next-generation weather prediction with individual components substituted or augmented by ML and DL techniques (centre), and a purely data-driven DL weather forecasting system (right). (Online version in colour.)
</p></caption><graphic xlink:href="rsta20200097-g1"/></fig></p><p>While considerable work is undertaken to substitute specific parts of the NWP workflow with DL approaches (<xref ref-type="fig" rid="RSTA20200097F1">figure 1</xref>, central column), in this paper, we take a bold step forward and address the question of whether it is possible to replace all core parts of the NWP workflow with one deep NN, which would take observations as input and generate end-user forecast products directly from the data (<xref ref-type="fig" rid="RSTA20200097F1">figure 1</xref>, right column). Such approaches have been investigated for the specific application of wind speed and power predictions (cf. [<xref rid="RSTA20200097C15" ref-type="bibr">15</xref>]), and there is at least one study, which successfully developed an end-to-end workflow to forecast multiple weather variables from the data of the US weather balloon network [<xref rid="RSTA20200097C16" ref-type="bibr">16</xref>]. However, most of these studies were restricted to short-term forecasts and a few individual target sites, and none has yet attempted to explore the wealth of combined meteorological observations from the plethora of instruments and sensors, which is routinely used in operational weather forecasts.</p><p>The goal of this paper is therefore to review the NWP workflow from a DL perspective and analyse the specific requirements and conditions of weather forecasting in light of current DL theory and practices. It is structured as follows: &#x000a7;<xref ref-type="sec" rid="s2">2</xref> gives a brief overview about the major developments and state of the art of NWP including aspects of data assimilation (DA) and model output processing. It is followed by &#x000a7;<xref ref-type="sec" rid="s3">3</xref> which surveys the literature on fundamental ML and DL developments and their application to weather and climate research. <xref ref-type="sec" rid="s4">Section 4</xref> discusses several fundamental aspects of meteorological data and other requirements of weather forecasts and points to corresponding solutions in DL research where these exist. <xref ref-type="sec" rid="s5">Section 5</xref> reflects on two aspects which are relevant for both weather forecasting and DL, but where we find different best practices in both domains. These aspects are data preparation and model evaluation. In &#x000a7;<xref ref-type="sec" rid="s6">6</xref>, we reflect on the issues of physical constraints and system-wide forecast consistency in a DL framework. <xref ref-type="sec" rid="s7">Section 7</xref> discusses the state of the art with respect to estimating forecast uncertainties. Finally, &#x000a7;<xref ref-type="sec" rid="s8">8</xref> presents conclusions. We hope that this article will lead to a better understanding between &#x02018;machine learners&#x02019; and &#x02018;weather researchers&#x02019; and thus contribute to a more effective development of DL solutions in the field of weather and climate.</p></sec><sec id="s2"><label>2.</label><title>State-of-the-art numerical weather prediction</title><p>Modern weather prediction relies extensively on massive numerical simulation systems which are routinely operated by national weather agencies all over the world. The associated process chain to generate these numerical weather forecasts can be divided into several steps which interact closely with each other (<xref ref-type="fig" rid="RSTA20200097F1">figure 1</xref>, left column).</p><p>In order to retrieve the initial state of the Earth system (atmosphere, soil and ocean), a great variety of meteorological observations are collected all over the world. In addition to classical weather and radiosondes stations, aircraft measurements and remote sensing products (such as radar and satellite observations) have become an integral part of the global observation network (e.g. [<xref rid="RSTA20200097C17" ref-type="bibr">17</xref>]). Although millions of different direct and indirect measurements are obtained every day, these observations are still not sufficient to describe the complete state of the atmosphere and other Earth system components with which the atmosphere exchanges energy or mass.</p><p>At this point, DA comes into play. The central task of DA is to fill the gap between the incomplete, heterogeneous, and scattered observations and the initial value fields which are required by the NWP models. To achieve this, the observation data must be projected onto the discrete model grid, interpolated in time and adjusted to be consistent in terms of state variables (e.g. temperature, pressure, wind etc.). DA also has to take into account measurement errors such as biases between different space instruments or malfunctions of individual ground-based sensors. The obtained initial state of the Earth system after the DA step therefore constitutes an optimized estimate of the real conditions (e.g. [<xref rid="RSTA20200097C18" ref-type="bibr">18</xref>]).</p><p>Given the initial conditions, the NWP model can perform a simulation of atmospheric processes. By solving numerically the coupled partial differential equation system describing the atmosphere in terms of momentum, mass and enthalphy (the Navier&#x02013;Stokes equations), the future atmospheric state is obtained in each model grid cell. Processes occurring at scales smaller than the model grid size are captured by empirical parameterizations. The direct model output constitutes the first forecast product of the NWP workflow. In contemporary global NWP models, the grid boxes cover an area of several square kilometres.</p><p>In order to arrive at finer scale end-user forecast products, a post-processing step is added to the NWP workflow. The outcome of such post-processing can cover a variety of forecast products starting with the conversion of the vertical axis from sigma-coordinates to pressure levels or geometric height (above mean sea level) or bias corrections. Statistical methods are applied to remove systematic biases of the NWP output and to incorporate local scale adjustments (statistical downscaling). Furthermore, limited-area models which allow for finer grid spacings (<inline-formula><mml:math id="IM1"><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mtext>&#x02013;</mml:mtext><mml:mn>5</mml:mn><mml:mo>&#x02009;</mml:mo><mml:mrow><mml:mtext>km</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> compared to <inline-formula><mml:math id="IM2"><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mo>&#x02009;</mml:mo><mml:mrow><mml:mtext>km</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> in global models) provide added value on forecasting meteorological features on finer scales. The output of ensemble simulations can be used to estimate forecast uncertainties which are of major interest especially for high impact weather situations, or for the renewable energy sector [<xref rid="RSTA20200097C19" ref-type="bibr">19</xref>,<xref rid="RSTA20200097C20" ref-type="bibr">20</xref>] (see also &#x000a7;<xref ref-type="sec" rid="s7">7</xref>).</p><p>Over the past decades, the ability of NWP models to predict the future atmospheric state has continuously improved. Contemporary global NWP models are not only able to predict the synoptic-scale weather pattern for several days, but they have also reached remarkable accuracy in forecasting end-user relevant meteorological quantities such as the 2 m temperature and regional-scale precipitation events. For instance, the deterministic forecasts of the Integrated Forecast System provided by the European Centre for Medium-Range Weather Forecasts maintains an anomaly correlation coefficient of the 500&#x02009;hPa geopotential height of 80% for about 7 days, while the root-mean square error for 2&#x02009;m temperature predictions of 72&#x02009;h forecasts is close to 2&#x02009;K [<xref rid="RSTA20200097C21" ref-type="bibr">21</xref>]. Larger scale high-impact events such as hurricane tracks can be predicted with an accuracy of 150&#x02009;km up to 4 days in advance [<xref rid="RSTA20200097C22" ref-type="bibr">22</xref>].</p><p>The increasing success of operational NWP models is due to improvements of all the steps involved in the NWP workflow and new capabilities of the global Earth observation system. In the following, we briefly highlight a couple of important developments which led to significant enhancements of forecast quality. A detailed review of recent advances in the NWP process chain is beyond the scope of this article.</p><p>While <italic>in situ</italic> observations (weather stations and radiosondes) have a long history in observing the Earth&#x02019;s atmosphere, fundamental improvements in the spatiotemporal coverage of observations has been achieved with the help of satellite data over recent decades (e.g [<xref rid="RSTA20200097C23" ref-type="bibr">23</xref>]).</p><p>Nowadays, several geostationary and polar-orbiting satellites deliver a great variety of data products (such as temperature and humidity profiles, soil moisture and atmospheric motion vectors). Satellite data are particularly valuable as they provide information on the atmosphere above the ocean and uninhabitated areas where conventional measurements are hard to come by. However, measurements from commercial aircraft (e.g. [<xref rid="RSTA20200097C24" ref-type="bibr">24</xref>]) and radar observations (e.g. [<xref rid="RSTA20200097C25" ref-type="bibr">25</xref>,<xref rid="RSTA20200097C26" ref-type="bibr">26</xref>]) have also contributed to better constraining the initial state of NWP models.</p><p>The ability of DA systems to make use of the manifold, diverse observations has seen continuous improvement due to algorithmic developments. Current DA systems are primarily based on three- or four-dimensional variational approaches (3D-Var and 4D-Var, respectively) and on ensemble methods (commonly Kalman filter). In the 3D-Var approach, a single deterministic state is estimated by minimizing a cost function which generally consists of the three terms background, observation, and model error. 4D-Var additionally captures observation changes in time (see [<xref rid="RSTA20200097C18" ref-type="bibr">18</xref>] for more details).</p><p>In order to obtain a loss function which can be optimized with reasonable efficiency, the model and observation operators have to be linearized. This can lead to forecast errors, in particular if the NWP model contains discontinuous parameterizations [<xref rid="RSTA20200097C27" ref-type="bibr">27</xref>]. Another simplification of the variational DA approach is the <italic>a priori</italic> definition of the uncertainties of the state vector <bold>X</bold> which leads to a static background error covariance matrix. By contrast, an ensemble approach allows for dynamic estimation of the probability density function of <bold>X</bold>. The Ensemble Kalman Filter approach makes use of such an estimation which then results in a non-static, i.e. flow-evolving background error covariance matrix. A disadvantage of classical ensemble methods is that they are only conditioned on past measurements [<xref rid="RSTA20200097C28" ref-type="bibr">28</xref>]. Therefore, leading meteorological centres have started to establish combinations of the variational approach with ensemble approaches such as the 4D-EnvVar DA method. The quality of ensemble DA depends on the number of ensemble members which is typically restricted to a relatively small number due to computational reasons. Therefore, so-called hybrid DA systems have been developed which include climatological error information in order to lessen the sensitivity to undersampling [<xref rid="RSTA20200097C29" ref-type="bibr">29</xref>].</p><p>NWP model improvements can in part be related to resolution enhancements. The continuous refinement of the grid spacing has also required re-formulating the dynamical cores of NWP models where the discretized Navier&#x02013;Stokes equations are solved. Simulating the atmosphere on kilometre-scale comes along with the demand of highly parallelizable algorithms of the dynamical core [<xref rid="RSTA20200097C30" ref-type="bibr">30</xref>]. Since (classical) global spectral transform models are less suited for such a requirement, finite-difference or finite-volume discretizations on platonic solids projected on the sphere (e.g. icosahedral [<xref rid="RSTA20200097C31" ref-type="bibr">31</xref>] or cubed-sphere grids [<xref rid="RSTA20200097C32" ref-type="bibr">32</xref>]) have been developed over the previous decade. Simultaneously, remarkable progress has been achieved in designing discretization approaches which enable the grid-scale dynamics to follow the conservation laws of energy, enthrophy and mass [<xref rid="RSTA20200097C33" ref-type="bibr">33</xref>,<xref rid="RSTA20200097C34" ref-type="bibr">34</xref>] while also minimizing the need for numerical filters to suppress artificial numerical modes [<xref rid="RSTA20200097C35" ref-type="bibr">35</xref>]. An extensive overview of contemporary dynamical core architectures can be found in [<xref rid="RSTA20200097C32" ref-type="bibr">32</xref>].</p><p>In addition to the improvement of dynamical cores, further gains in accuracy have been achieved by fine-tuning physical parameterizations which are mandatory to represent atmospheric processes that cannot be captured by the grid-scale thermodynamics. Among others, these parameterizations encompass the representation of (deep) convection, turbulent mixing, smaller-scale atmosphere-land/ocean coupling, the representation of cloud microphysics and radiative transfer. Advances in capturing the diurnal cycle of convection (e.g. [<xref rid="RSTA20200097C36" ref-type="bibr">36</xref>,<xref rid="RSTA20200097C37" ref-type="bibr">37</xref>]), the turbulent transports in the planetary boundary layer (e.g. [<xref rid="RSTA20200097C38" ref-type="bibr">38</xref>]) and in simulating the bulk properties of hydrometeors (e.g. [<xref rid="RSTA20200097C39" ref-type="bibr">39</xref>]), i.e. clouds and precipitation, are only a small sample of recent progress in tuning physical parameterization schemes.</p></sec><sec id="s3"><label>3.</label><title>Deep learning in weather research</title><p>The increased computational power, the availability of large datasets, and the rapid development of new NN architectures all contribute to the ongoing success of DL. Some of these new NN can solve certain ML tasks much more efficiently than the classical fully connected, feed-forward networks. One especially successful concept, which has been widely applied, is convolutional neural networks [<xref rid="RSTA20200097C40" ref-type="bibr">40</xref>] (CNN), where a stack of small-sized filters with few trainable parameters is applied to images or other gridded data to extract coarser scale features. CNNs have been used in weather and climate applications, where the NN was trained to recognize spatial features, for example in the analysis of satellite imagery [<xref rid="RSTA20200097C41" ref-type="bibr">41</xref>] or weather model output [<xref rid="RSTA20200097C42" ref-type="bibr">42</xref>].</p><p>The family of recurrent neural networks (RNN) was designed specifically for the learning of time-dependent features (i.e. text and speech recognition). More advanced RNN architectures are long short-term memory (LSTM) nodes [<xref rid="RSTA20200097C43" ref-type="bibr">43</xref>,<xref rid="RSTA20200097C44" ref-type="bibr">44</xref>] and gated recurrent units (GRU, [<xref rid="RSTA20200097C45" ref-type="bibr">45</xref>]). LSTM and GRU cells can be embedded in more complex neural network architectures. For example, the combination of a normal CNN with LSTM yields the so-called ConvLSTM network [<xref rid="RSTA20200097C46" ref-type="bibr">46</xref>].</p><p>Two more recent DL concepts are variational auto-encoders (VAE) [<xref rid="RSTA20200097C47" ref-type="bibr">47</xref>] and generative adversarial networks (GAN) [<xref rid="RSTA20200097C48" ref-type="bibr">48</xref>]. Both of these are so-called generative models, i.e. they learn the data distributions from training samples and use generators to produce novel samples which match the characteristics of the training data. They are widely used in different applications such as image-to-image translation [<xref rid="RSTA20200097C49" ref-type="bibr">49</xref>], super-resolution image generation [<xref rid="RSTA20200097C50" ref-type="bibr">50</xref>], in-painting [<xref rid="RSTA20200097C51" ref-type="bibr">51</xref>], image enhancement [<xref rid="RSTA20200097C52" ref-type="bibr">52</xref>], image synthesis [<xref rid="RSTA20200097C53" ref-type="bibr">53</xref>], style transfer and texture synthesis [<xref rid="RSTA20200097C54" ref-type="bibr">54</xref>] and video generation and prediction [<xref rid="RSTA20200097C12" ref-type="bibr">12</xref>,<xref rid="RSTA20200097C13" ref-type="bibr">13</xref>]. VAEs use an encoder to project the high-dimensional data with posterior distribution into a latent space with lower dimensionality. This latent space is then sampled by a decoder to reconstruct the original feature space in all dimensions. For further information on VAE, we refer to [<xref rid="RSTA20200097C47" ref-type="bibr">47</xref>,<xref rid="RSTA20200097C55" ref-type="bibr">55</xref>]. In GANs, the competition of two NNs is used to improve on image generation during training. One network is optimized to generate realistic images, while a second one is trained concurrently to discriminate between generated and real images. Typically, both VAE and GAN-based architectures are coupled to multiple convolutional layers for capturing the semantic features of the input data and represent them with fewer dimensions. Examples are PixelVAE [<xref rid="RSTA20200097C56" ref-type="bibr">56</xref>], DCGAN [<xref rid="RSTA20200097C57" ref-type="bibr">57</xref>], sinGAN [<xref rid="RSTA20200097C58" ref-type="bibr">58</xref>] and SAVP [<xref rid="RSTA20200097C13" ref-type="bibr">13</xref>]. It is a general tendency in DL research that new NN architectures are composed of many building blocks which are themselves substantially large DL networks. The problem-complexity which can be addressed with modern DL networks is already quite substantial. The largest NNs have several million degrees of freedom which is comparable to operational NWP models.</p><p>ML as &#x02018;an approach to data analysis that involves building and adapting models, which allow programs to learn through experience&#x02019; has been employed by meteorologists for a long time, for example in curve fitting, linear regression, or DA (see &#x000a7;<xref ref-type="sec" rid="s2">2</xref>). However, in this article, we focus on ML in a narrower sense, i.e. involving NNs and in particular modern DL.</p><p>First studies employing NN for meteorological and air quality applications appeared during the 1990s [<xref rid="RSTA20200097C59" ref-type="bibr">59</xref>&#x02013;<xref rid="RSTA20200097C61" ref-type="bibr">61</xref>]. These studies used multi-layer-perceptron architectures with typically three layers to analyse and forecast time series at individual station locations. Later, other simple semantic network techniques were used for post-processing and prediction optimization of NWP output [<xref rid="RSTA20200097C62" ref-type="bibr">62</xref>,<xref rid="RSTA20200097C63" ref-type="bibr">63</xref>], and as surrogate models for different parameterization schemes in climate models [<xref rid="RSTA20200097C64" ref-type="bibr">64</xref>,<xref rid="RSTA20200097C65" ref-type="bibr">65</xref>].</p><p>It took a few years before the weather and climate research community started to pick up modern DL concepts and began to explore their use in NWP and other environmental applications. <xref rid="RSTA20200097TB1" ref-type="table">Table 1</xref> lists various state-of-the-art DL architectures and their first applications in weather and climate research. A couple of examples are briefly described below. A review of ML in remote sensing can be found in [<xref rid="RSTA20200097C41" ref-type="bibr">41</xref>]. Zhou <italic>et al.</italic> [<xref rid="RSTA20200097C83" ref-type="bibr">83</xref>] and Denby <italic>et al.</italic> [<xref rid="RSTA20200097C84" ref-type="bibr">84</xref>] used a CNN for classification of weather satellite images, while Xu <italic>et al.</italic> [<xref rid="RSTA20200097C85" ref-type="bibr">85</xref>] used a combination of GAN and LSTM for prediction of cloud images. Based on the concept of video prediction, various types of networks were used for short-term prediction of sky images and radar images [<xref rid="RSTA20200097C46" ref-type="bibr">46</xref>,<xref rid="RSTA20200097C79" ref-type="bibr">79</xref>,<xref rid="RSTA20200097C81" ref-type="bibr">81</xref>]. There have also been some attempts to produce data-driven weather forecasts, for example by Dueben &#x00026; Bauer [<xref rid="RSTA20200097C86" ref-type="bibr">86</xref>], who used a multi-layer perceptron, or Grover <italic>et al.</italic> [<xref rid="RSTA20200097C16" ref-type="bibr">16</xref>], who constructed a three-stage model consisting of boosted decision trees, a dynamic Gaussian Process model, and a deep belief network consisting of restricted Boltzman machines [<xref rid="RSTA20200097C77" ref-type="bibr">77</xref>,<xref rid="RSTA20200097C78" ref-type="bibr">78</xref>]. The study of Wandel <italic>et al.</italic> [<xref rid="RSTA20200097C87" ref-type="bibr">87</xref>] could be regarded as a first step towards replacing the dynamical core of a numerical weather model as they demonstrate unsupervised learning of the full incompressible Navier&#x02013;Stokes equations on a Eulerian, grid-based representation. Gong <italic>et al.</italic> (A. B. Gong, unpublished manuscript, 2020), experimented with a state-of-the-art video prediction model and tried to use it for 2 m temperature predictions over 10 h.
<table-wrap id="RSTA20200097TB1" orientation="portrait" position="float"><label>Table&#x000a0;1.</label><caption><p>State-of-the-art neural network architectures and their application in weather and climate research. Entries marked with * denote hybrid NN architectures.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">architecture</th><th align="left" rowspan="1" colspan="1">introduced for the first time (original references)</th><th align="left" rowspan="1" colspan="1">early weather and climate applications (current state in July 2020)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">convolutional neural network (CNN)</td><td rowspan="1" colspan="1">AlexNet (Alex <italic>et al.</italic> 2012 [<xref rid="RSTA20200097C8" ref-type="bibr">8</xref>])</td><td rowspan="1" colspan="1">VGG (Shi <italic>et al.</italic> 2018 [<xref rid="RSTA20200097C66" ref-type="bibr">66</xref>])</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">VGG (Simonyan &#x00026; Zisserman, 2013 [<xref rid="RSTA20200097C67" ref-type="bibr">67</xref>])</td><td rowspan="1" colspan="1">ResNet (Pothineni <italic>et al.</italic> 2019 [<xref rid="RSTA20200097C68" ref-type="bibr">68</xref>])</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">ResNet (He <italic>et al.</italic> 2015 [<xref rid="RSTA20200097C69" ref-type="bibr">69</xref>])</td><td rowspan="1" colspan="1">Vgg, ResNet (Wen <italic>et al.</italic> 2020 [<xref rid="RSTA20200097C70" ref-type="bibr">70</xref>])</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">GoogleLeNet (Szegedy <italic>et al.</italic> 2015 [<xref rid="RSTA20200097C71" ref-type="bibr">71</xref>])</td><td rowspan="1" colspan="1">Inception blocks (Kleinert <italic>et al.</italic> 2021 [<xref rid="RSTA20200097C72" ref-type="bibr">72</xref>])</td></tr><tr><td rowspan="1" colspan="1">long short-term memory network (LSTM)</td><td rowspan="1" colspan="1">RNN (Bengio <italic>et al.</italic> 1994 [<xref rid="RSTA20200097C73" ref-type="bibr">73</xref>])</td><td rowspan="1" colspan="1">LSTM (G&#x000f3;mez <italic>et al.</italic> 2003 [<xref rid="RSTA20200097C74" ref-type="bibr">74</xref>])</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">LSTM (Hochreiter and Schmidhuber, 1997 [<xref rid="RSTA20200097C43" ref-type="bibr">43</xref>])</td><td rowspan="1" colspan="1">LSTM (Qing &#x00026; Niu, 2018 [<xref rid="RSTA20200097C75" ref-type="bibr">75</xref>])</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">PhyDNet (Le Guen, 2020 [<xref rid="RSTA20200097C76" ref-type="bibr">76</xref>])</td></tr><tr><td rowspan="1" colspan="1">deep belief network (DBN)</td><td rowspan="1" colspan="1">Smolensky, 1986 [<xref rid="RSTA20200097C77" ref-type="bibr">77</xref>], Hinton &#x00026; Salakhutdinov [<xref rid="RSTA20200097C78" ref-type="bibr">78</xref>]</td><td rowspan="1" colspan="1">Grover <italic>et al.</italic> 2015 [<xref rid="RSTA20200097C16" ref-type="bibr">16</xref>]</td></tr><tr><td rowspan="1" colspan="1">variational autoencoder (VAE)</td><td rowspan="1" colspan="1">Vanilla VAE, 2013 [<xref rid="RSTA20200097C47" ref-type="bibr">47</xref>]</td><td rowspan="1" colspan="1">None</td></tr><tr><td rowspan="1" colspan="1">generative adversarial neural network (GAN)</td><td rowspan="1" colspan="1">Vanilla GAN (Goodfellow <italic>et al.</italic> 2014 [<xref rid="RSTA20200097C48" ref-type="bibr">48</xref>])</td><td rowspan="1" colspan="1">MD-GAN (Xiong, 2018 [<xref rid="RSTA20200097C79" ref-type="bibr">79</xref>])</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">conditional GAN (Schmidt <italic>et al.</italic> 2020 [<xref rid="RSTA20200097C80" ref-type="bibr">80</xref>])</td></tr><tr><td rowspan="1" colspan="1">convolutional long short-term memory network (ConvLSTM)</td><td rowspan="1" colspan="1">convLSTM (Shi <italic>et al.</italic> 2015 [<xref rid="RSTA20200097C46" ref-type="bibr">46</xref>])</td><td rowspan="1" colspan="1">ConvLSTM* (Shi <italic>et al.</italic> 2015 [<xref rid="RSTA20200097C46" ref-type="bibr">46</xref>]),</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">PredRNN (Wang <italic>et al.</italic> 2017 [<xref rid="RSTA20200097C81" ref-type="bibr">81</xref>]); MIM (Wang <italic>et al.</italic> 2019 [<xref rid="RSTA20200097C82" ref-type="bibr">82</xref>])</td></tr><tr><td rowspan="1" colspan="1">stochastic adversarial video prediction (SAVP*)</td><td rowspan="1" colspan="1">SAVP (Lee <italic>et al.</italic> 2018 [<xref rid="RSTA20200097C13" ref-type="bibr">13</xref>])</td><td rowspan="1" colspan="1">Gong <italic>et al.</italic> (A. B. Gong, unpublished manuscript, 2020)</td></tr></tbody></table></table-wrap></p></sec><sec id="s4"><label>4.</label><title>Challenges of end-to-end deep learning weather prediction</title><p>The studies that were cited in the previous section already demonstrate that DL concepts can be successfully applied to problems related to weather forecasting. However, the few existing attempts to replace the entire NWP workflow with a DL system have been limited to short-term forecasting (up to 24 h or less) or used a rather limited subset of the available meteorological data. In this section, we discuss a number of challenges which need to be overcome before a complete end-to-end DL weather forecasting system can deliver results of comparable quality as current NWP.</p><p>Weather forecasting is essentially a prediction of spatiotemporal features based on a diverse array of observations from ground-based, airborne and satellite platforms. If we treat the core part of the NWP workflow (<xref ref-type="fig" rid="RSTA20200097F1">figure 1</xref>), i.e. DA, model forecast and output post processing, as one entity, then a weather forecast can be described as a function, which maps observation data to a final forecast product (<xref ref-type="fig" rid="RSTA20200097F2">figure 2</xref>). The forecast product can be a map of a specific weather variable (e.g. temperature), a time series of one or more variables at a specific location or aggregated over a region, some aggregate statistics of a specific variable over a given time range, a (categorical) warning index, etc. With current NWP, we are used to employing one forecasting system (which may well have several components) to derive the whole set of forecast products which are requested by end users, or needed for the system evaluation and further improvement. By contrast, DL applications excel if they can focus on a specific task, i.e. a reasonably small set of target variables. Therefore, an end-to-end DL weather forecast as depicted in the right column of <xref ref-type="fig" rid="RSTA20200097F1">figure 1</xref> would likely consist of several deep NNs which would be trained individually on specific subsets of forecast products.
<fig id="RSTA20200097F2" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Schematic view of the principal task of weather prediction, i.e. a mapping from diverse observation data to specific forecast products.
</p></caption><graphic xlink:href="rsta20200097-g2"/></fig></p><p>Advantages of such a DL weather forecasting system could be the intrinsic absence of model bias (because the system would be trained to reproduce the target values) and the possible savings of computational resources. Once NNs are trained, they can very efficiently calculate forecasts with new data. Forward propagation in NNs consists only of fast add and multiply operations. Therefore, even NNs with O(10<sup>8</sup>) parameters (i.e. similar complexity as contemporary NWP models) can be expected to use far less computing resources than current numerical models. The determining factors of the required computational resources in an end-to-end DL weather forecasting system are the necessary training cycles and the data processing. The former will depend on the learning approach (e.g. lifelong learning [<xref rid="RSTA20200097C88" ref-type="bibr">88</xref>] requires regular re-training of some NN components) and on the success of transfer learning [<xref rid="RSTA20200097C89" ref-type="bibr">89</xref>] concepts (i.e. whether it is possible to re-use NNs trained in one region of the globe for weather forecasts in another region). Data processing is a challenge which also limits further scaling of NWP models and other applications on current and next generation supercomputing systems (see [<xref rid="RSTA20200097C90" ref-type="bibr">90</xref>]). At present, it is impossible to predict how much computing time, if any, could be saved if all weather forecasting would be based on DL. A fair comparison should always consider the entire weather prediction workflow and the whole range of forecast products that shall be generated.</p><p>In the following, we discuss a number of challenges for end-to-end DL weather predictions, which are mostly a consequence of the specific properties of meteorological data and the complexity of the atmosphere and its interactions with other Earth system compartments. These challenges are graphically summarized in <xref ref-type="fig" rid="RSTA20200097F3">figure 3</xref>. As will be seen, many of these challenges also appear in other DL contexts, and the DL community has begun to develop solutions for these problems. Nevertheless, there are no systems in place which can cope with all of these challenges together.
<fig id="RSTA20200097F3" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>Graphical summary of the inter-connected DL challenges imposed by properties of meteorological data and the complexity of the weather forecasting problem. The colours of the jigsaw pieces are arbitrary. (Online version in colour.)
</p></caption><graphic xlink:href="rsta20200097-g3"/></fig></p><p>The success of DL methods hinges on a good understanding of relevant data properties. Meteorological variables can be described with different cumulative distribution functions or corresponding probability density functions. Some variables (e.g. temperature) are nearly normally distributed, while others (e.g. precipitation and cloud droplet size) might be better approximated by gamma or beta distributions [<xref rid="RSTA20200097C91" ref-type="bibr">91</xref>]. The fraction of cloud cover is often reported in eighths and therefore needs to be treated as a discrete variable. Ignoring the different properties of meteorological data can cause erroneous results when they are not accounted for in a statistical analysis or forecasting procedure. This is particularly relevant as some DL methods (e.g. the Bayesian approach by [<xref rid="RSTA20200097C92" ref-type="bibr">92</xref>]) make implicit assumptions about the frequency distribution of variables.</p><p>Meteorological features can show dynamic behaviour on a wide range of spatiotemporal scales, and the quality of the weather forecast is influenced by phenomena on many different scales [<xref rid="RSTA20200097C93" ref-type="bibr">93</xref>]. Consider sea ice as an example, which may change little during the time of a typical forecast lead time, but whose mid- to longer-term variations can have a profound influence on the local and non-local atmospheric state [<xref rid="RSTA20200097C94" ref-type="bibr">94</xref>]. Multiple spatial scales have been investigated in the context of video prediction (e.g. [<xref rid="RSTA20200097C12" ref-type="bibr">12</xref>,<xref rid="RSTA20200097C58" ref-type="bibr">58</xref>]).</p><p>Related to the interaction of scales, the spectra of energy and momentum are important aspects in meteorology, but such spectra do not play a similar role in most mainstream DL applications. However, spectral transformations have already been used in DL applications (e.g. [<xref rid="RSTA20200097C9" ref-type="bibr">9</xref>] in the context of speech recognition). First attempts at using such concepts in ML weather applications [<xref rid="RSTA20200097C95" ref-type="bibr">95</xref>&#x02013;<xref rid="RSTA20200097C98" ref-type="bibr">98</xref>] were limited to simple NNs and limited complexity datasets.</p><p>Many meteorological features vary periodically, although there can be large variability between cycles. This periodicity is induced by orbital parameters and the Earth rotation together with various solar cycles. As shown in Ziyin <italic>et al.</italic> [<xref rid="RSTA20200097C99" ref-type="bibr">99</xref>], NNs generally have difficulties extrapolating periodic features correctly. However, as the authors show, replacing common monotonic activation functions with functions which include a periodic term can solve such problems and produce, for example, better temperature forecasts.</p><p>Meteorological variables are correlated in space and time, and these correlations change with time [<xref rid="RSTA20200097C16" ref-type="bibr">16</xref>]. For example, temperatures at different altitudes may exhibit very similar patterns (possibly with a time lag) in a well-mixed boundary layer (i.e. summer, daytime), while different vertical levels can be almost completely decoupled during an inversion (often during winter or at night). This can also be seen in larger scale features such as tropical cyclones, Rossby waves, fronts and (organized) convection. While we are not aware of a publication which addresses this issue in the context of DL with weather data, there are other DL studies which demonstrate that it is possible to cope with such correlations (e.g. [<xref rid="RSTA20200097C100" ref-type="bibr">100</xref>]).</p><p>A related property of meteorological features is auto-correlation. While auto-correlation in a way simplifies the forecasting task (at least on short time scales), it imposes the risk of over-estimating the forecasting capabilities of a DL model, in particular if this is not factored into the data preparation and model evaluation (see &#x000a7;<xref ref-type="sec" rid="s5">5</xref>).</p><p>Meteorological features may appear and vanish on time scales much shorter than the forecast range. Prominent examples are the triggering of convective cells or the transition between convective and relief precipitation in the presence of orography. An NWP model has some skill in predicting such features, because it can diagnose their potential occurrence from relations between other variables. In principle, such complex relations may be decipherable by NNs as well. However, we believe that this will require additional measures to make the NN aware of such relations. Such measures could be feature engineering (i.e. the calculation of derived properties from combinations of input variables) or the implementation of physical constraints (see &#x000a7;<xref ref-type="sec" rid="s6">6</xref>).</p><p>Another challenge for a DL weather forecast application is the scarcity of extreme events, which are, however, very important to get right as extreme weather phenomena have the largest impact on civil safety and economy. For example, to accurately predict heavy precipitation events (&#x0003e;25&#x02009;mm&#x02005;h<sup>&#x02212;1</sup>) over Germany, the DL model must be trained with less than 10 episodes at any given location during a full decade [<xref rid="RSTA20200097C101" ref-type="bibr">101</xref>]. A few studies have touched upon the subject of ML and extreme climate events: Vandal <italic>et al.</italic> [<xref rid="RSTA20200097C102" ref-type="bibr">102</xref>] find that complex DL models have more problems in capturing extremes than classical statistical downscaling models, whereas O&#x02019;Gorman <italic>et al.</italic> [<xref rid="RSTA20200097C103" ref-type="bibr">103</xref>] state that their model captures extremes quite well without the need for special training on these cases. While the subject of classifying imbalanced data has received considerable attention (cf. [<xref rid="RSTA20200097C104" ref-type="bibr">104</xref>]), there appears to be little research on dealing with imbalanced sample sizes in regression problems [<xref rid="RSTA20200097C105" ref-type="bibr">105</xref>]. In contrast to standard DL algorithms, humans have acquired the ability to learn from isolated extreme events because they pay special attention to extraordinary occurrences in their environment and quickly generalize to other situations [<xref rid="RSTA20200097C106" ref-type="bibr">106</xref>]. Some studies have explored the possibility to have deep neural networks learn extreme events in a similar way. For example, Li <italic>et al.</italic> [<xref rid="RSTA20200097C107" ref-type="bibr">107</xref>] implemented one-shot learning of object categories by using prior knowledge learned from other training samples. Lake <italic>et al.</italic> [<xref rid="RSTA20200097C108" ref-type="bibr">108</xref>] developed ML methods within Bayesian program learning (BPL) to mimic human capabilities of learning visual recognition from a few samples.</p><p>Finally, other critical aspects related to meteorological observation data are the frequent appearance of missing values and the possibility of input data errors and biases. Current DA procedures often include a substantial code base for filtering or interpolating missing values, blacklisting observations, and monitoring of biases and their evolution over time. Similar issues occur in various application areas of DL. For example, Smieja <italic>et al.</italic> [<xref rid="RSTA20200097C109" ref-type="bibr">109</xref>] have dealt with the issue of missing data values, and &#x0017d;liobait&#x000e9; [<xref rid="RSTA20200097C110" ref-type="bibr">110</xref>] and Lu <italic>et al.</italic> [<xref rid="RSTA20200097C111" ref-type="bibr">111</xref>] investigated the problem of drifting biases (known as concept drift in the ML community). A recent example of a DL application working on meteorological satellite data with missing values is Barth <italic>et al.</italic> [<xref rid="RSTA20200097C112" ref-type="bibr">112</xref>].</p></sec><sec id="s5"><label>5.</label><title>Data preparation and model evaluation</title><p>In this section, we discuss two aspects of ML in weather and climate, which we have found to be important in our practical experience and where best practices differ between the meteorological and ML communities. These are data preparation and model evaluation. This discussion may shed some light upon the reasons why it has been difficult for the DL community to tackle weather data problems and why, conversely, the meteorological community has been cautious to adopt DL in their research and for routine weather analyses and forecasts.</p><p>All ML techniques are data-driven. Therefore, proper selection and preparation of data are essential to gain good and generalizable results. Data selection should aim to capture the full variability of the predictor variables, avoid too much redundant information, and allow the network to capture relations among variables, from which a prediction can be made. For the sake of brevity, we will not discuss data selection in more detail, but instead focus on data preparation aspects in the following.</p><p>Modern supervised ML studies generally divide the available data into three different datasets to train, develop and evaluate an ML model [<xref rid="RSTA20200097C113" ref-type="bibr">113</xref>]. The training set is the largest and is used to update the model parameters by back propagation or other learning algorithms. The second set, which is often referred to as the validation or development set, is used exclusively for hyper-parameter tuning. The hyper-parameters, i.e. number of layers, type of layer, activation function, loss function, learning rate etc. are set manually by the model developer. A key target of this hyper-parameter tuning is the optimization of the network&#x02019;s generalization capabilities to ensure that the network will function well on previously unseen data. Both parameters and hyper-parameters are essential for building a suitable DL model. The third dataset is the test set, a collection of previously unseen data which is used to evaluate the network after the tuning to assess the true generalization capability of the network. The three datasets should be independent of each other, but at the same time they should reflect the same statistical distribution. Therefore, one has to be careful how to split the data before starting to train a new network, especially if, as in meteorological time series, the data are auto-correlated.</p><p><xref ref-type="fig" rid="RSTA20200097F4">Figure 4</xref> shows four different data split strategies for a hypothetical time series of meteorological data. In order to enable an NN to forecast the next <italic>k</italic> time steps, one will generally feed an input vector of <italic>l</italic> past time steps to the network as input. In many DL applications, it is standard to draw random samples from a huge database of mutually independent data records (e.g. images) and arbitrarily assign these samples to the train, validation and test sets, respectively. This would correspond to <xref ref-type="fig" rid="RSTA20200097F4">figure 4</xref><italic>a</italic>, where each drawn &#x02018;slice&#x02019; has a length of one sample. However, as noted in &#x000a7;<xref ref-type="sec" rid="s4">4</xref>, meteorological data constitutes a continuous time series with auto-correlation on different time scales. Therefore, randomly drawn samples would overlap and therefore no longer be independent. Consequently, results obtained with such a test set over-estimate the true generalization capability of the NN, because the test set contains information already used for training. When researching for this article, we found several studies on ML and DL for environmental data analysis, where this principle was violated and which therefore made overly optimistic conclusions concerning the capabilities of (often simple) NNs.
<fig id="RSTA20200097F4" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>Different train-dev-test splitting strategies for meteorological data with periodic features as indicated in the conceptual time series at the bottom of the figure. Every sand coloured block stands for 1 year of data. Case (<italic>a</italic>) depicts random sampling as is commonly applied in many DL applications. Cases (<italic>b</italic>&#x02013;<italic>d</italic>) show different variants of random block sampling, which avoid spurious correlations between the train, val and test sets, if the block length chosen is long enough. (Online version in colour.)
</p></caption><graphic xlink:href="rsta20200097-g4"/></fig></p><p>Another point of concern, which also has implications for the data preparation, is the multi-scale aspect of data in the time domain. While a typical forecast application considers time scales of a few hours to several days, there are longer-term quasi-periodic patterns, such as the El Ni&#x000f1;o Southern Oscillation (ENSO), and also continuous trends such as global warming. When training NNs with long-term data series (so that a sufficient number of samples becomes available), it is not trivial to find a good data split, which on the one hand fulfils the requirement of independence, while on the other hand allows the network to be trained on as many parts of the underlying data distribution as possible. For example, the model developer should ensure that all seasons are sampled appropriately, and, when using multi-year data, that the training data contains different phases of ENSO as one example out of many other oscillations.</p><p>To solve these issues, we propose a random block sampling strategy (<xref ref-type="fig" rid="RSTA20200097F4">figure 4</xref><italic>b</italic>&#x02013;<italic>d</italic>), where the train, validation and test sets all contain several coherent blocks of length <italic>L</italic>, and <italic>L</italic> is much larger than the auto-correlation time. Multiple runs of this random block sampling should be carried out to assess the robustness of the results.</p><p>The second aspect which we would like to discuss in this section concerns model evaluation. While it is relatively straightforward to evaluate the success of an image classification (e.g. [<xref rid="RSTA20200097C8" ref-type="bibr">8</xref>,<xref rid="RSTA20200097C69" ref-type="bibr">69</xref>]) or a video prediction task (cf. [<xref rid="RSTA20200097C12" ref-type="bibr">12</xref>,<xref rid="RSTA20200097C79" ref-type="bibr">79</xref>]), the evaluation metrics used in these studies (e.g. MSE or Peak Signal to Noise Ratio, PSNR) are usually not appropriate for weather and climate applications. Quantifying the success of a weather prediction model is highly non-trivial and an area of active research. Meteorological centres have developed a plethora of scores and skill scores over the last decades, which elucidate different aspects of weather forecast quality (cf. [<xref rid="RSTA20200097C21" ref-type="bibr">21</xref>,<xref rid="RSTA20200097C91" ref-type="bibr">91</xref>,<xref rid="RSTA20200097C114" ref-type="bibr">114</xref>]). In addition to verification methods based on point by point comparisons (e.g. [<xref rid="RSTA20200097C115" ref-type="bibr">115</xref>]), various methods have been introduced to account for the intrinsic spatial and temporal correlation in meteorological datasets (e.g. [<xref rid="RSTA20200097C116" ref-type="bibr">116</xref>&#x02013;<xref rid="RSTA20200097C118" ref-type="bibr">118</xref>]). Other verification metrics also account for the stochastic nature of meteorological quantities by estimating probabilities of binary events, such as rain, no-rain [<xref rid="RSTA20200097C119" ref-type="bibr">119</xref>]. Evaluating spatiotemporal patterns, for example, precipitation forecasts, with the help of radar data is particularly challenging due to the double penalty problem (cf. [<xref rid="RSTA20200097C116" ref-type="bibr">116</xref>,<xref rid="RSTA20200097C120" ref-type="bibr">120</xref>]). Indeed, verification of precipitation forecasts is still a hot topic in the meteorological community [<xref rid="RSTA20200097C121" ref-type="bibr">121</xref>]. The evaluation of extreme events suffers from the &#x02018;forecaster&#x02019;s dilemma&#x02019;, which discredits skilful forecasts when they are evaluated only under the condition that an extreme event occurred. This conditioning on outcomes and observations violates the theoretical assumptions of forecast verification methods [<xref rid="RSTA20200097C21" ref-type="bibr">21</xref>,<xref rid="RSTA20200097C122" ref-type="bibr">122</xref>].</p></sec><sec id="s6"><label>6.</label><title>Physical constraints and consistency</title><p>As has been demonstrated in other application areas of ML (e.g. [<xref rid="RSTA20200097C123" ref-type="bibr">123</xref>,<xref rid="RSTA20200097C124" ref-type="bibr">124</xref>]), NNs can be prone to learning spurious relationships in data. A purely data-driven model for weather forecasting might fail to adhere to the underlying physical principles and thus generate false forecasts as it lacks understanding of the fact that every atmospheric process obeys physical laws described in terms of conservation of momentum, enthalpy and mass. The incorporation of physical laws in NNs is becoming a vibrant area of research (e.g. [<xref rid="RSTA20200097C125" ref-type="bibr">125</xref>&#x02013;<xref rid="RSTA20200097C128" ref-type="bibr">128</xref>]) and is now often denoted as scientific ML.</p><p>One of the first studies to demonstrate that physical constraints can efficiently reduce systematic biases in lake temperature predictions, while at the same time enhancing generalization capability, was Karpatne <italic>et al.</italic> [<xref rid="RSTA20200097C129" ref-type="bibr">129</xref>]. They included numerical model results as constraints for sparse observations and added a loss term to punish non-physical behaviour of the DL model. De B&#x000e9;zenac <italic>et al.</italic> [<xref rid="RSTA20200097C127" ref-type="bibr">127</xref>] embedded an advection diffusion equation in their loss function to predict sea surface temperatures and thereby improved the predictive capabilities of their model. Le Guen <italic>et al.</italic> [<xref rid="RSTA20200097C76" ref-type="bibr">76</xref>] introduced a new two-stream model and a recurrent cell, which is based on concepts from DA. The cell includes a physical predictor and a Kalman filter in order to assimilate the inputs. Effectively, this leads to the encoding of physical laws in the latent space of their model. Other approaches to introduce physical constraints into the latent space of DL models are the adversarial autoencoder by Makhzani <italic>et al.</italic> [<xref rid="RSTA20200097C130" ref-type="bibr">130</xref>] and the non-parametric Bayesian method by Goyal <italic>et al.</italic> [<xref rid="RSTA20200097C131" ref-type="bibr">131</xref>].</p><p>It may be useful to reflect on the potential and necessity of physically constraining DL models from an abstract point of view. In spite of their complexity and dimensionality, DL models still adhere to the fundamental principles of (data-driven) statistical modelling. This implies that there must be some rules in place to constrain the future, because otherwise extrapolation will be unbound. Classical statistical modelling tries to strike a balance between a sufficiently explicit formulation of the time-dependent system evolution and the remaining degrees of freedom to accommodate the intrinsic variability of the data. For example, to fit hourly temperature observations, a classical statistical model will usually include (at least) two periodic terms to capture the diurnal and seasonal cycles. In addition, there may be terms to describe correlations of temperature with other variables. On the other hand, the statistician will avoid over-fitting the data by adding too many terms into the statistical model.</p><p>Even though in DL one expects the NN to learn many of the inherent data relations by itself, the system nevertheless must get some guidance to be able to identify meaningful patterns. Knowingly or not, the researcher always imposes constraints on the NN, for example through data selection and choice of NN architecture. NNs learn faster when patterns are clearly visible in the data. However, with meteorological data the most obvious patterns are usually the least interesting ones, therefore it makes sense to let the NN know in advance that such patterns will occur. A similar argument applies to physical constraints: if the NN is forced, for example to conserve mass, it will not need to waste parameters and training cycles on learning this rule and can instead concentrate on analysing relevant and less obvious relations. Many meteorological studies show that it is often necessary to pre-select or filter data and build a good statistical model before meaningful relations become apparent. It is, therefore, likely that an end-to-end DL weather forecast system will only be successful if it contains at least some <italic>a priori</italic> knowledge in the form of engineered data features and physical constraints. Just how much of this is needed remains to be seen.</p><p>Newcomers to the field of NWP often think that such numerical simulation models are inherently self-consistent, because they are based on a well-defined set of differential equations. However, the discretization of partial differential equations describing the flow dynamics in NWP models is not always fully mass or energy conserving, and parameterization schemes, which are needed to incorporate the effects of unresolved sub-grid scale processes on the grid-scale variables, may lead to spurious competition between grid-scale and sub-grid-scale processes, for example in cloud schemes [<xref rid="RSTA20200097C132" ref-type="bibr">132</xref>,<xref rid="RSTA20200097C133" ref-type="bibr">133</xref>]. Furthermore, there can be a grey zone between different parameterizations describing related aspects, such as the classical distinction of shallow and deep convection. Also, physically related parameterization schemes may be derived from different empirical data. Furthermore, internal consistency of classical NWP no longer applies if one considers the entire NWP workflow, i.e. if statistical models are used to post-process the model output, remove biases and apply other, non-physical corrections to the model forecasts. This discussion does not intend to devalue classical NWP, but it should inspire some reflection on the exact meaning and the value of consistency in the weather forecasting and DL communities. An end-to-end DL weather forecast system will generate consistency among forecast products only to the extent that this is already embedded in the data, unless the system will be governed by physical constraints as discussed above. To what extent consistency is needed to obtain a &#x02018;good&#x02019; forecast will be a worthwhile question to study as it may deepen the understanding of the problem at hand and the potential which DL can bring to weather forecasting.</p></sec><sec id="s7"><label>7.</label><title>Uncertainty estimation</title><p>The final discussion point of this article concerns the estimation of forecast uncertainty. Owing to increased computer power, it has become possible in recent years to produce ensemble forecasts operationally. Ensemble approaches have also been introduced in DA (see &#x000a7;<xref ref-type="sec" rid="s2">2</xref>). In a nutshell, ensemble forecasts aim to estimate the probability density function of the forecast variables. Ensembles are most often generated by varying the initial conditions of the model simulation, but there are also attempts to sample the parameter space of empirical model parameterizations.</p><p>In the field of DL research ensemble methods are used less often because they are computationally expensive. Statistical concepts such as Gaussian process (GP), and probabilistic graphical models (PGM) excel at probabilistic inference and uncertainty estimation. However, these methods do not scale well for high-dimensional and high-volume data [<xref rid="RSTA20200097C134" ref-type="bibr">134</xref>&#x02013;<xref rid="RSTA20200097C136" ref-type="bibr">136</xref>]. Therefore, Bayesian deep learning (BDL) has been developed and applied across several scientific and engineering domains, for example in medical diagnosis [<xref rid="RSTA20200097C137" ref-type="bibr">137</xref>] or autonomous driving [<xref rid="RSTA20200097C138" ref-type="bibr">138</xref>]. In essence, these methods estimate a probability density function of the DL model parameters. As side effects BDL increases the robustness against over-fitting and allows training of the NN from relatively small sample sizes [<xref rid="RSTA20200097C139" ref-type="bibr">139</xref>]. Modern BDL methods include variational inference, Markov Chain Monte Carlo (MCMC) sampling [<xref rid="RSTA20200097C136" ref-type="bibr">136</xref>], and Monte Carlo dropout [<xref rid="RSTA20200097C140" ref-type="bibr">140</xref>].</p><p>Some recent studies explored the BDL concept for weather forecasting applications. A model built on GRU and 3D CNN, along with variational Bayesian inference for estimating posterior parameter distributions, has been presented by Liu <italic>et al.</italic> [<xref rid="RSTA20200097C141" ref-type="bibr">141</xref>] for probabilistic wind speed forecasting of up to 3 h. A study from Vandal <italic>et al</italic>. [<xref rid="RSTA20200097C92" ref-type="bibr">92</xref>] demonstrates the use of BDL to capture the uncertainty from observation data and unknown model parameters in the context of statistical downscaling of precipitation forecasts. These are relevant contributions, but a lot remains to be done before the uncertainty of DL weather forecasts can be assessed at a level similar to current NWP ensemble systems.</p></sec><sec id="s8"><label>8.</label><title>Conclusion</title><p>In this article, we discussed the potential of modern DL approaches to develop purely data-driven end-to-end weather forecast applications. While there have been some stunning success stories from DL applications in other fields and initial attempts were made to apply DL to meteorological data, this research is still in its infancy. As we argue in &#x000a7;<xref ref-type="sec" rid="s4">4</xref>, there are specific properties of weather data which require the development of new approaches beyond the classical concepts from computer vision, speech recognition, and other typical ML tasks. Even though DL solutions for many of these issues are being developed, there is no DL method up to now which can deal with all of these issues concurrently as it would be required in a complete weather forecast system.</p><p>We expect that the field of ML in weather and climate science will grow rapidly in the coming years as more and more sophisticated ML architectures are becoming available and can easily be deployed on modern computer systems. What is largely missing in the field of meteorological DL are benchmark datasets with a specification of appropriate baseline scores and software frameworks which make it easy for the DL community to adopt a meteorological problem and try out different approaches. One notable exception is Weatherbench [<xref rid="RSTA20200097C142" ref-type="bibr">142</xref>]. Such benchmark datasets and frameworks are well established in the ML community (e.g. MNIST [<xref rid="RSTA20200097C143" ref-type="bibr">143</xref>] or ImageNet [<xref rid="RSTA20200097C144" ref-type="bibr">144</xref>]) and they contributed substantially to the rapid pace of DL developments in application areas such as image recognition, video prediction, speech recognition, gaming and robotics. While a lot of meteorological data is freely available from weather centres and research institutions, proper use of these data requires some knowledge about Earth system science and the data formats and tools, which are used by the environmental research community. It might help if tools for reading and working with these datasets were integrated in major ML frameworks.</p><p>When reflecting on the ultimate goal of replacing computationally expensive NWP models with DL algorithms, it is important to reconsider the objectives of weather forecasting and carefully define the requirements, which must be met by any potential alternative method. Certain criteria, which we now consider essential for a &#x02018;good&#x02019; weather forecast, may in fact be conceptions, which are resulting from our experiences with numerical models, and they may not be applicable to forecasting systems based on DL. One particular aspect in this regard is self-consistency of forecast results, which is often taken for granted by numerical modellers, even though it is not strictly fulfilled in current NWP forecast systems. In this article, we consciously propose thinking about a replacement of the entire core NWP workflow including the DA, numerical modelling, and output processing, because the task of weather forecasting can then be described as a huge Big Data problem of mapping a plethora of Earth system observations onto a well-defined set of specific end-user weather forecast products. Seen in this way, the problem of weather forecasting is more amenable to DL methods than a replacement of the actual NWP model itself with its grid structure, operator concepts etc. which are tied to the very concept of classical numerical modelling.</p><p>We expect that the success of DL weather forecast applications will hinge on the consideration of physical constraints in the NN design. Taken to the extreme, portions or variants of current numerical models could eventually end up as regulators in the latent space of deep neural weather forecasting networks. So, to answer the question posed in the title of this article, we can only say that there might be potential for end-to-end DL weather forecast applications to produce equal or better quality forecasts for specific end-user demands, especially if these systems can exploit small-scale patterns in the observational data which are not resolved in the traditional NWP model chain. Whether DL will evolve enough to replace most or all of the current NWP systems cannot be answered at this point.</p></sec></body><back><ack><title>Acknowledgements</title><p>The authors are grateful to two anonymous reviewers who provided very helpful comments.</p></ack><sec id="s9"><title>Data accessibility</title><p>This article has no additional data.</p></sec><sec id="s10"><title>Authors' contributions</title><p>M.G.S. designed and conceived the study. All authors jointly drafted, read and approved the manuscript.</p></sec><sec id="s11" sec-type="COI-statement"><title>Competing interests</title><p>We declare we have no competing interests.</p></sec><sec id="s12"><title>Funding</title><p>C.B., B.G., F.K., L.H.L. and S.S. acknowledge funding from ERC-2017-ADG#787576 (IntelliAQ); B.G., F.K., M.L. and A.M. are funded through BMBF grant no. 01 IS 18047 (DeepRain). M.G.S. is funded by the programme Supercomputing &#x00026; Big Data of the Helmholtz Association&#x02019;s research field Key Technologies.</p></sec><ref-list><title>References</title><ref id="RSTA20200097C1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lynch</surname>
<given-names>P</given-names></string-name></person-group>
<year>2008</year>
<article-title>The origins of computer weather prediction and climate modeling</article-title>. <source>J. Comput. Phys.</source>
<volume>227</volume>, <fpage>3431</fpage>&#x02013;<lpage>3444</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2007.02.034</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bauer</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Thorpe</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Brunet</surname>
<given-names>G</given-names></string-name></person-group>
<year>2015</year>
<article-title>The quiet revolution of numerical weather prediction</article-title>. <source>Nature</source>
<volume>525</volume>, <fpage>47</fpage>&#x02013;<lpage>55</lpage>. (<pub-id pub-id-type="doi">10.1038/nature14956</pub-id>)<pub-id pub-id-type="pmid">26333465</pub-id></mixed-citation></ref><ref id="RSTA20200097C3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schmidhuber</surname>
<given-names>J</given-names></string-name></person-group>
<year>2015</year>
<article-title>Deep learning in neural networks: an overview</article-title>. <source>Neural Netw.</source>
<volume>61</volume>, <fpage>85</fpage>&#x02013;<lpage>117</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neunet.2014.09.003</pub-id>)<pub-id pub-id-type="pmid">25462637</pub-id></mixed-citation></ref><ref id="RSTA20200097C4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCulloch</surname>
<given-names>WS</given-names></string-name>, <string-name><surname>Pitts</surname>
<given-names>W</given-names></string-name></person-group>
<year>1943</year>
<article-title>A logical calculus of the ideas immanent in nervous activity</article-title>. <source>Bull. Math. Biophys.</source>
<volume>5</volume>, <fpage>115</fpage>&#x02013;<lpage>133</lpage>. (<pub-id pub-id-type="doi">10.1007/BF02478259</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C5"><label>5</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Linnainmaa</surname>
<given-names>S</given-names></string-name></person-group>
<year>1970</year>
<comment>The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors. <italic>Master&#x02019;s Thesis (in Finnish), Univ. Helsinki</italic></comment>.</mixed-citation></ref><ref id="RSTA20200097C6"><label>6</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>LeCun</surname>
<given-names>Y</given-names></string-name></person-group>
<year>1988</year>
<comment>A theoretical framework for back-propagation. In <italic>Proc. of the 1988 Connectionist Models Summer School, CMU, Pittsburg, PA</italic> (eds D Touretzky, G Hinton, T Sejnowski), pp. 21&#x02013;28. Morgan Kaufmann</comment>.</mixed-citation></ref><ref id="RSTA20200097C7"><label>7</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Rumelhart</surname>
<given-names>DE</given-names></string-name>, <string-name><surname>Hinton</surname>
<given-names>GE</given-names></string-name>, <string-name><surname>Williams</surname>
<given-names>RJ</given-names></string-name></person-group>
<year>1986</year>
<comment><italic>Learning Internal Representations by Error Propagation</italic>, pp. 318&#x02013;362. Cambridge, MA: MIT Press</comment>.</mixed-citation></ref><ref id="RSTA20200097C8"><label>8</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname>
<given-names>GE</given-names></string-name></person-group>
<year>2012</year>
<comment>Imagenet classification with deep convolutional neural networks. In <italic>Advances in Neural Information Processing Systems 25</italic> (eds F Pereira, CJC Burges, L Bottou, KQ Weinberger), pp. 1097&#x02013;1105. Curran Associates, Inc</comment>.</mixed-citation></ref><ref id="RSTA20200097C9"><label>9</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Amodei</surname>
<given-names>D</given-names></string-name></person-group>
<italic>et al.</italic>
<year>2016</year>
<comment>Deep speech 2: end-to-end speech recognition in english and mandarin. In <italic>Proc. of The 33rd Int. Conf. on Machine Learning</italic>, volume 48 of <italic>Proc. of Machine Learning Research</italic>, pp. 173&#x02013;182. New York, NY: PMLR. (<uri xlink:href="https://arxiv.org/abs/1512.02595">https://arxiv.org/abs/1512.02595</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silver</surname>
<given-names>D</given-names></string-name></person-group>
<italic>et al.</italic>
<year>2016</year>
<article-title>Mastering the game of go with deep neural networks and tree search</article-title>. <source>Nature</source>
<volume>529</volume>, <fpage>484</fpage>&#x02013;<lpage>489</lpage>. (<pub-id pub-id-type="doi">10.1038/nature16961</pub-id>)<pub-id pub-id-type="pmid">26819042</pub-id></mixed-citation></ref><ref id="RSTA20200097C11"><label>11</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Oh</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Guo</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Lee</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Lewis</surname>
<given-names>RL</given-names></string-name>, <string-name><surname>Singh</surname>
<given-names>S</given-names></string-name></person-group>
<year>2015</year>
<comment>Action-conditional video prediction using deep networks in atari games. In <italic>Advances in Neural Information Processing Systems 28</italic> (eds C Cortes, ND Lawrence, DD Lee, M Sugiyama, R Garnett), pp. 2863&#x02013;2871. Curran Associates, Inc</comment>.</mixed-citation></ref><ref id="RSTA20200097C12"><label>12</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Mathieu</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Couprie</surname>
<given-names>C</given-names></string-name>, <string-name><surname>LeCun</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2015</year>
<comment>Deep multi-scale video prediction beyond mean square error. (<uri xlink:href="http://arxiv.org/abs/1511.05440">http://arxiv.org/abs/1511.05440</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C13"><label>13</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Lee</surname>
<given-names>AX</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ebert</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Abbeel</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Finn</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Levine</surname>
<given-names>S</given-names></string-name></person-group>
<year>2018</year>
<comment>Stochastic adversarial video prediction. (<uri xlink:href="http://arxiv.org/abs/1804.01523">http://arxiv.org/abs/1804.01523</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reichstein</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Camps-Valls</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Stevens</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Jung</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Denzler</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Carvalhais</surname>
<given-names>N</given-names></string-name></person-group>
<year>2019</year>
<article-title>Deep learning and process understanding for data-driven earth system science</article-title>. <source>Nature</source>
<volume>566</volume>, <fpage>195</fpage>&#x02013;<lpage>204</lpage>. (<pub-id pub-id-type="doi">10.1038/s41586-019-0912-1</pub-id>)<pub-id pub-id-type="pmid">30760912</pub-id></mixed-citation></ref><ref id="RSTA20200097C15"><label>15</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Jiang</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Jia</surname>
<given-names>Q</given-names></string-name>, <string-name><surname>Guan</surname>
<given-names>X</given-names></string-name></person-group>
<year>2017</year>
<comment>Review of wind power forecasting methods: From multi-spatial and temporal perspective. In <italic>2017 36th Chinese Control Conference (CCC)</italic>, pp. 10&#x02009;576&#x02013;10&#x02009;583</comment> (<pub-id pub-id-type="doi">10.23919/chicc.2017.8029042</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C16"><label>16</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Grover</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Kapoor</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Horvitz</surname>
<given-names>E</given-names></string-name></person-group>
<year>2015</year>
<comment>A deep hybrid model for weather forecasting. In <italic>Proc. of the 21th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</italic>, KDD &#x02019;15, pp. 379&#x02013;386. New York, NY: Association for Computing Machinery.</comment> (<pub-id pub-id-type="doi">10.1145/2783258.2783275</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C17"><label>17</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Th&#x000e9;paut</surname>
<given-names>JN</given-names></string-name>, <string-name><surname>Andersson</surname>
<given-names>E</given-names></string-name></person-group>
<year>2010</year>
<comment><italic>The global observing system</italic>, pp. 263&#x02013;281. Berlin, Heidelberg: Springer.</comment> (<pub-id pub-id-type="doi">10.1007/978-3-540-74703-1_10</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bannister</surname>
<given-names>RN</given-names></string-name></person-group>
<year>2017</year>
<article-title>A review of operational methods of variational and ensemble-variational data assimilation: ensemble-variational data assimilation</article-title>. <source>Q. J. R. Meteorol. Soc.</source>
<volume>143</volume>, <fpage>607</fpage>&#x02013;<lpage>633</lpage>. (<pub-id pub-id-type="doi">10.1002/qj.2982</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Al-Yahyai</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Charabi</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Gastli</surname>
<given-names>A</given-names></string-name></person-group>
<year>2010</year>
<article-title>Review of the use of numerical weather prediction (NWP) models for wind energy assessment</article-title>. <source>Renew. Sustain. Energy Rev.</source>
<volume>14</volume>, <fpage>3192</fpage>&#x02013;<lpage>3198</lpage>. (<pub-id pub-id-type="doi">10.1016/j.rser.2010.07.001</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pelland</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Galanis</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Kallos</surname>
<given-names>G</given-names></string-name></person-group>
<year>2013</year>
<article-title>Solar and photovoltaic forecasting through post-processing of the Global Environmental Multiscale numerical weather prediction model: solar and photovoltaic forecasting</article-title>. <source>Prog. Photovoltaics Res. Appl.</source>
<volume>21</volume>, <fpage>284</fpage>&#x02013;<lpage>296</lpage>. (<pub-id pub-id-type="doi">10.1002/pip.1180</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C21"><label>21</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Haiden</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Janousek</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Vitart</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Ferranti</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Prates</surname>
<given-names>F</given-names></string-name></person-group>
<year>2019</year>
<comment>Evaluation of ECMWF forecasts, including the 2019 upgrade.</comment> (<pub-id pub-id-type="doi">10.21957/mlvapkke</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>
<given-names>JH</given-names></string-name></person-group>
<italic>et al.</italic>
<year>2019</year>
<article-title>Advancements in hurricane prediction with noaa&#x02019;s next-generation forecast system</article-title>. <source>Geophys. Res. Lett.</source>
<volume>46</volume>, <fpage>4495</fpage>&#x02013;<lpage>4501</lpage>. (<pub-id pub-id-type="doi">10.1029/2019GL082410</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geer</surname>
<given-names>AJ</given-names></string-name></person-group>
<italic>et al.</italic>
<year>2017</year>
<article-title>The growing impact of satellite observations sensitive to humidity, cloud and precipitation</article-title>. <source>Q. J. R. Meteorol. Soc.</source>
<volume>143</volume>, <fpage>3189</fpage>&#x02013;<lpage>3206</lpage>. (<pub-id pub-id-type="doi">10.1002/qj.3172</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lange</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Janjic Pfander</surname>
<given-names>T</given-names></string-name></person-group>
<year>2015</year>
<article-title>Assimilation of mode-S EHS aircraft observations in cosmo-kenda</article-title>. <source>Mon. Weather Rev.</source>
<volume>144</volume>, <fpage>151221072928005</fpage> (<pub-id pub-id-type="doi">10.1175/MWR-D-15-0112.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schraff</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Reich</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Rhodin</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Schomburg</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Stephan</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Peri&#x000e0;&#x000f1;ez</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Potthast</surname>
<given-names>R</given-names></string-name></person-group>
<year>2016</year>
<article-title>Kilometre-scale ensemble data assimilation for the cosmo model (kenda)</article-title>. <source>Q. J. R. Meteorol. Soc.</source>
<volume>142</volume>, <fpage>1453</fpage>&#x02013;<lpage>1472</lpage>. (<pub-id pub-id-type="doi">10.1002/qj.2748</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Huang</surname>
<given-names>XY</given-names></string-name>, <string-name><surname>Aulign&#x000e9;</surname>
<given-names>T</given-names></string-name></person-group>
<year>2013</year>
<article-title>Radar data assimilation with WRF 4D-Var. Part I: system development and preliminary testing</article-title>. <source>Mon. Weather Rev.</source>
<volume>141</volume>, <fpage>2224</fpage>&#x02013;<lpage>2244</lpage>. (<pub-id pub-id-type="doi">10.1175/MWR-D-12-00168.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vuki&#x00107;evi&#x00107;</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Bao</surname>
<given-names>JW</given-names></string-name></person-group>
<year>1998</year>
<article-title>The effect of linearization errors on 4dvar data assimilation</article-title>. <source>Mon. Weather Rev.</source>
<volume>126</volume>, <fpage>1695</fpage>&#x02013;<lpage>1706</lpage>. (<pub-id pub-id-type="doi">10.1175/1520-0493(1998)126&#x000a1;1695:TEOLEO&#x000bf;2.0.CO;2</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steele-Dunne</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Entekhabi</surname>
<given-names>D</given-names></string-name></person-group>
<year>2005</year>
<article-title>An ensemble-based reanalysis approach to land data assimilation</article-title>. <source>Water Resour. Res.</source>
<volume>41</volume> (<pub-id pub-id-type="doi">10.1029/2004WR003449</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Hansen</surname>
<given-names>JA</given-names></string-name></person-group>
<year>2009</year>
<article-title>Coupling ensemble kalman filter with four-dimensional variational data assimilation</article-title>. <source>Adv. Atmos. Sci.</source>
<volume>26</volume>, <fpage>1</fpage>&#x02013;<lpage>8</lpage>. (<pub-id pub-id-type="doi">10.1007/s00376-009-0001-8</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Z&#x000e4;ngl</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Reinert</surname>
<given-names>D</given-names></string-name>, <string-name><surname>R&#x000ed;podas</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Baldauf</surname>
<given-names>M</given-names></string-name></person-group>
<year>2015</year>
<article-title>The icon (icosahedral non-hydrostatic) modelling framework of DWD and MPI-M: description of the non-hydrostatic dynamical core</article-title>. <source>Q. J. R. Meteorol. Soc.</source>
<volume>141</volume>, <fpage>563</fpage>&#x02013;<lpage>579</lpage>. (<pub-id pub-id-type="doi">10.1002/qj.2378</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tomita</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Satoh</surname>
<given-names>M</given-names></string-name></person-group>
<year>2004</year>
<article-title>A new dynamical framework of nonhydrostatic global model using the icosahedral grid</article-title>. <source>Fluid Dyn. Res.</source>
<volume>34</volume>, <fpage>357</fpage>&#x02013;<lpage>400</lpage>. (<pub-id pub-id-type="doi">10.1016/j.fluiddyn.2004.03.003</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ullrich</surname>
<given-names>PA</given-names></string-name>, <string-name><surname>Jablonowski</surname>
<given-names>C</given-names></string-name></person-group>
<year>2012</year>
<article-title>Mcore: A non-hydrostatic atmospheric dynamical core utilizing high-order finite-volume methods</article-title>. <source>J. Comput. Phys.</source>
<volume>231</volume>, <fpage>5078</fpage>&#x02013;<lpage>5108</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2012.04.024</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ringler</surname>
<given-names>TD</given-names></string-name>, <string-name><surname>Thuburn</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Klemp</surname>
<given-names>JB</given-names></string-name>, <string-name><surname>Skamarock</surname>
<given-names>WC</given-names></string-name></person-group>
<year>2010</year>
<article-title>A unified approach to energy conservation and potential vorticity dynamics for arbitrarily-structured c-grids</article-title>. <source>J. Comput. Phys.</source>
<volume>229</volume>, <fpage>3065</fpage>&#x02013;<lpage>3090</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2009.12.007</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wood</surname>
<given-names>N</given-names></string-name></person-group>
<italic>et al.</italic>
<year>2014</year>
<article-title>An inherently mass-conserving semi-implicit semi-lagrangian discretization of the deep-atmosphere global non-hydrostatic equations</article-title>. <source>Q. J. R. Meteorol. Soc.</source>
<volume>140</volume>, <fpage>1505</fpage>&#x02013;<lpage>1520</lpage>. (<pub-id pub-id-type="doi">10.1002/qj.2235</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gassmann</surname>
<given-names>A</given-names></string-name></person-group>
<year>2018</year>
<article-title>Discretization of generalized coriolis and friction terms on the deformed hexagonal c-grid</article-title>. <source>Q. J. R. Meteorol. Soc.</source>
<volume>144</volume>, <fpage>2038</fpage>&#x02013;<lpage>2053</lpage>. (<pub-id pub-id-type="doi">10.1002/qj.3294</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bechtold</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Semane</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Lopez</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Chaboureau</surname>
<given-names>JP</given-names></string-name>, <string-name><surname>Beljaars</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Bormann</surname>
<given-names>N</given-names></string-name></person-group>
<year>2014</year>
<article-title>Representing equilibrium and nonequilibrium convection in large-scale models</article-title>. <source>J. Atmos. Sci.</source>
<volume>71</volume>, <fpage>734</fpage>&#x02013;<lpage>753</lpage>. (<pub-id pub-id-type="doi">10.1175/JAS-D-13-0163.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwon</surname>
<given-names>YC</given-names></string-name>, <string-name><surname>Hong</surname>
<given-names>SY</given-names></string-name></person-group>
<year>2017</year>
<article-title>A mass-flux cumulus parameterization scheme across gray-zone resolutions</article-title>. <source>Mon. Weather Rev.</source>
<volume>145</volume>, <fpage>583</fpage>&#x02013;<lpage>598</lpage>. (<pub-id pub-id-type="doi">10.1175/MWR-D-16-0034.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neggers</surname>
<given-names>RAJ</given-names></string-name>, <string-name><surname>K&#x000f6;hler</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Beljaars</surname>
<given-names>ACM</given-names></string-name></person-group>
<year>2009</year>
<article-title>A dual mass flux framework for boundary layer convection. Part I: transport</article-title>. <source>J. Atmos. Sci.</source>
<volume>66</volume>, <fpage>1465</fpage>&#x02013;<lpage>1487</lpage>. (<pub-id pub-id-type="doi">10.1175/2008JAS2635.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seifert</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Beheng</surname>
<given-names>KD</given-names></string-name></person-group>
<year>2006</year>
<article-title>A two-moment cloud microphysics parameterization for mixed-phase clouds. Part 1: model description</article-title>. <source>Meteorol. Atmos. Phys.</source>
<volume>92</volume>, <fpage>45</fpage>&#x02013;<lpage>66</lpage>. (<pub-id pub-id-type="doi">10.1007/s00703-005-0112-4</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C40"><label>40</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>LeCun</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Haffner</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Bottou</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Bengio</surname>
<given-names>Y</given-names></string-name></person-group>
<year>1999</year>
<comment><italic>Object Recognition with Gradient-Based Learning</italic>, pp. 319&#x02013;345. Berlin, Heidelberg: Springer.</comment> (<pub-id pub-id-type="doi">10.1007/3-540-46805-6_19</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname>
<given-names>XX</given-names></string-name>, <string-name><surname>Tuia</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Mou</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Xia</surname>
<given-names>GS</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Xu</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Fraundorfer</surname>
<given-names>F</given-names></string-name></person-group>
<year>2017</year>
<article-title>Deep learning in remote sensing: a comprehensive review and list of resources</article-title>. <source>IEEE Geosci. Remote Sensing Mag.</source>
<volume>5</volume>, <fpage>8</fpage>&#x02013;<lpage>36</lpage>. (<pub-id pub-id-type="doi">10.1109/MGRS.2017.2762307</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gagne</surname>
<suffix>II</suffix>
<given-names>DJ</given-names></string-name>, <string-name><surname>Haupt</surname>
<given-names>SE</given-names></string-name>, <string-name><surname>Nychka</surname>
<given-names>DW</given-names></string-name>, <string-name><surname>Thompson</surname>
<given-names>G</given-names></string-name></person-group>
<year>2019</year>
<article-title>Interpretable deep learning for spatial analysis of severe hailstorms</article-title>. <source>Mon. Weather Rev.</source>
<volume>147</volume>, <fpage>2827</fpage>&#x02013;<lpage>2845</lpage>. (<pub-id pub-id-type="doi">10.1175/MWR-D-18-0316.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochreiter</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Schmidhuber</surname>
<given-names>J</given-names></string-name></person-group>
<year>1997</year>
<article-title>Long short-term memory</article-title>. <source>Neural Comput.</source>
<volume>9</volume>, <fpage>1735</fpage>&#x02013;<lpage>1780</lpage>. (<pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>)<pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation></ref><ref id="RSTA20200097C44"><label>44</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Gers</surname>
<given-names>FA</given-names></string-name>, <string-name><surname>Schmidhuber</surname>
<given-names>J</given-names></string-name></person-group>
<year>2000</year>
<comment>Recurrent nets that time and count. In <italic>Proc. of the IEEE-INNS-ENNS Int. Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</italic>, volume 3, pp. 189&#x02013;194. Como, Italy, Italy.</comment> (<pub-id pub-id-type="doi">10.1109/IJCNN.2000.861302</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C45"><label>45</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Chung</surname>
<given-names>J</given-names></string-name>, <string-name><surname>G&#x000fc;l&#x000e7;ehre</surname>
<given-names>&#x000c7;</given-names></string-name>, <string-name><surname>Cho</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Bengio</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2014</year>
<comment>Empirical evaluation of gated recurrent neural networks on sequence modeling. <italic>CoRR</italic><bold>abs/1412.3555</bold>. (<uri xlink:href="http://arxiv.org/abs/1412.3555">http://arxiv.org/abs/1412.3555</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C46"><label>46</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Xingjian</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Chen</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Yeung</surname>
<given-names>DY</given-names></string-name>, <string-name><surname>Wong</surname>
<given-names>WK</given-names></string-name>, <string-name><surname>Woo</surname>
<given-names>Wc</given-names></string-name></person-group>
<year>2015</year>
<comment>Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In <italic>Advances in neural information processing systems</italic>, pp. 802&#x02013;810</comment>.</mixed-citation></ref><ref id="RSTA20200097C47"><label>47</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Kingma</surname>
<given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname>
<given-names>M</given-names></string-name></person-group>
<year>2014</year>
<comment>Auto-Encoding Variational Bayes. <italic>CoRR</italic>. (<uri xlink:href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C48"><label>48</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Goodfellow</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Pouget-Abadie</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Mirza</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Xu</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Warde-Farley</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Ozair</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Courville</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Bengio</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2014</year>
<comment>Generative adversarial nets. In <italic>Advances in Neural Information Processing Systems</italic>, pp. 2672&#x02013;2680. Curran Associates, Inc</comment>.</mixed-citation></ref><ref id="RSTA20200097C49"><label>49</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Isola</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Zhu</surname>
<given-names>JY</given-names></string-name>, <string-name><surname>Zhou</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Efros</surname>
<given-names>AA</given-names></string-name></person-group>
<year>2017</year>
<comment>Image-to-image translation with conditional adversarial networks. In <italic>Proc. of the IEEE Conf. on Ccomputer Vision and Pattern Recognition</italic>, Vol. 1, pp. 5967&#x02013;5976.</comment> (<pub-id pub-id-type="doi">10.1109/CVPR.2017.632</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C50"><label>50</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Johnson</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Alahi</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Fei-Fei</surname>
<given-names>L</given-names></string-name></person-group>
<year>2016</year>
<comment>Perceptual losses for real-time style transfer and super-resolution. <italic>Lecture Notes in Computer Science</italic>, pp. 694&#x02013;711.</comment> (<pub-id pub-id-type="doi">10.1007/978-3-319-46475-6_43</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C51"><label>51</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Pathak</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Krahenbuhl</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Donahue</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Darrell</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Efros</surname>
<given-names>AA</given-names></string-name></person-group>
<year>2016</year>
<comment>Context encoders: Feature learning by inpainting. In <italic>2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</italic>, pp. 2536&#x02013;2544. Los Alamitos, CA, USA: IEEE Computer Society.</comment> (<pub-id pub-id-type="doi">10.1109/CVPR.2016.278</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C52"><label>52</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Zhang</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Sindagi</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Patel</surname>
<given-names>VM</given-names></string-name></person-group>
<year>2019</year>
<comment>Image de-raining using a conditional generative adversarial network. <italic>IEEE Transactions on Circuits and Systems for Video Technology</italic> p. 1&#x02013;1.</comment> (<pub-id pub-id-type="doi">10.1109/tcsvt.2019.2920407</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C53"><label>53</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Reed</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Akata</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Yan</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Logeswaran</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Schiele</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Lee</surname>
<given-names>H</given-names></string-name></person-group>
<year>2016</year>
<comment>Generative adversarial text to image synthesis. In <italic>Proc. of The 33rd Int. Conf. on Machine Learning</italic> (eds MF Balcan, KQ Weinberger), volume 48 of <italic>Proc. of Machine Learning Research</italic>, pp. 1060&#x02013;1069. New York, NY: PMLR</comment>.</mixed-citation></ref><ref id="RSTA20200097C54"><label>54</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Li</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Wand</surname>
<given-names>M</given-names></string-name></person-group>
<year>2016</year>
<comment>Precomputed real-time texture synthesis with markovian generative adversarial networks. <italic>Lecture Notes in Computer Science</italic>, pp. 702&#x02013;716.</comment> (<pub-id pub-id-type="doi">10.1007/978-3-319-46487-9_43</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C55"><label>55</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Kingma</surname>
<given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname>
<given-names>M</given-names></string-name></person-group>
<year>2019</year>
<comment>An introduction to variational autoencoders. (<uri xlink:href="http://arxiv.org/abs/1906.02691">http://arxiv.org/abs/1906.02691</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C56"><label>56</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Gulrajani</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Kumar</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Ahmed</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Taiga</surname>
<given-names>AA</given-names></string-name>, <string-name><surname>Visin</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Vazquez</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Courville</surname>
<given-names>A</given-names></string-name></person-group>
<year>2016</year>
<comment>Pixelvae: A latent variable model for natural images. (<uri xlink:href="http://arxiv.org/abs/1611.05013">http://arxiv.org/abs/1611.05013</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C57"><label>57</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Radford</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Metz</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Chintala</surname>
<given-names>S</given-names></string-name></person-group>
<year>2015</year>
<comment>Unsupervised representation learning with deep convolutional generative adversarial networks. (<uri xlink:href="http://arxiv.org/abs/1511.06434">http://arxiv.org/abs/1511.06434</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C58"><label>58</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Shaham</surname>
<given-names>TR</given-names></string-name>, <string-name><surname>Dekel</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Michaeli</surname>
<given-names>T</given-names></string-name></person-group>
<year>2019</year>
<comment>SinGAN: Learning a generative model from a single natural image. <italic>2019 IEEE/CVF Int. Conf. on Computer Vision (ICCV)</italic>.</comment> (<pub-id pub-id-type="doi">10.1109/iccv.2019.00467</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C59"><label>59</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Schizas</surname>
<given-names>CN</given-names></string-name>, <string-name><surname>Michaelides</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Pattichis</surname>
<given-names>CS</given-names></string-name>, <string-name><surname>Livesay</surname>
<given-names>RR</given-names></string-name></person-group>
<year>1991</year>
<comment>Artificial neural networks in forecasting minimum temperature (weather). In <italic>Second Int. Conf. on Artificial Neural Networks</italic>, volume 349, pp. 112&#x02013;114. Bournemouth Int. CTR, Bournemouth, England: IEEE</comment>.</mixed-citation></ref><ref id="RSTA20200097C60"><label>60</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Comrie</surname>
<given-names>AC</given-names></string-name></person-group>
<year>1997</year>
<article-title>Comparing neural networks and regression models for ozone forecasting</article-title>. <source>J. Air Waste Manage. Assoc.</source>
<volume>47</volume>, <fpage>653</fpage>&#x02013;<lpage>663</lpage>. (<pub-id pub-id-type="doi">10.1080/10473289.1997.10463925</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C61"><label>61</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hall</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Brooks</surname>
<given-names>HE</given-names></string-name>, <string-name><surname>Doswell</surname>
<suffix>III</suffix>
<given-names>CA</given-names></string-name></person-group>
<year>1999</year>
<article-title>Precipitation forecasting using a neural network</article-title>. <source>Weather Forecast.</source>
<volume>14</volume>, <fpage>338</fpage>&#x02013;<lpage>345</lpage>. (<pub-id pub-id-type="doi">10.1175/1520-0434(1999)014&#x000a1;0338:PFUANN&#x000bf;2.0.CO;2</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C62"><label>62</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krasnopolsky</surname>
<given-names>VM</given-names></string-name>, <string-name><surname>Lin</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2012</year>
<article-title>A neural network nonlinear multimodel ensemble to improve precipitation forecasts over continental US</article-title>. <source>Adv. Meteorol.</source>
<volume>2012</volume>, <fpage>1</fpage>&#x02013;<lpage>11</lpage>. (<pub-id pub-id-type="doi">10.1155/2012/649450</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C63"><label>63</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rasp</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Lerch</surname>
<given-names>S</given-names></string-name></person-group>
<year>2018</year>
<article-title>Neural networks for postprocessing ensemble weather forecasts</article-title>. <source>Mon. Weather Rev.</source>
<volume>146</volume>, <fpage>3885</fpage>&#x02013;<lpage>3900</lpage>. (<pub-id pub-id-type="doi">10.1175/mwr-d-18-0187.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C64"><label>64</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krasnopolsky</surname>
<given-names>VM</given-names></string-name>, <string-name><surname>Chalikov</surname>
<given-names>DV</given-names></string-name>, <string-name><surname>Tolman</surname>
<given-names>HL</given-names></string-name></person-group>
<year>2002</year>
<article-title>A neural network technique to improve computational efficiency of numerical oceanic models</article-title>. <source>Ocean Modelling</source>
<volume>4</volume>, <fpage>363</fpage>&#x02013;<lpage>383</lpage>. (<pub-id pub-id-type="doi">10.1016/S1463-5003(02)00010-0</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C65"><label>65</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krasnopolsky</surname>
<given-names>VM</given-names></string-name>, <string-name><surname>Fox-Rabinovitz</surname>
<given-names>MS</given-names></string-name></person-group>
<year>2006</year>
<article-title>Complex hybrid models combining deterministic and machine learning components for numerical climate modeling and weather prediction</article-title>. <source>Neural Netw.</source>
<volume>19</volume>, <fpage>122</fpage>&#x02013;<lpage>134</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neunet.2006.01.002</pub-id>)<pub-id pub-id-type="pmid">16527454</pub-id></mixed-citation></ref><ref id="RSTA20200097C66"><label>66</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Shi</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Murphey</surname>
<given-names>YL</given-names></string-name></person-group>
<year>2018</year>
<comment>Weather recognition based on edge deterioration and convolutional neural networks. In <italic>2018 24th Int. Conf. on Pattern Recognition (ICPR)</italic>, pp. 2438&#x02013;2443. IEEE.</comment> (<pub-id pub-id-type="doi">10.1109/ICPR.2018.8546085</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C67"><label>67</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Simonyan</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname>
<given-names>A</given-names></string-name></person-group>
<year>2014</year>
<comment>Very deep convolutional networks for large-scale image recognition. (<uri xlink:href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C68"><label>68</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Pothineni</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Oswald</surname>
<given-names>MR</given-names></string-name>, <string-name><surname>Poland</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Pollefeys</surname>
<given-names>M</given-names></string-name></person-group>
<year>2019</year>
<comment>Kloudnet: Deep learning for sky image analysis and irradiance forecasting. In <italic>Pattern Recognition</italic> (eds T Brox, A Bruhn, M Fritz), pp. 535&#x02013;551. Cham: Springer International Publishing.</comment> (<pub-id pub-id-type="doi">10.1007/978-3-030--12939-2_37</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C69"><label>69</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>He</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Ren</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>J</given-names></string-name></person-group>
<year>2016</year>
<comment>Deep residual learning for image recognition. In <italic>2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</italic>, pp. 770&#x02013;778. Las Vegas, NV: IEEE.</comment> (<pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C70"><label>70</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Du</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Lim</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Wen</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Jiang</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Xiang</surname>
<given-names>W</given-names></string-name></person-group>
<year>2020</year>
<article-title>Deep learning-based multi-step solar forecasting for PV ramp-rate control using sky images</article-title>. <source>IEEE Trans. Ind. Inf.</source>
<volume>17</volume>, <fpage>1397</fpage>&#x02013;<lpage>1406</lpage>. (<pub-id pub-id-type="doi">10.1109/TII.2020.2987916</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C71"><label>71</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Szegedy</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Jia</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Sermanet</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Reed</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Anguelov</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Erhan</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Vanhoucke</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Rabinovich</surname>
<given-names>A</given-names></string-name></person-group>
<year>2015</year>
<comment>Going deeper with convolutions. In <italic>The IEEE Conf. on Computer Vision and Pattern Recognition (CVPR2015)</italic>.</comment> (<pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C72"><label>72</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kleinert</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Leufen</surname>
<given-names>LH</given-names></string-name>, <string-name><surname>Schultz</surname>
<given-names>MG</given-names></string-name></person-group>
<year>2021</year>
<article-title>IntelliO3-ts v1.0: a neural network approach to predict near-surface ozone concentrations in Germany</article-title>. <source>Geosci. Model Dev.</source>
<volume>14</volume>, <fpage>1</fpage>&#x02013;<lpage>25</lpage>. (<pub-id pub-id-type="doi">10.5194/gmd-14-1-2021</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C73"><label>73</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bengio</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Simard</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Frasconi</surname>
<given-names>P</given-names></string-name></person-group>
<year>1994</year>
<article-title>Learning long-term dependencies with gradient descent is difficult</article-title>. <source>IEEE Trans. Neural Netw.</source>
<volume>5</volume>, <fpage>157</fpage>&#x02013;<lpage>166</lpage>. (<pub-id pub-id-type="doi">10.1109/72.279181</pub-id>)<pub-id pub-id-type="pmid">18267787</pub-id></mixed-citation></ref><ref id="RSTA20200097C74"><label>74</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>G&#x000f3;mez</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Nebot</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Ribeiro</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Alqu&#x000e9;zar</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Mugica</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Wotawa</surname>
<given-names>F</given-names></string-name></person-group>
<year>2003</year>
<article-title>Local maximum ozone concentration prediction using soft computing methodologies</article-title>. <source>Syst. Anal. Model. Simul.</source>
<volume>43</volume>, <fpage>1011</fpage>&#x02013;<lpage>1031</lpage>. (<pub-id pub-id-type="doi">10.1080/0232929031000081244</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C75"><label>75</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qing</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Niu</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2018</year>
<article-title>Hourly day-ahead solar irradiance prediction using weather forecasts by LSTM</article-title>. <source>Energy</source>
<volume>148</volume>, <fpage>461</fpage>&#x02013;<lpage>468</lpage>. (<pub-id pub-id-type="doi">10.1016/j.energy.2018.01.177</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C76"><label>76</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Le Guen</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Thome</surname>
<given-names>N</given-names></string-name></person-group>
<year>2020</year>
<comment>Disentangling physical dynamics from unknown factors for unsupervised video prediction. (<uri xlink:href="http://arxiv.org/abs/2003.01460">http://arxiv.org/abs/2003.01460</uri>)</comment>.</mixed-citation></ref><ref id="RSTA20200097C77"><label>77</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Smolensky</surname>
<given-names>P</given-names></string-name></person-group>
<year>1986</year>
<comment><italic>Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory</italic>, pp. 194&#x02013;281. Cambridge, MA: MIT Press</comment>.</mixed-citation></ref><ref id="RSTA20200097C78"><label>78</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hinton</surname>
<given-names>GE</given-names></string-name>, <string-name><surname>Salakhutdinov</surname>
<given-names>RR</given-names></string-name></person-group>
<year>2006</year>
<article-title>Reducing the dimensionality of data with neural networks</article-title>. <source>Science</source>
<volume>313</volume>, <fpage>504</fpage>&#x02013;<lpage>507</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1127647</pub-id>)<pub-id pub-id-type="pmid">16873662</pub-id></mixed-citation></ref><ref id="RSTA20200097C79"><label>79</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Xiong</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Luo</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Ma</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Luo</surname>
<given-names>J</given-names></string-name></person-group>
<year>2018</year>
<comment>Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In <italic>2018 IEEE/CVF Conf. on Computer Vision and Pattern Recognition</italic>. IEEE.</comment> (<pub-id pub-id-type="doi">10.1109/cvpr.2018.00251</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C80"><label>80</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Schmidt</surname>
<given-names>VH</given-names></string-name>, <string-name><surname>Alghali</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Sankaran</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Yuan</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Bengio</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2020</year>
<comment>Modeling cloud reflectance fields using conditional generative adversarial networks. <italic>arXiv: Atmospheric and Oceanic Physics</italic></comment>.</mixed-citation></ref><ref id="RSTA20200097C81"><label>81</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Long</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Gao</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Yu</surname>
<given-names>PS</given-names></string-name></person-group>
<year>2017</year>
<comment>PredRNN: recurrent neural networks for predictive learning using spatiotemporal LSTMs. In <italic>Proc. of the 31st Int. Conf. on Neural Information Processing Systems (NIPS 2017)</italic>, pp. 879&#x02013;888</comment>.</mixed-citation></ref><ref id="RSTA20200097C82"><label>82</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Zhu</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Long</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Yu</surname>
<given-names>PS</given-names></string-name></person-group>
<year>2019</year>
<comment>Memory in Memory: a predictive neural network for learning higher-order non-stationarity from spatiotemporal dynamics. In <italic>The IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</italic>.</comment> (<pub-id pub-id-type="doi">10.1109/CVPR.2019.00937</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C83"><label>83</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Xu</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Jin</surname>
<given-names>YQ</given-names></string-name></person-group>
<year>2016</year>
<article-title>Polarimetric SAR image classification using deep convolutional neural networks</article-title>. <source>IEEE Geosci. Remote Sens. Lett.</source>
<volume>13</volume>, <fpage>1935</fpage>&#x02013;<lpage>1939</lpage>. (<pub-id pub-id-type="doi">10.1109/LGRS.2016.2618840</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C84"><label>84</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denby</surname>
<given-names>L</given-names></string-name></person-group>
<year>2020</year>
<article-title>Discovering the importance of mesoscale cloud organization through unsupervised classification</article-title>. <source>GeoRL</source>
<volume>47</volume>, <fpage>e85190</fpage> (<pub-id pub-id-type="doi">10.1029/2019GL085190</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C85"><label>85</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Xu</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Du</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Jiang</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Ren</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2019</year>
<comment>Satellite image prediction relying on gan and lstm neural networks. In <italic>ICC 2019-2019 IEEE Int. Conf. on Communications (ICC)</italic>, pp. 1&#x02013;6. IEEE.</comment> (<pub-id pub-id-type="doi">10.1109/ICC.2019.8761462</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C86"><label>86</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dueben</surname>
<given-names>PD</given-names></string-name>, <string-name><surname>Bauer</surname>
<given-names>P</given-names></string-name></person-group>
<year>2018</year>
<article-title>Challenges and design choices for global weather and climate models based on machine learning</article-title>. <source>Geoscientific Model Dev.</source>
<volume>11</volume>, <fpage>3999</fpage>&#x02013;<lpage>4009</lpage>. (<pub-id pub-id-type="doi">10.5194/gmd-11-3999-2018</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C87"><label>87</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Wandel</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Weinmann</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Klein</surname>
<given-names>R</given-names></string-name></person-group>
<year>2020</year>
<comment>Unsupervised deep learning of incompressible fluid dynamics. <uri xlink:href="http://arxiv.org/abs/2006.08762">http://arxiv.org/abs/2006.08762</uri></comment>.</mixed-citation></ref><ref id="RSTA20200097C88"><label>88</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Parisi</surname>
<given-names>GI</given-names></string-name>, <string-name><surname>Kemker</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Part</surname>
<given-names>JL</given-names></string-name>, <string-name><surname>Kanan</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Wermter</surname>
<given-names>S</given-names></string-name></person-group>
<year>2018</year>
<comment>Continual lifelong learning with neural networks: a review. <uri xlink:href="http://arxiv.org/abs/1802.07569v4">http://arxiv.org/abs/1802.07569v4</uri></comment>.</mixed-citation></ref><ref id="RSTA20200097C89"><label>89</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Silver</surname>
<given-names>DL</given-names></string-name>, <string-name><surname>Bennett</surname>
<given-names>KP</given-names></string-name></person-group>
<year>2008</year>
<article-title>Guest editor&#x02019;s introduction: special issue on inductive transfer learning</article-title>. <source>Mach. Learn.</source>
<volume>73</volume>, <fpage>215</fpage>&#x02013;<lpage>220</lpage>. (<pub-id pub-id-type="doi">10.1007/s10994-008-5087-1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C90"><label>90</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Schultz</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Pleiter</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Bauer</surname>
<given-names>P</given-names></string-name></person-group> eds. <year>2019</year>, <comment>2018. Extreme Data Workshop 2018, Forschungszentrum J&#x000fc;lich, 18&#x02013;19 September 2018, ISBN 978-3-95806-392-1, Schriften des Forschungszentrums J&#x000fc;lich, IAS Series 40</comment>.</mixed-citation></ref><ref id="RSTA20200097C91"><label>91</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wilks</surname>
<given-names>DS</given-names></string-name></person-group>
<year>2006</year>
<source>Statistical methods in the atmospheric sciences</source>, <volume>vol. 91</volume>, <edition>2nd edn</edition>
<series>International geophysics series</series>
<publisher-loc>Amsterdam; The Netherlands</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</mixed-citation></ref><ref id="RSTA20200097C92"><label>92</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Vandal</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Kodra</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Dy</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Ganguly</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Nemani</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ganguly</surname>
<given-names>AR</given-names></string-name></person-group>
<year>2018</year>
<comment>Quantifying uncertainty in discrete-continuous and skewed data with Bayesian deep learning. In <italic>Proc. of the 24th ACM SIGKDD Int. Conf. on Knowledge Discovery &#x00026; Data Mining</italic>, KDD-18, p. 2377&#x02013;2386. New York, NY: Association for Computing Machinery.</comment> (<pub-id pub-id-type="doi">10.1145/3219819.3219996</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C93"><label>93</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Orlanski</surname>
<given-names>I</given-names></string-name></person-group>
<year>1975</year>
<article-title>A rational subdivision of scales for atmospheric processes</article-title>. <source>Bull. Am. Meteorol. Soc.</source>
<volume>56</volume>, <fpage>527</fpage>&#x02013;<lpage>530</lpage>.</mixed-citation></ref><ref id="RSTA20200097C94"><label>94</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Semmler</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Jung</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Serrar</surname>
<given-names>S</given-names></string-name></person-group>
<year>2016</year>
<article-title>Fast atmospheric response to a sudden thinning of Arctic sea ice</article-title>. <source>Clim. Dyn.</source>
<volume>46</volume>, <fpage>1015</fpage>&#x02013;<lpage>1025</lpage>. (<pub-id pub-id-type="doi">10.1007/s00382-015-2629-7</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C95"><label>95</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Partal</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Ki&#x0015f;i</surname>
<given-names>&#x000d6;</given-names></string-name></person-group>
<year>2007</year>
<article-title>Wavelet and neuro-fuzzy conjunction model for precipitation forecasting</article-title>. <source>J. Hydrol.</source>
<volume>342</volume>, <fpage>199</fpage>&#x02013;<lpage>212</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jhydrol.2007.05.026</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C96"><label>96</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramana</surname>
<given-names>RV</given-names></string-name>, <string-name><surname>Krishna</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Kumar</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Pandey</surname>
<given-names>N</given-names></string-name></person-group>
<year>2013</year>
<article-title>Monthly rainfall prediction using wavelet neural network analysis</article-title>. <source>Water Resour. Manage</source>
<volume>27</volume>, <fpage>3697</fpage>&#x02013;<lpage>3711</lpage>. (<pub-id pub-id-type="doi">10.1007/s11269-013-0374-4</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C97"><label>97</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Solgi</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Nourani</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Pourhaghi</surname>
<given-names>A</given-names></string-name></person-group>
<year>2014</year>
<article-title>Forecasting daily precipitation using hybrid model of wavelet-artificial neural network and comparison with adaptive neurofuzzy inference system (case study: Verayneh station, Nahavand)</article-title>. <source>Adv. Civil Eng.</source>
<volume>2014</volume>, <fpage>1</fpage>&#x02013;<lpage>12</lpage>. (<pub-id pub-id-type="doi">10.1155/2014/279368</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C98"><label>98</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kisi</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Cimen</surname>
<given-names>M</given-names></string-name></person-group>
<year>2012</year>
<article-title>Precipitation forecasting by using wavelet-support vector machine conjunction model</article-title>. <source>Eng. Appl. Artif. Intell.</source>
<volume>25</volume>, <fpage>783</fpage>&#x02013;<lpage>792</lpage>. (<pub-id pub-id-type="doi">10.1016/j.engappai.2011.11.003</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C99"><label>99</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Ziyin</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Hartwig</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Ueda</surname>
<given-names>M</given-names></string-name></person-group>
<year>2020</year>
<comment>Neural networks fail to learn periodic functions and how to fix it. <uri xlink:href="http://arxiv.org/abs/2006.08195">http://arxiv.org/abs/2006.08195</uri></comment>.</mixed-citation></ref><ref id="RSTA20200097C100"><label>100</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Cirstea</surname>
<given-names>RG</given-names></string-name>, <string-name><surname>Micu</surname>
<given-names>DV</given-names></string-name>, <string-name><surname>Muresan</surname>
<given-names>GM</given-names></string-name>, <string-name><surname>Guo</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Yang</surname>
<given-names>B</given-names></string-name></person-group>
<year>2018</year>
<comment>Correlated time series forecasting using deep neural networks: a summary of results. <uri xlink:href="http://arxiv.org/abs/1808.09794">http://arxiv.org/abs/1808.09794</uri></comment>.</mixed-citation></ref><ref id="RSTA20200097C101"><label>101</label><mixed-citation publication-type="std"><comment>Unwetterklimatologie: Starkregen. <uri xlink:href="www.dwd.de/DE/leistungen/unwetterklima/starkregen/starkregen.html">www.dwd.de/DE/leistungen/unwetterklima/starkregen/starkregen.html</uri>. (accessed 30 April 2020)</comment>.</mixed-citation></ref><ref id="RSTA20200097C102"><label>102</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vandal</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Kodra</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Ganguly</surname>
<given-names>AR</given-names></string-name></person-group>
<year>2019</year>
<article-title>Intercomparison of machine learning methods for statistical downscaling: the case of daily and extreme precipitation</article-title>. <source>Theor. Appl. Climatol.</source>
<volume>137</volume>, <fpage>557</fpage>&#x02013;<lpage>570</lpage>. (<pub-id pub-id-type="doi">10.1007/s00704-018-2613-3</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C103"><label>103</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O&#x02019;Gorman</surname>
<given-names>PA</given-names></string-name>, <string-name><surname>Dwyer</surname>
<given-names>JG</given-names></string-name></person-group>
<year>2018</year>
<article-title>Using machine learning to parameterize moist convection: potential for modeling of climate, climate change, and extreme events</article-title>. <source>J. Adv. Model. Earth Syst.</source>
<volume>10</volume>, <fpage>2548</fpage>&#x02013;<lpage>2563</lpage>. (<pub-id pub-id-type="doi">10.1029/2018MS001351</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C104"><label>104</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Ma</surname>
<given-names>Y</given-names></string-name></person-group> eds. <year>2013</year>
<source>Imbalanced learning: foundations, algorithms, and applications</source>, <edition>1st edn</edition>
<publisher-name>Hoboken, NJ: Wiley</publisher-name> (<pub-id pub-id-type="doi">10.1002/9781118646106</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C105"><label>105</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Torgo</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Ribeiro</surname>
<given-names>RP</given-names></string-name>, <string-name><surname>Pfahringer</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Branco</surname>
<given-names>P</given-names></string-name></person-group>
<year>2013</year>
<comment>Smote for regression. In <italic>Progress in Artificial Intelligence</italic> (eds L Correia, LP Reis, J Cascalho), pp. 378&#x02013;389. Berlin, Heidelberg: Springer.</comment> (<pub-id pub-id-type="doi">10.1007/978-3-642-40669-0_33</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C106"><label>106</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Lake</surname>
<given-names>BM</given-names></string-name>, <string-name><surname>Baroni</surname>
<given-names>M</given-names></string-name></person-group>
<year>2017</year>
<comment>Generalization without systematicity: on the compositional skills of sequence-to-sequence recurrent networks</comment>.</mixed-citation></ref><ref id="RSTA20200097C107"><label>107</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>
<given-names>Fei-Fei</given-names></string-name>, <string-name><surname>Fergus</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Perona</surname>
<given-names>P</given-names></string-name></person-group>
<year>2006</year>
<article-title>One-shot learning of object categories</article-title>. <source>IEEE Trans. Pattern. Anal.</source>
<volume>28</volume>, <fpage>594</fpage>&#x02013;<lpage>611</lpage>. (<pub-id pub-id-type="doi">10.1109/TPAMI.2006.79</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C108"><label>108</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lake</surname>
<given-names>BM</given-names></string-name>, <string-name><surname>Salakhutdinov</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Tenenbaum</surname>
<given-names>JB</given-names></string-name></person-group>
<year>2015</year>
<article-title>Human-level concept learning through probabilistic program induction</article-title>. <source>Science</source>
<volume>350</volume>, <fpage>1332</fpage>&#x02013;<lpage>1338</lpage>. (<pub-id pub-id-type="doi">10.1126/science.aab3050</pub-id>)<pub-id pub-id-type="pmid">26659050</pub-id></mixed-citation></ref><ref id="RSTA20200097C109"><label>109</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Smieja</surname>
<given-names>M</given-names></string-name>, <string-name><surname>&#x00141;ukasz</surname>
<given-names>Struski</given-names></string-name>, <string-name><surname>Tabor</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Zieli&#x00144;ski</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Spurek</surname>
<given-names>P</given-names></string-name></person-group>
<year>2018</year>
<comment>Processing of missing data by neural networks. <uri xlink:href="http://arxiv.org/abs/1805.07405">http://arxiv.org/abs/1805.07405</uri></comment>.</mixed-citation></ref><ref id="RSTA20200097C110"><label>110</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>&#x0017e;liobait&#x00117;</surname>
<given-names>I</given-names></string-name></person-group>
<year>2010</year>
<comment>Learning under concept drift: an overview. <uri xlink:href="http://arxiv.org/abs/1010.4784">http://arxiv.org/abs/1010.4784</uri></comment>.</mixed-citation></ref><ref id="RSTA20200097C111"><label>111</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Dong</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Gu</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Gama</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>G</given-names></string-name></person-group>
<year>2018</year>
<article-title>Learning under concept drift: a review</article-title>. <source>IEEE Trans. Knowl. Data Eng.</source>
<volume>31</volume>, <fpage>2346</fpage>&#x02013;<lpage>2363</lpage>. (<pub-id pub-id-type="doi">10.1109/tkde.2018.2876857</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C112"><label>112</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barth</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Alvera-Azc&#x000e1;rate</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Licer</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Beckers</surname>
<given-names>JM</given-names></string-name></person-group>
<year>2020</year>
<article-title>Dincae 1.0: a convolutional neural network with error estimates to reconstruct sea surface temperature satellite observations</article-title>. <source>Geoscientific Model Dev.</source>
<volume>13</volume>, <fpage>1609</fpage>&#x02013;<lpage>1622</lpage>. (<pub-id pub-id-type="doi">10.5194/gmd-13-1609-2020</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C113"><label>113</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Goodfellow</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Bengio</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Courville</surname>
<given-names>A</given-names></string-name></person-group>
<year>2016</year>
<source>Deep learning</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref><ref id="RSTA20200097C114"><label>114</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Casati</surname>
<given-names>B</given-names></string-name></person-group>
<italic>et al.</italic>
<year>2008</year>
<article-title>Forecast verification: current status and future directions</article-title>. <source>Meteorol. Appl.</source>
<volume>15</volume>, <fpage>3</fpage>&#x02013;<lpage>18</lpage>. (<pub-id pub-id-type="doi">10.1002/met.52</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C115"><label>115</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murphy</surname>
<given-names>AH</given-names></string-name>, <string-name><surname>Epstein</surname>
<given-names>ES</given-names></string-name></person-group>
<year>1989</year>
<article-title>Skill scores and correlation coefficients in model verification</article-title>. <source>Mon. Weather Rev.</source>
<volume>117</volume>, <fpage>572</fpage>&#x02013;<lpage>582</lpage>. (<pub-id pub-id-type="doi">10.1175/1520-0493(1989)117&#x000a1;0572:SSACCI&#x000bf;2.0.CO;2</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C116"><label>116</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ebert</surname>
<given-names>EE</given-names></string-name></person-group>
<year>2008</year>
<article-title>Fuzzy verification of high-resolution gridded forecasts: a review and proposed framework</article-title>. <source>Meteorol. Appl.</source>
<volume>15</volume>, <fpage>51</fpage>&#x02013;<lpage>64</lpage>. (<pub-id pub-id-type="doi">10.1002/met.25</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C117"><label>117</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weniger</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Kapp</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Friederichs</surname>
<given-names>P</given-names></string-name></person-group>
<year>2017</year>
<article-title>Spatial verification using wavelet transforms: a review: spatial verification using wavelet transforms</article-title>. <source>Q. J. R. Meteorol. Soc.</source>
<volume>143</volume>, <fpage>120</fpage>&#x02013;<lpage>136</lpage>. (<pub-id pub-id-type="doi">10.1002/qj.2881</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C118"><label>118</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buschow</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Pidstrigach</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Friederichs</surname>
<given-names>P</given-names></string-name></person-group>
<year>2019</year>
<article-title>Assessment of wavelet-based spatial verification by means of a stochastic precipitation model (wv_verif v0.1.0)</article-title>. <source>Geoscientific Model Dev.</source>
<volume>12</volume>, <fpage>3401</fpage>&#x02013;<lpage>3418</lpage>. (<pub-id pub-id-type="doi">10.5194/gmd-12-3401-2019</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C119"><label>119</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roberts</surname>
<given-names>NM</given-names></string-name>, <string-name><surname>Lean</surname>
<given-names>HW</given-names></string-name></person-group>
<year>2008</year>
<article-title>Scale-selective verification of rainfall accumulations from high-resolution forecasts of convective events</article-title>. <source>Mon. Weather Rev.</source>
<volume>136</volume>, <fpage>78</fpage>&#x02013;<lpage>97</lpage>. (<pub-id pub-id-type="doi">10.1175/2007MWR2123.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C120"><label>120</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilleland</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Ahijevych</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Brown</surname>
<given-names>BG</given-names></string-name>, <string-name><surname>Casati</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Ebert</surname>
<given-names>EE</given-names></string-name></person-group>
<year>2009</year>
<article-title>Intercomparison of spatial forecast verification methods</article-title>. <source>Weather Forecast.</source>
<volume>24</volume>, <fpage>1416</fpage>&#x02013;<lpage>1430</lpage>. (<pub-id pub-id-type="doi">10.1175/2009WAF2222269.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C121"><label>121</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dorninger</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Gilleland</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Casati</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Mittermaier</surname>
<given-names>MP</given-names></string-name>, <string-name><surname>Ebert</surname>
<given-names>EE</given-names></string-name>, <string-name><surname>Brown</surname>
<given-names>BG</given-names></string-name>, <string-name><surname>Wilson</surname>
<given-names>LJ</given-names></string-name></person-group>
<year>2018</year>
<article-title>The setup of the MesoVICT project</article-title>. <source>Bull. Am. Meteorol. Soc.</source>
<volume>99</volume>, <fpage>1887</fpage>&#x02013;<lpage>1906</lpage>. (<pub-id pub-id-type="doi">10.1175/BAMS-D-17-0164.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C122"><label>122</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lerch</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Thorarinsdottir</surname>
<given-names>TL</given-names></string-name>, <string-name><surname>Ravazzolo</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Gneiting</surname>
<given-names>T</given-names></string-name></person-group>
<year>2017</year>
<article-title>Forecaster&#x02019;s dilemma: extreme events and forecast evaluation</article-title>. <source>Stat. Sci.</source>
<volume>32</volume>, <fpage>106</fpage>&#x02013;<lpage>127</lpage>. (<pub-id pub-id-type="doi">10.1214/16-STS588</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C123"><label>123</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lazer</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Kennedy</surname>
<given-names>R</given-names></string-name>, <string-name><surname>King</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Vespignani</surname>
<given-names>A</given-names></string-name></person-group>
<year>2014</year>
<article-title>The parable of google flu: traps in big data analysis</article-title>. <source>Science</source>
<volume>343</volume>, <fpage>1203</fpage>&#x02013;<lpage>1205</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1248506</pub-id>)<pub-id pub-id-type="pmid">24626916</pub-id></mixed-citation></ref><ref id="RSTA20200097C124"><label>124</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lapuschkin</surname>
<given-names>S</given-names></string-name>, <string-name><surname>W&#x000e4;ldchen</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Binder</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Montavon</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Samek</surname>
<given-names>W</given-names></string-name>, <string-name><surname>M&#x000fc;ller</surname>
<given-names>KR</given-names></string-name></person-group>
<year>2019</year>
<article-title>Unmasking clever Hans predictors and assessing what machines really learn</article-title>. <source>Nat. Commun.</source>
<volume>10</volume>, <fpage>1</fpage>&#x02013;<lpage>8</lpage>. (<pub-id pub-id-type="doi">10.1038/s41467-019-08987-4</pub-id>)<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation></ref><ref id="RSTA20200097C125"><label>125</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karpatne</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Atluri</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Faghmous</surname>
<given-names>JH</given-names></string-name>, <string-name><surname>Steinbach</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Banerjee</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Ganguly</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Shekhar</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Samatova</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Kumar</surname>
<given-names>V</given-names></string-name></person-group>
<year>2017</year>
<article-title>Theory-guided data science: a new paradigm for scientific discovery from data</article-title>. <source>IEEE Trans. Knowl. Data Eng.</source>
<volume>29</volume>, <fpage>2318</fpage>&#x02013;<lpage>2331</lpage>. (<pub-id pub-id-type="doi">10.1109/TKDE.2017.2720168</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C126"><label>126</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Jia</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Willard</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Karpatne</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Read</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Zwart</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Steinbach</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Kumar</surname>
<given-names>V</given-names></string-name></person-group>
<year>2019</year>
<comment>Physics guided rnns for modeling dynamical systems: a case study in simulating lake temperature profiles. In <italic>Proc. of the 2019 SIAM Int. Conf. on Data Mining</italic>, pp. 558&#x02013;566. SIAM.</comment> (<pub-id pub-id-type="doi">10.1137/1.9781611975673.63</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C127"><label>127</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de B&#x000e9;zenac</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Pajot</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Gallinari</surname>
<given-names>P</given-names></string-name></person-group>
<year>2019</year>
<article-title>Deep learning for physical processes: incorporating prior scientific knowledge</article-title>. <source>J. Stat. Mech: Theory Exp.</source>
<volume>2019</volume>, <fpage>124009</fpage> (<pub-id pub-id-type="doi">10.1088/1742-5468/ab3195</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C128"><label>128</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raissi</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Perdikaris</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Karniadakis</surname>
<given-names>GE</given-names></string-name></person-group>
<year>2019</year>
<article-title>Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</article-title>. <source>J. Comput. Phys.</source>
<volume>378</volume>, <fpage>686</fpage>&#x02013;<lpage>707</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2018.10.045</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C129"><label>129</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Karpatne</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Watkins</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Read</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Kumar</surname>
<given-names>V</given-names></string-name></person-group>
<year>2017</year>
<comment>Physics-guided neural networks (PGNN): an application in lake temperature modeling. <uri xlink:href="http://arxiv.org/abs/1710.11431">http://arxiv.org/abs/1710.11431</uri></comment></mixed-citation></ref><ref id="RSTA20200097C130"><label>130</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Makhzani</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Shlens</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Jaitly</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Goodfellow</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Frey</surname>
<given-names>B</given-names></string-name></person-group>
<year>2015</year>
<comment>Adversarial autoencoders. <uri xlink:href="http://arxiv.org/abs/1511.05644">http://arxiv.org/abs/1511.05644</uri></comment></mixed-citation></ref><ref id="RSTA20200097C131"><label>131</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Goyal</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Hu</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Liang</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Xing</surname>
<given-names>EP</given-names></string-name></person-group>
<year>2017</year>
<comment>Nonparametric variational auto-encoders for hierarchical representation learning. In <italic>Proc. of the IEEE Int. Conf. on Computer Vision</italic>, pp. 5094&#x02013;5102.</comment> (<pub-id pub-id-type="doi">10.1109/ICCV.2017.545</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C132"><label>132</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arakawa</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Wu</surname>
<given-names>CM</given-names></string-name></person-group>
<year>2013</year>
<article-title>A unified representation of deep moist convection in numerical modeling of the atmosphere. Part I</article-title>. <source>J. Atmos. Sci.</source>
<volume>70</volume>, <fpage>1977</fpage>&#x02013;<lpage>1992</lpage>. (<pub-id pub-id-type="doi">10.1175/JAS-D-12-0330.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C133"><label>133</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>
<given-names>CM</given-names></string-name>, <string-name><surname>Arakawa</surname>
<given-names>A</given-names></string-name></person-group>
<year>2014</year>
<article-title>A unified representation of deep moist convection in numerical modeling of the atmosphere. Part II</article-title>. <source>J. Atmos. Sci.</source>
<volume>71</volume>, <fpage>2089</fpage>&#x02013;<lpage>2103</lpage>. (<pub-id pub-id-type="doi">10.1175/JAS-D-13-0382.1</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C134"><label>134</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Zabaras</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Koutsourelakis</surname>
<given-names>PS</given-names></string-name>, <string-name><surname>Perdikaris</surname>
<given-names>P</given-names></string-name></person-group>
<year>2019</year>
<article-title>Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data</article-title>. <source>J. Comput. Phys.</source>
<volume>394</volume>, <fpage>56</fpage>&#x02013;<lpage>81</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2019.05.024</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C135"><label>135</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Yeung</surname>
<given-names>DY</given-names></string-name></person-group>
<year>2016</year>
<comment>Towards Bayesian deep learning: A survey. <uri xlink:href="http://arxiv.org/abs/1604.01662">http://arxiv.org/abs/1604.01662</uri></comment></mixed-citation></ref><ref id="RSTA20200097C136"><label>136</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Yeung</surname>
<given-names>D</given-names></string-name></person-group>
<year>2016</year>
<article-title>Towards Bayesian deep learning: a framework and some existing methods</article-title>. <source>IEEE Trans. Knowl. Data Eng.</source>
<volume>28</volume>, <fpage>3395</fpage>&#x02013;<lpage>3408</lpage>. (<pub-id pub-id-type="doi">10.1109/TKDE.2016.2606428</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C137"><label>137</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Gal</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Islam</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ghahramani</surname>
<given-names>Z</given-names></string-name></person-group>
<year>2017</year>
<comment>Deep Bayesian active learning with image data. In <italic>Proc. of the 34th Int. Conf. on Machine Learning</italic> (ed. D Precup, YW Teh), volume 70 of <italic>Proc. of Machine Learning Research</italic>, pp. 1183&#x02013;1192. International Convention Centre, Sydney, Australia: PMLR. <uri xlink:href="http://proceedings.mlr.press/v70/gal17a">http://proceedings.mlr.press/v70/gal17a</uri></comment></mixed-citation></ref><ref id="RSTA20200097C138"><label>138</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>McAllister</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Gal</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Kendall</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Van</surname>
<given-names>Der</given-names></string-name>, <string-name><surname>Shah</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Cipolla</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Weller</surname>
<given-names>A</given-names></string-name></person-group>
<year>2017</year>
<comment>Concrete problems for autonomous vehicle safety: Advantages of Bayesian deep learning. In <italic>Proc. of the Twenty-Sixth Int. Joint Conf. on Artificial Intelligence</italic>. International Joint Conferences on Artificial Intelligence, Inc.</comment> (<pub-id pub-id-type="doi">10.24963/ijcai.2017/661</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C139"><label>139</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Gal</surname>
<given-names>Y</given-names></string-name></person-group>
<year>2016</year>
<comment>Uncertainty in deep learning. <italic>University of Cambridge</italic><bold>1</bold>. <uri xlink:href="https://pdfs.semanticscholar.org/55cd/9e1bb7ce02cd2bb01b364e7b331fcc1ef2c7.pdf?_ga=2.240865606.499308263.1596635628-148943261.1587029296">https://pdfs.semanticscholar.org/55cd/9e1bb7ce02cd2bb01b364e7b331fcc1ef2c7.pdf?_ga=2.240865606.499308263.1596635628-148943261.1587029296</uri>.</comment></mixed-citation></ref><ref id="RSTA20200097C140"><label>140</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Gal</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Ghahramani</surname>
<given-names>Z</given-names></string-name></person-group>
<year>2016</year>
<comment>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In <italic>Proc. of The 33rd Int. Conf. on Machine Learning</italic> (ed. MF Balcan, KQ Weinberger), volume 48 of <italic>Proc. of Machine Learning Research</italic>, pp. 1050&#x02013;1059. New York, NY: PMLR. <uri xlink:href="http://proceedings.mlr.press/v48/gal16.html">http://proceedings.mlr.press/v48/gal16.html</uri></comment></mixed-citation></ref><ref id="RSTA20200097C141"><label>141</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Qin</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Pei</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Jiang</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Feng</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Zhou</surname>
<given-names>J</given-names></string-name></person-group>
<year>2020</year>
<article-title>Probabilistic spatiotemporal wind speed forecasting based on a variational Bayesian deep learning model</article-title>. <source>Appl. Energy</source>
<volume>260</volume>, <fpage>114259</fpage> (<pub-id pub-id-type="doi">10.1016/j.apenergy.2019.114259</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C142"><label>142</label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Rasp</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Dueben</surname>
<given-names>PD</given-names></string-name>, <string-name><surname>Scher</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Weyn</surname>
<given-names>JA</given-names></string-name>, <string-name><surname>Mouatadid</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Thuerey</surname>
<given-names>N</given-names></string-name></person-group>
<year>2020</year>
<comment>Weatherbench: a benchmark dataset for data-driven weather forecasting. <uri xlink:href="http://arxiv.org/abs/2002.00469">http://arxiv.org/abs/2002.00469</uri></comment>.</mixed-citation></ref><ref id="RSTA20200097C143"><label>143</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Bottou</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Bengio</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Haffner</surname>
<given-names>P</given-names></string-name></person-group>
<year>1998</year>
<article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc. IEEE</source>
<volume>86</volume>, <fpage>2278</fpage>&#x02013;<lpage>2324</lpage>. (<pub-id pub-id-type="doi">10.1109/5.726791</pub-id>)</mixed-citation></ref><ref id="RSTA20200097C144"><label>144</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Russakovsky</surname>
<given-names>O</given-names></string-name></person-group>
<italic>et al.</italic>
<year>2015</year>
<article-title>Imagenet large scale visual recognition challenge</article-title>. <source>Int. J. Comput. Vision</source>
<volume>115</volume>, <fpage>211</fpage>&#x02013;<lpage>252</lpage>. (<pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>)</mixed-citation></ref></ref-list></back></article>