<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">7969919</article-id><article-id pub-id-type="publisher-id">85454</article-id><article-id pub-id-type="doi">10.1038/s41598-021-85454-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Generalizability of deep learning models for dental image analysis</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Krois</surname><given-names>Joachim</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Garcia Cantu</surname><given-names>Anselmo</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Chaurasia</surname><given-names>Akhilanand</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Patil</surname><given-names>Ranjitkumar</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Chaudhari</surname><given-names>Prabhat Kumar</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Gaudin</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Gehrung</surname><given-names>Sascha</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Schwendicke</surname><given-names>Falk</given-names></name><address><email>falk.schwendicke@charite.de</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.6363.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2218 4662</institution-id><institution>Department of Oral Diagnostics, Digital Health and Health Services Research, </institution><institution>Charit&#x000e9; - Universit&#x000e4;tsmedizin Berlin, </institution></institution-wrap>A&#x000df;mannshauser Str. 4-6, 14197 Berlin, Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.411275.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0645 6578</institution-id><institution>Department of Oral Medicine and Radiology, </institution><institution>King George&#x02019;s Medical University, </institution></institution-wrap>Lucknow, India </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.413618.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1767 6103</institution-id><institution>Division of Orthodontics and Dentofacial Deformities, </institution><institution>AIIMS, </institution></institution-wrap>New Delhi, India </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.6363.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2218 4662</institution-id><institution>Department of Oral and Maxillofacial Surgery, </institution><institution>Charit&#x000e9; - Universit&#x000e4;tsmedizin Berlin, </institution></institution-wrap>Berlin, Germany </aff></contrib-group><pub-date pub-type="epub"><day>17</day><month>3</month><year>2021</year></pub-date><pub-date pub-type="pmc-release"><day>17</day><month>3</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>11</volume><elocation-id>6102</elocation-id><history><date date-type="received"><day>26</day><month>11</month><year>2020</year></date><date date-type="accepted"><day>1</day><month>3</month><year>2021</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2021</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">We assessed the generalizability of deep learning models and how to improve it. Our exemplary use-case was the detection of apical lesions on panoramic radiographs. We employed two datasets of panoramic radiographs from two centers, one in Germany (Charit&#x000e9;, Berlin, n&#x02009;=&#x02009;650) and one in India (KGMU, Lucknow, n&#x02009;=&#x02009;650): First, U-Net type models were trained on images from Charit&#x000e9; (n&#x02009;=&#x02009;500) and assessed on test sets from Charit&#x000e9; and KGMU (each n&#x02009;=&#x02009;150). Second, the relevance of image characteristics was explored using pixel-value transformations, aligning the image characteristics in the datasets. Third, cross-center training effects on generalizability were evaluated by stepwise replacing Charite with KGMU images. Last, we assessed the impact of the dental status (presence of root-canal fillings or restorations). Models trained only on Charit&#x000e9; images showed a (mean&#x02009;&#x000b1;&#x02009;SD) F1-score of 54.1&#x02009;&#x000b1;&#x02009;0.8% on Charit&#x000e9; and 32.7&#x02009;&#x000b1;&#x02009;0.8% on KGMU data (<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001/t-test). Alignment of image data characteristics between the centers did not improve generalizability. However, by gradually increasing the fraction of KGMU images in the training set (from 0 to 100%) the F1-score on KGMU images improved (46.1&#x02009;&#x000b1;&#x02009;0.9%) at a moderate decrease on Charit&#x000e9; images (50.9&#x02009;&#x000b1;&#x02009;0.9%, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01). Model performance was good on KGMU images showing root-canal fillings and/or restorations, but much lower on KGMU images without root-canal fillings and/or restorations. Our deep learning models were not generalizable across centers. Cross-center training improved generalizability. Noteworthy, the dental status, but not image characteristics were relevant. Understanding the reasons behind limits in generalizability helps to mitigate generalizability problems.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Dental diseases</kwd><kwd>Computer science</kwd></kwd-group><funding-group><award-group><funding-source><institution>Charit&#x000e9; (3093)</institution></funding-source></award-group><open-access><p>Open Access funding enabled and organized by Projekt DEAL.</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2021</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">In recent years, the analysis of medical images in a range of disciplines, e.g. dermatology, ophthalmology and radiology has been increasingly assisted by the application of multi-layered (deep) neural networks, a technique known as deep learning<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. In dentistry, deep learning has been successfully applied to detect caries on peri-apical and bitewing images, as well as periodontal bone loss and apical lesions on panoramics and peri-apicals<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p><p id="Par3">Deep neural networks learn representations of statistical patterns and inherent structures from a large amount of data. In particular, deep convolutional neural networks (CNN) are suited to abstract highly complex spatial patterns from images. These models are trained in a supervised manner, by repeatedly presenting data points (e.g. images) and their corresponding labels (e.g. &#x0201c;apical lesion present&#x0201d;). Along this learning process, the internal parameters (weights) of the CNN are iteratively adjusted by minimizing a loss function, i.e. a quantifier of the deviation of the model predictions from the known labels<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p><p id="Par4">A range of limitations in deep learning applications in medicine have been identified<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Among those is the uncertainty about the generalizability of the developed models, i.e. their capacity to adequately predict on data which sources differ from those involved in the model training<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Hence, the use of independent datasets for model evaluation is recommended, as deep learning models trained and evaluated in-sample are at the risk of being over-parametrized, i.e. of &#x0201c;memorizing&#x0201d; the data. Under such conditions, the evaluation of the model may result in overly optimistic assumptions about its overall performance<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. Limited generalizability of deep learning models may be related to differences in image characteristics (associated with different data generation protocols, e.g. machine types or acquirement settings) or population characteristics (e.g. age, sex, dental status etc.)<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>.</p><p id="Par5">The generalizability of deep CNNs in medicine has not been widely evaluated, and there is currently no study available on this matter in dentistry. Moreover, elucidating the causes underlying possible deficits of generalizability is relevant, as this can facilitate the development of improved modeling strategies to overcome this problem as well as to define standards for model benchmarking prior to clinical usage to ensure robustness and generalizability. In the present study, we assessed the generalizability of deep CNNs for detecting apical lesions on panoramic radiographs. Our hypothesis was that a model developed on data from only one population, characterized by image and cohort features, shows significantly worse performance on unseen imagery from another population. Beyond gauging the models&#x02019; generalizability, our analysis focused on investigating the causes of limited generalizability and consequently on possible improvements in model training strategies.</p></sec><sec id="Sec2"><title>Materials and methods</title><sec id="Sec3"><title>Study design</title><p id="Par6">This study employed two datasets of panoramic radiographs from two centers, one in Germany (Charit&#x000e9;, Berlin) and one in India (King George Medical University, Lucknow, KGMU). Images had been pixel-wise annotated for apical lesions by four independent dental specialists and a master reviewer. U-Net type deep CNNs were trained to detect apical lesions. The models were trained using different proportions of data from the two centers. A range of experiments was performed to explore possible sources of differences in model performance when evaluating them on different datasets (with different proportions of data from both centers). Reporting of this study follows the STARD guideline<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> and the Checklist for Artificial Intelligence in Medical Imaging, CLAIM<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>.</p></sec><sec id="Sec4"><title>Performance metrics</title><p id="Par7">Model performance was assessed using a binary classification of every single pixel contained in an image, employing four performance metrics; F1-score, sensitivity, predicted positive value (PPV), and specificity. During training, the model was validated using the mean intersection-over-union score. Details on the metrics are provided in the appendix.</p></sec><sec id="Sec5"><title>Sample size</title><p id="Par8">Our primary outcome metric was the F1-score, which was suitable to reflect on the imbalances in our dataset (only a minority of pixels of any image are associated with apical lesions). For sample size estimation, we considered an independent two-sided t-test to compare the F1-score of the same models on Charit&#x000e9; test data (assumed to be F1&#x02009;=&#x02009;50%) versus KGMU data (F1&#x02009;=&#x02009;45%), and conservatively assumed a standard deviation of 15% in both groups, a power of 1-beta&#x02009;=&#x02009;0.80 and alpha&#x02009;=&#x02009;0.05. Under these assumptions, we required a minimum of 143 images per group. The final test set consisted of 150 images per group.</p></sec><sec id="Sec6"><title>Image dataset</title><p id="Par9">The analysis involved a set of 1300 panoramic radiographs, each of them cropped in order to isolate the region of interest, as shown in the appendix<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. A total of 650 images were provided by Charit&#x000e9; and 650 images by KGMU, respectively. Only radiographs from dentate adults were included, without any additional image selection criteria. The collection of data was ethically approved (EA4/080/18). Charit&#x000e9; images were generated by radiographic devices from Sirona Densply (Bensheim, Germany) and D&#x000fc;rr Dental (Bietigheim-Bissingen, Germany), while KGMU images were produced using Planmeca machines (Helsinki, Finland), both at different tube voltages and exposure times (depending on age and sex of the patient, among other parameters).</p></sec><sec id="Sec7"><title>Reference set</title><p id="Par10">The panoramic images were labeled pixel-wise by dental experts, each with at least 4&#x000a0;years of experience. Each annotator independently assessed each image under standardized conditions using an in-house custom-built annotation tool as described before<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. Prior to annotation, the examiners were advised on how to discriminate apical lesions from other entities (e.g. endodontic-periodontal lesions, a widened periodontal ligament etc.), as described in detail elsewhere<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. Each image was independently assessed by four experts. In a second step, the provided annotations were reviewed (addition, deletion, confirmation) by another expert with at least 10&#x000a0;years of experience and a focus on conservative dentistry and endodontology. Finally, the reference set was established as the union of all pixel labels on each image.</p></sec><sec id="Sec8"><title>Data preparation, model and training</title><p id="Par11">A fully convolutional neural network of the U-Net type<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> was trained to detect apical lesions. The model performs segmentation of images, i.e. classifies each pixel in an input image and thereby explicitly reproduces the spatial coverage of the object of interest (here, apical lesions). The U-Net architecture consists of encoding and decoding parts. The encoder abstracts image features that and the decoder uses this information to reconstruct the spatial coverage. The EfficientNet-B5 encoder was used, with its weights being initialized with those of a network previously trained to detect caries lesions on bitewings<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. The optimization of the model weights themselves was based on the backpropagation algorithm and the binary focal loss function, which is a generalization of the binary cross-entropy loss.</p><p id="Par12">The initial training set consisted of 500 images, all of them containing at least one apical lesion (=&#x02009;positive annotations) selected at random from the Charit&#x000e9; dataset. The decision was taken to reduce class imbalance on pixel level to some degree (the majority of pixels will nevertheless be negative, i.e. not affected by apical lesions). Prior to training, all images were re-scaled (width&#x02009;=&#x02009;704 px, height&#x02009;=&#x02009;352 px). For training, a fivefold cross-validation approach was used (see Fig. <xref rid="MOESM1" ref-type="media">A1</xref>a in appendix). To ensure similarity of these cross-validation splits, each dataset (Charit&#x000e9;, KGMU) was subset into clusters of homogenous dental status with respect to the number of apical lesions per image and the number of posterior and anterior teeth. To generate these clusters, a mini batch K-means clustering algorithm<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> was used, with the optimal number of clusters being established by quantifying the Silhouette<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> and the Davies&#x02013;Bouldin<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> scores. Clustering generated two data subsets, one featuring high and the other low number of teeth. In both sets the mean number of apical lesions per image was similar. Images from both clusters were eventually combined in order to achieve homogeneous cross-validation splits.<fig id="Fig1"><label>Figure 1</label><caption><p>Flowchart of the experimental workflow. From both centers, 650 panoramic images were used to train and validate (500 images per center) and test (150 images per center, 100 with apical lesions, 50 without, respectively) models. Image characteristics were compared, the Charit&#x000e9; training dataset augmented accordingly and then the re-trained model was tested on KGMU data. Further, models were trained on an increasingly mixed Charit&#x000e9;-KGMU dataset and tested on KGMU data. Last, models were tested on subsamples of KGMU data consisting root-canal fillings (and other restorations) or no root-canal fillings/restorations at all.</p></caption><graphic xlink:href="41598_2021_85454_Fig1_HTML" id="MO1"/></fig></p><p id="Par13">U-net models were trained and validated for every train-validation split, thus yielding 5 different models. During training, the images were augmented by applying random geometric transformations. The models were optimized by minimizing a linear combination of binary cross entropy and Dice loss. The learning rate was set to 0.002 and the batch size to 4. The output of each model was binarized by selecting a cutoff optimizing the F1-score. The training was stopped after 200 epochs. Convergence was assessed by monitoring the mean IoU on the validation set. After every epoch the model was evaluated on the validation split. The best performing model evaluated on each of the validation splits was selected. Each of the finally selected five models was evaluated on the test set (see below) and their metrics average reported (see Fig. <xref rid="MOESM1" ref-type="media">A2</xref>b in the appendix). Model training and data augmentation were carried out on a GeForce GTX 1080 Ti GPU, using Keras and the imgaug library.<fig id="Fig2"><label>Figure 2</label><caption><p>Differences between the Charit&#x000e9; and KGMU sets. (<bold>a</bold>) Representative images of the KGMU and Charit&#x000e9; datasets, showing the typical observed differences in population characteristics (number of teeth, presence of dental restorations) and image conditions (brightness and contrast). (<bold>b</bold>) The plot of the pixel means (proxy for brightness) and standard deviations (proxy for contrast) of images included in the training sets. (<bold>c</bold>) Boxplots of the pixel means and standard deviations of the Charit&#x000e9; and KGMU images in the training sets as well as of the same Charit&#x000e9; images after application of the pixel-wise transformations by data augmentation. Box and line: 25/75th percentiles and median. Whiskers: Minimum/maximum or 1.5 inter quartile rage if outliers are present; Dots: Outliers.</p></caption><graphic xlink:href="41598_2021_85454_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec9"><title>Testing generalizability</title><p id="Par14">Two main test sets (Charit&#x000e9;, KGMU) were constructed, each consisting 150 images (100 with positive annotations and 50 with negative ones, i.e. without apical lesions). Starting with a training set containing only radiographs from Charit&#x000e9; (see above), the trained model was tested on both these test sets, respectively, to gauge generalizability. In a second step, the relevance of image characteristics for generalizability was explored. To do so, pixel value transformations were included in the data augmentation, thus aligning the two data sets with respect to pixel value distributions, shifting the mean and standard deviation of the pixel values of Charit&#x000e9; images towards the mean and standard deviation of pixels values of KGMU. This was done by modifying the brightness and contrast of the images by random factor multiplication and by applying Contrast Limited Adaptive Histogram Equalization, respectively. Notably, these pixel augmentations were executed in absence of geometric transformations as these can artificially introduce black pixels at the boarders of the images. In a third step we evaluated cross-center training effects on generalizability, i.e. how introducing KGMU images to the initial training and validation dataset (incrementally and randomly replacing images from Charit&#x000e9;) impacts on the models&#x02019; generalizability. Replacement was performed to hold the overall dataset size constant. Last, we assessed the impact of the dental status, here characterized by the presence of root-canal fillings or restorations (fillings, crowns, bridges), on generalizability. Therefore, two subsamples (each of 30 positive images) were sampled from the KGMU test set, one where images lacked root-canal fillings or restorations and the other where each image contained root-canal fillings and restorations.</p></sec><sec id="Sec10"><title>Statistical analysis</title><p id="Par15">Differences in model performance were evaluated via independent two-sided t-tests, using <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05 as discriminating criterion. Computations were performed using the Python library scipy 1.5.2<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>.</p></sec><sec id="Sec11"><title>Ethical approval and informed consent</title><p id="Par16">All experiments were carried out in accordance with relevant guidelines and regulations. Data collection was ethically approved (Charit&#x000e9; ethics committee EA4/080/18).</p></sec></sec><sec id="Sec12"><title>Results</title><sec id="Sec13"><title>Dataset characteristics</title><p id="Par17">The KGMU and Charit&#x000e9; datasets showed differences in the population characteristics (including dental status) as well as in image characteristics (Table <xref rid="Tab1" ref-type="table">1</xref>). Patients in Charit&#x000e9; showed fewer teeth in both the anterior and posterior area, but more restorations (fillings, crowns) and root-canal fillings per image than in KGMU. On the other hand, KGMU images were brighter and exhibited higher contrast. Details on the clusters used to generate homogenous cross-validation splits are also shown in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Characteristics of the different datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="3">Patient/Image features</th><th align="left" colspan="4">Charit&#x000e9;</th><th align="left" colspan="6">KGMU</th></tr><tr><th align="left" colspan="3">Train/validation</th><th align="left" rowspan="2">Test</th><th align="left" colspan="3">Train/validation</th><th align="left" colspan="3">Test</th></tr><tr><th align="left">Full set</th><th align="left">Cluster low</th><th align="left">Cluster high</th><th align="left">Full set</th><th align="left">Cluster low</th><th align="left">Cluster high</th><th align="left">Full set</th><th align="left">With restorations</th><th align="left">Without restorations</th></tr></thead><tbody><tr><td align="left">No. anterior teeth; Mean (std, min&#x02013;max)</td><td align="left">10.53 (2.68, 1&#x02013;12)</td><td align="left">8.51, (3.37, 2&#x02013;12)</td><td align="left">11.7, (0.95, 1&#x02013;12)</td><td align="left">10.54, (2.89, 1&#x02013;12)</td><td align="left">10.92, (2.26, 0&#x02013;12)</td><td align="left">8.38, (3.55, 0&#x02013;12)</td><td align="left">11.51, (1.25, 2&#x02013;12)</td><td align="left">1.60, (1.23, 2&#x02013;12)</td><td align="left">11.9, (0.54, 9&#x02013;12)</td><td align="left">11.3, (1.99, 2&#x02013;12)</td></tr><tr><td align="left">No. posterior teeth; Mean (std, min&#x02013;max)</td><td align="left">12.03, (5.32, 0&#x02013;20)</td><td align="left">6.09, (2.88, 0&#x02013;10)</td><td align="left">15.5, (2.67, 11&#x02013;20)</td><td align="left">12.90, (5.47, 0&#x02013;20)</td><td align="left">16.62, (3.79, 1&#x02013;20)</td><td align="left">9.97, (3.0, 1&#x02013;16)</td><td align="left">18.18, (1.78, 14&#x02013;20)</td><td align="left">17.48, (2.86, 6&#x02013;20)</td><td align="left">17.43, (2.68, 7&#x02013;20)</td><td align="left">17.34, (3.0, 6&#x02013;20)</td></tr><tr><td align="left">No. apical lesions; Mean (std, min&#x02013;max)</td><td align="left">2.10, (1.5, 1&#x02013;13)</td><td align="left">2.08, (1.5, 1&#x02013;13)</td><td align="left">2.12, (1.5, 1&#x02013;8)</td><td align="left">1.6, (2.25, 0&#x02013;16)</td><td align="left">2.12, (1.46, 1&#x02013;8)</td><td align="left">2.20, (1.28, 1&#x02013;5)</td><td align="left">2.11, (1.49, 1&#x02013;8)</td><td align="left">1.25, (1.24, 0&#x02013;6)</td><td align="left">1.77, (0.99, 1&#x02013;5)</td><td align="left">1.57, (0.92, 1&#x02013;5)</td></tr><tr><td align="left">Images with fillings (%)</td><td align="left">0.87</td><td align="left">0.73</td><td align="left">0.94</td><td align="left">0.61</td><td align="left">0.32</td><td align="left">0.20</td><td align="left">0.34</td><td align="left">0.34</td><td align="left">0.83</td><td align="left">0</td></tr><tr><td align="left">Images with crowns and/or bridges (%)</td><td align="left">0.76</td><td align="left">0.8</td><td align="left">0.75</td><td align="left">0.62</td><td align="left">0.17</td><td align="left">0.20</td><td align="left">0.16</td><td align="left">0.13</td><td align="left">0.37</td><td align="left">0</td></tr><tr><td align="left">Images with root-canal fillings (%)</td><td align="left">0.78</td><td align="left">0.7</td><td align="left">0.82</td><td align="left">0.82</td><td align="left">0.28</td><td align="left">0.27</td><td align="left">0.29</td><td align="left">0.28</td><td align="left">1.0</td><td align="left">0</td></tr><tr><td align="left">Median image pixel-mean (Quartile 1, Quartile 3)</td><td align="left">94.27 (91.1, 96.7)</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">108.67 (101.0, 116.3)</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr><tr><td align="left">Median image pixel-std (Quartile 1, Quartile 3)</td><td align="left">37.55 (33.06, 41.30)</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">51.68, (48.38, 55.21)</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr></tbody></table><table-wrap-foot><p>For the Charit&#x000e9; and KGMU training sets, the clusters featuring a high/low number of teeth, as well as the full set where both clusters were combined. In the case of the KGMU test set, two additional subsamples were considered, one with root-canal fillings and restorations being present and the other without any.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec14"><title>Model performance and generalizability</title><p id="Par18">The experimental and data flow is summarized in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. In the first experiment, models trained only on Charit&#x000e9; images showed a (mean&#x02009;&#x000b1;&#x02009;SD) F1-score of 54.1&#x02009;&#x000b1;&#x02009;0.8% if evaluated on Charit&#x000e9; test data and 32.7&#x02009;&#x000b1;&#x02009;0.8% on KGMU test data, respectively (<italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001, t-test). The limited generalizability was mainly grounded in a lower sensitivity on KGMU versus Charit&#x000e9; data (48.0&#x02009;&#x000b1;&#x02009;1.0% on Charit&#x000e9; vs 22.0&#x02009;&#x000b1;&#x02009;1.3% on KGMU, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001). Only limited and non-significant differences between both data sets were observed for the PPV (64.0&#x02009;&#x000b1;&#x02009;4.0% Charit&#x000e9; vs 63.0&#x02009;&#x000b1;&#x02009;3.0% KGMU) and specificity (99.95&#x02009;&#x000b1;&#x02009;0.01% Charit&#x000e9; vs 99.97&#x02009;&#x000b1;&#x02009;0.01% KGMU).</p><p id="Par19">In a second experiment pixelwise augmentation related to the brightness and contrast was applied to the Charit&#x000e9; training set, aligning the distributions of the mean and standard deviation of the image pixels towards KGMU image characteristics (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). We found that training the models on images sharing similar pixel values did not lead to significant differences in F1-scores and did not improve generalizability. Moreover, this augmentation significantly lowered the sensitivity on both Charit&#x000e9; (45.7&#x02009;&#x000b1;&#x02009;1.2%, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.02) and KGMU (19.2&#x02009;&#x000b1;&#x02009;1.6%, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.02) images.</p><p id="Par20">In a third experiment we assessed the impact of gradually increasing the fraction of KGMU images in the training set. Increasing this fraction from 0 to 100% had the F1-score on KGMU images increasing monotonically to reach 46.1&#x02009;&#x000b1;&#x02009;0.9 (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a) as well as sensitivity (40.3&#x02009;&#x000b1;&#x02009;2.0%, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001). On the other hand, a decrease was observed for PPV (to 54.1&#x02009;&#x000b1;&#x02009;3.2%, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01) but not specificity (99.93&#x02009;&#x000b1;&#x02009;0.02%; <italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05). Concomitantly, the increase of KGMU data reduced the F1-score on Charit&#x000e9; images (to 50.9&#x02009;&#x000b1;&#x02009;0.9%, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01) as well as the PPV (56.66&#x02009;&#x000b1;&#x02009;5.75%, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01), without significantly modifying the sensitivity or specificity (<italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05).<fig id="Fig3"><label>Figure 3</label><caption><p>(<bold>a</bold>) The model performance on the Charit&#x000e9; and KGMU test sets with different fractions of KGMU images in the training set (markers and error bars denote the mean and standard deviation of the scores over the set of the 5 best models selected from cross-validation, respectively). (<bold>b</bold>) Model performance for the subsamples of the KGMU dataset with and without root-canal fillings and restorations being present. <italic>PPV</italic> Positive predictive value.</p></caption><graphic xlink:href="41598_2021_85454_Fig3_HTML" id="MO3"/></fig></p><p id="Par21">In a fourth experiment we evaluated the model on KGMU imagery with and without root-canal fillings or restorations (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b). The model was trained exclusively on Charit&#x000e9; data. We observed that the presence of root-canal fillings and restorations significantly improved the model performance compared in terms of both the sensitivity (31.7&#x02009;&#x000b1;&#x02009;2.2% versus 16.5&#x02009;&#x000b1;&#x02009;0.9%; <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01) and the F1-score (42.7&#x02009;&#x000b1;&#x02009;1.9% versus 25.8&#x02009;&#x000b1;&#x02009;1.2%; <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.001), although no significant differences were detected for PPV and specificity, respectively (<italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05).</p></sec></sec><sec id="Sec15"><title>Discussion</title><p id="Par22">Generalizability and robustness of machine learning models are relevant properties not usually known a priori. Models which are not generalizable across populations or image sources will only be applicable in the setting they were developed in. A number of possible sources for limits in generalizability have been identified<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Generalizability of CNNs has not been explored in dentistry, while the number of research studies in the field is increasing rapidly<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. We hypothesized that generalizability of deep learning models to detect apical lesions on panoramic images (as one exemplary use-case) was not given and that the models&#x02019; performance (measured via the F1-score) would significantly differ on test data from different centers. We confirm this hypothesis. Moreover, we showed that cross-center training can mitigate the lack of generalizability to some degree.</p><p id="Par23">Our findings need to be discussed in more detail. First, we showed generalizability between the two evaluated populations is not given, as exhibited by a significantly lower sensitivity and F1-score. While we cannot ascertain if the same behavior also applies for other conditions and detection tasks, our findings are noteworthy for researchers in dental image analysis. Second, we found that population characteristics (including dental status) and image conditions differed between the two centers. Indian individuals had more teeth and fewer dental work experience (restorations, root-canal fillings), while images were brighter and had more contrast (as they stemmed from different x-ray machines but also as different exposure conditions may have been used). We started to explore the effects of image characteristics first, and tried to overcome the differences in the training dataset by data augmentation. However, this did not overcome the problem of lacking generalizability; obviously, image pixel value characteristics were not at the heart of the problem. This is relevant from two perspectives: (1) Data augmentation can only limitedly mitigate limits in generalizability, with other approaches being required. (2) Image characteristics differences may not be the main problem leading to limited generalizability, and researchers may want to explore heterogeneity of training data towards other aspects than image conditions. Third, we found that by training the models on an increasingly mixed dataset, adding more and more data from the second center (KGMU), generalizability improved. We showed that with increasing cross-center training, the sensitivity of the model for KGMU data increased, at only limited detriment for Charit&#x000e9; data. Overall, adding KGMU data in a stepwise manner nearly mitigated the lack in generalizability. The increase in sensitivity (and F1-score) of the model on KGMU data was steepest when the first 20% of the Charit&#x000e9; data were replaced but continued to increase further when replacing up to 100%. In fact, training only on KGMU data (100% replacement) led to the models nevertheless performing quite well on Charit&#x000e9; data, which is noteworthy: Obviously, generalizability was not bidirectional in our experiments; models trained solely on KGMU data showed generalizability when applied to Charit&#x000e9; data, but not vice versa. Fourth, we explored this behavior and concluded that differences in the dental status of the two populations were a key factor. The largest difference in model performance was identified when the models were tested on KGMU data with root-canal fillings or restorations being present and without any restorations being present. Models trained on Charit&#x000e9; data generalized well on KGMU data with dental work experience, while the generalizability was poor on KGMU data without such experience. This might be, as in the (Charit&#x000e9;) training data such work experience was quite common (indicating the differences in dental treatment provision), enhancing the model sensitivity for detecting apical lesions, possibly as the model exploits correlations between apical lesions and root-canal fillings or restorations. This may also explain that in our case the discussed generalizability was not bi-directional: Models trained on Charit&#x000e9; data did not generalize well on KGMU data given the missing option to exploit this correlation, while models trained on KGMU data generalized better as they did not show this type of learning bias. Such a finding should encourage AI researchers to actively leverage clinical knowledge a priori, which may lead to better model performance as shown previously<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. However, our findings should also raise the awareness of researchers, reviewers and practitioners that the complexity of dental radiographic imagery may yet be underrepresented in many studies and outcome metrics on a hold-out test which originates from the same population as the training set may yield overly optimistic estimates for the model&#x02019;s generalizability.</p><p id="Par24">This study has a number of strengths and limitations. First, and as a strength, it assessed generalizability, a highly relevant property of deep learning algorithms, and aimed to identify reasons for limited generalizability as well as how to overcome them. Our study will inform the definition of standards within the ITU/WHO Focus Group AI for Health (FG-AI4H). Second, and as limitation, it focused on one exemplary use-case, the detection of apical lesions on panoramic radiographs, while a large range of further pathological or non-pathological findings on the same imagery or other material (other radiographs, but also photos, scan data etc.) are of interest. Also, the image material stemmed from two centers, and generalizability may be more or less affected when considering further centers, but also further machinery etc. Hence, one cannot deduce that our findings will be applicable to other settings and challenges. Third, we performed only a limited range of experiments to understand and mitigate limitations in generalizability. It is noteworthy that it may well be that further parameters beyond root-canal fillings or restorations are similarly associated with the model&#x02019;s performance. As a mean to overcome this difficulty, future studies could resort to apply methods of explainable AI to identify image level features and structures which are particularly relevant for the model. This could serve to identify correlation structures, contrast the areas of interest with those dentists use in their diagnostics performance, and safeguard the model against bias. Last, we used pixelwise metrics, which are easy to interpret and useful for this particular study. From a clinical perspective, it will be less important to identify the exact pixels, but the entities depicted by groups of pixels belonging to a same class. In previous studies, we used tooth-level metrics for this purpose<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>.</p></sec><sec id="Sec16"><title>Conclusion</title><p id="Par25">In conclusion, deep learning models trained to detect apical lesions on panoramic radiographs did not necessarily show generalizability. Replacing training data from one center with data from the other center (cross-center training) improved the model performance. We identified the presence of dental work experience in the training dataset to significantly affect generalizability, while image characteristics (brightness, contrast) were less important. Researchers should aim to demonstrate generalizability of their models and should employ cross-center training to increase it. Understanding the reasons behind limits in generalizability will help to devise further strategies to mitigate generalizability problems. Clinicians should scrutinize deep learning applications for applicability in their setting of interest.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Information</title><sec id="Sec17"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2021_85454_MOESM1_ESM.docx"><caption><p>Supplementary Information</p></caption></media></supplementary-material></p></sec></sec></body><back><fn-group><fn><p><bold>Publisher's note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Joachim Krois and Anselmo Garcia Cantu.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-021-85454-5.</p></sec><ack><title>Acknowledgements</title><p>We thank the dentists for their effort of labeling the image data.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceived or designed the study: A.G.C., J.K., F.S. Analyzed data: A.G.C., J.K., F.S. Interpreted the data: All authors. Wrote the paper: A.G.C., J.K., F.S. Reviewed the paper, approved the manuscript and are accountable for its content: All authors.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>Data used in this study can be made available if needed within data protection regulation boundaries.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par26">FS, RG and JK are co-founders of a Charit&#x000e9; startup on dental image analysis. The conduct, analysis and interpretation of this study and its findings was unrelated to this.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwendicke</surname><given-names>F</given-names></name><name><surname>Golla</surname><given-names>T</given-names></name><name><surname>Dreher</surname><given-names>M</given-names></name><name><surname>Krois</surname><given-names>J</given-names></name></person-group><article-title>Convolutional neural networks for dental image diagnostics: a scoping review</article-title><source>J. Dentist.</source><year>2019</year><pub-id pub-id-type="doi">10.1016/j.jdent.2019.103226</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Topol</surname><given-names>EJ</given-names></name></person-group><article-title>High-performance medicine: the convergence of human and artificial intelligence</article-title><source>Nat. Med.</source><year>2019</year><volume>25</volume><fpage>44</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/s41591-018-0300-7</pub-id><?supplied-pmid 30617339?><pub-id pub-id-type="pmid">30617339</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Kawaguchi, K., Pack Kaelbling, L. &#x00026; Bengio, Y. Generalization in Deep Learning. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1710.05468">http://arxiv.org/abs/1710.05468</ext-link> (2020).</mixed-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><etal/></person-group><article-title>Improving the generalizability of convolutional neural network-based segmentation on CMR images</article-title><source>Front. Cardiovasc. Med.</source><year>2020</year><pub-id pub-id-type="doi">10.3389/fcvm.2020.00105</pub-id><?supplied-pmid 33644123?><pub-id pub-id-type="pmid">33644123</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Nay, J. &#x00026; Strandburg, K. J. Generalizability: machine learning and humans-in-the-loop. <italic>Research Handbook on Big Data Law</italic> (Roland Vogl, ed., Edward Elgar, 2020 Forthcoming): NYU School of Law Public Law Research Paper <bold>20</bold> (2020).</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bossuyt</surname><given-names>PM</given-names></name><etal/></person-group><article-title>STARD 2015: an updated list of essential items for reporting diagnostic accuracy studies</article-title><source>BMJ</source><year>2015</year><volume>351</volume><fpage>h5527</fpage><pub-id pub-id-type="doi">10.1136/bmj.h5527</pub-id><?supplied-pmid 4623764?><pub-id pub-id-type="pmid">26511519</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongan</surname><given-names>J</given-names></name><name><surname>Moy</surname><given-names>L</given-names></name><name><surname>Kahn</surname><given-names>CE</given-names></name></person-group><article-title>Checklist for artificial intelligence in medical imaging (CLAIM): a guide for authors and reviewers</article-title><source>Radiol. Artif. Intell.</source><year>2020</year><volume>2</volume><fpage>29</fpage><pub-id pub-id-type="doi">10.1148/ryai.2020200029</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>H-S</given-names></name><name><surname>Song</surname><given-names>I-S</given-names></name><name><surname>Jung</surname><given-names>K-H</given-names></name></person-group><article-title>DeNTNet: Deep Neural Transfer Network for the detection of periodontal bone loss using panoramic dental radiographs</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><fpage>17615</fpage><pub-id pub-id-type="doi">10.1038/s41598-019-53758-2</pub-id><?supplied-pmid 31772195?><pub-id pub-id-type="pmid">31772195</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muramatsu</surname><given-names>C</given-names></name><etal/></person-group><article-title>Tooth detection and classification on panoramic radiographs for automatic dental chart filing: improved classification by multi-sized input data</article-title><source>Oral Radiol.</source><year>2020</year><pub-id pub-id-type="doi">10.1007/s11282-019-00418-w</pub-id><?supplied-pmid 31893343?><pub-id pub-id-type="pmid">31893343</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cantu</surname><given-names>AG</given-names></name><etal/></person-group><article-title>Detecting caries lesions of different radiographic extension on bitewings using deep learning</article-title><source>J. Dent.</source><year>2020</year><volume>100</volume><fpage>103425</fpage><pub-id pub-id-type="doi">10.1016/j.jdent.2020.103425</pub-id><?supplied-pmid 32634466?><pub-id pub-id-type="pmid">32634466</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekert</surname><given-names>T</given-names></name><etal/></person-group><article-title>Deep learning for the radiographic detection of apical lesions</article-title><source>J. Endod.</source><year>2019</year><volume>45</volume><fpage>917</fpage><lpage>922.e915</lpage><pub-id pub-id-type="doi">10.1016/j.joen.2019.03.016</pub-id><?supplied-pmid 31160078?><pub-id pub-id-type="pmid">31160078</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &#x00026; Brox, T. Dental X-ray image segmentation using a U-shaped Deep convolutional network. 1&#x02013;13 (2015).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Sculley, D. In <italic>Proceedings of the 19th International Conference on World Wide Web</italic> 1177&#x02013;1178 (Association for Computing Machinery, Raleigh, North Carolina, USA, 2010).</mixed-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name></person-group><article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title><source>J. Comput. Appl. Math.</source><year>1987</year><volume>20</volume><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>DL</given-names></name><name><surname>Bouldin</surname><given-names>DW</given-names></name></person-group><article-title>A cluster separation measure</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1979</year><volume>1</volume><fpage>224</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.1979.4766909</pub-id><pub-id pub-id-type="pmid">21868852</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><etal/></person-group><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nat. Methods</source><year>2020</year><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id><?supplied-pmid 7056644?><pub-id pub-id-type="pmid">32015543</pub-id></element-citation></ref></ref-list></back></article>